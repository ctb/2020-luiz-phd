
@inproceedings{heule_hyperloglog_2013,
	title = {{HyperLogLog} in practice: {Algorithmic} engineering of a state of the art cardinality estimation algorithm},
	shorttitle = {{HyperLogLog} in practice},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Extending} {Database} {Technology}},
	publisher = {ACM},
	author = {Heule, Stefan and Nunkesser, Marc and Hall, Alexander},
	year = {2013},
	pages = {683--692},
	file = {[PDF] from google.com:/home/luizirber/Zotero/storage/5PVSFZJR/Heule et al. - 2013 - HyperLogLog in practice Algorithmic engineering o.pdf:application/pdf}
}

@inproceedings{alon_space_1996,
	title = {The space complexity of approximating the frequency moments},
	url = {http://dl.acm.org/citation.cfm?id=237823},
	urldate = {2015-09-02},
	booktitle = {Proceedings of the twenty-eighth annual {ACM} symposium on {Theory} of computing},
	publisher = {ACM},
	author = {Alon, Noga and Matias, Yossi and Szegedy, Mario},
	year = {1996},
	pages = {20--29},
	file = {[PDF] from psu.edu:/home/luizirber/Zotero/storage/IQ7F6GNE/Alon et al. - 1996 - The space complexity of approximating the frequenc.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/ZSA2ZQAI/citation.html:text/html}
}

@article{giroire_directions_nodate,
	title = {Directions to use probabilistic algorithms for cardinality for {DNA} analysis},
	abstract = {Probabilistic algorithms for cardinality (see for example [1]) allow to estimate the number of distinct words of very large multisets. Best of them are very fast (only few tens of CPU operations per element) c and use constant memory (standard error of √M attained using M units of memory) to be compared with the linear memory used by exact algorithms. Hence they allow to do multiple experiments in few minutes with few KiloBytes on files of several GigaBytes that would be unfeasible with exact counting algorithms. Typically they are used for applications in the area of databases (see [2]) or networking (see [3] or [4]). Such algorithms are used here to analyze base correlation in human genome. The correlation is measured by the number of distinct subwords of fixed size k (10 bases for example) in a DNA piece of size N. The idea is that a sequence with few distinct subwords is more corrolated than a sequence of same size with more distinct subwords. Three different angles of study are introduced:- Are all possible words (4 k subwords of size k) present in the genome or, on the contrary, are a lot of patterns forbidden?- Is the genome homogeneus or are some areas more corrolated than others? In the late case, is it possible to recognize or have location hints, in a fast and easy way, for regions of different natures such as repetitions, coding or not coding regions?- What is the arrival rate of distinct subwords in the genome when considered as a sequence read},
	journal = {Journées Ouvertes Biologie Informatique Mathématiques},
	author = {Giroire, Frédéric},
	pages = {2006},
	file = {10.1.1.218.5647.pdf:/home/luizirber/Zotero/storage/5C6DSPEE/10.1.1.218.5647.pdf:application/pdf;Citeseer - Snapshot:/home/luizirber/Zotero/storage/2QI34GHS/summary.html:text/html}
}

@article{flajolet_hyperloglog:_2008,
	title = {{HyperLogLog}: the analysis of a near-optimal cardinality estimation algorithm},
	shorttitle = {{HyperLogLog}},
	url = {http://www.dmtcs.org/dmtcs-ojs/index.php/proceedings/article/viewArticle/914},
	number = {1},
	urldate = {2015-09-29},
	journal = {DMTCS Proceedings},
	author = {Flajolet, Philippe and Fusy, Éric and Gandouet, Olivier and Meunier, Frédéric},
	year = {2008},
	file = {914-3045-2-PB.pdf:/home/luizirber/Zotero/storage/BVJJ6NHE/914-3045-2-PB.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/GI55SV2U/914.html:text/html}
}

@article{holley_bloom_2016,
	title = {Bloom {Filter} {Trie}: an alignment-free and reference-free data structure for pan-genome storage},
	volume = {11},
	issn = {1748-7188},
	shorttitle = {Bloom {Filter} {Trie}},
	url = {http://dx.doi.org/10.1186/s13015-016-0066-8},
	doi = {10.1186/s13015-016-0066-8},
	abstract = {High throughput sequencing technologies have become fast and cheap in the past years. As a result, large-scale projects started to sequence tens to several thousands of genomes per species, producing a high number of sequences sampled from each genome. Such a highly redundant collection of very similar sequences is called a pan-genome. It can be transformed into a set of sequences “colored” by the genomes to which they belong. A colored de Bruijn graph (C-DBG) extracts from the sequences all colored k-mers, strings of length k, and stores them in vertices.},
	urldate = {2016-04-25},
	journal = {Algorithms for Molecular Biology},
	author = {Holley, Guillaume and Wittler, Roland and Stoye, Jens},
	year = {2016},
	keywords = {Bloom filter, Colored de bruijn graph, Index, Pan-genome, Population genomics, Similar genomes, Succinct data structure, Trie, compression},
	pages = {3},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/ZZ8AQVVR/Holley et al. - 2016 - Bloom Filter Trie an alignment-free and reference.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/MCJX4PV8/s13015-016-0066-8.html:text/html}
}

@article{mohamadi_ntcard:_nodate,
	title = {{ntCard}: a streaming algorithm for cardinality estimation in genomics data},
	shorttitle = {{ntCard}},
	url = {https://academic.oup.com/bioinformatics/article/doi/10.1093/bioinformatics/btw832/2832780/ntCard-a-streaming-algorithm-for-cardinality},
	doi = {10.1093/bioinformatics/btw832},
	urldate = {2017-04-25},
	journal = {Bioinformatics},
	author = {Mohamadi, Hamid and Khan, Hamza and Birol, Inanc},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/Z9K5FVRH/Mohamadi et al. - ntCard a streaming algorithm for cardinality esti.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/XHIQGWUA/btw832.html:text/html}
}

@article{melsted_kmerstream:_2014,
	title = {{KmerStream}: streaming algorithms for k -mer abundance estimation},
	volume = {30},
	issn = {1367-4803},
	shorttitle = {{KmerStream}},
	url = {https://academic.oup.com/bioinformatics/article/30/24/3541/2422237/KmerStream-streaming-algorithms-for-k-mer},
	doi = {10.1093/bioinformatics/btu713},
	number = {24},
	urldate = {2017-04-25},
	journal = {Bioinformatics},
	author = {Melsted, Páll and Halldórsson, Bjarni V.},
	month = dec,
	year = {2014},
	pages = {3541--3547},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/K5R82TGG/Melsted and Halldórsson - 2014 - KmerStream streaming algorithms for k -mer abunda.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/PGWUWEMG/KmerStream-streaming-algorithms-for-k-mer.html:text/html}
}

@article{benet_ipfs_2014,
	title = {{IPFS} - {Content} {Addressed}, {Versioned}, {P}2P {File} {System}},
	url = {http://arxiv.org/abs/1407.3561},
	abstract = {The InterPlanetary File System (IPFS) is a peer-to-peer distributed file system that seeks to connect all computing devices with the same system of files. In some ways, IPFS is similar to the Web, but IPFS could be seen as a single BitTorrent swarm, exchanging objects within one Git repository. In other words, IPFS provides a high throughput content-addressed block storage model, with content-addressed hyper links. This forms a generalized Merkle DAG, a data structure upon which one can build versioned file systems, blockchains, and even a Permanent Web. IPFS combines a distributed hashtable, an incentivized block exchange, and a self-certifying namespace. IPFS has no single point of failure, and nodes do not need to trust each other.},
	urldate = {2017-04-26},
	journal = {arXiv:1407.3561 [cs]},
	author = {Benet, Juan},
	month = jul,
	year = {2014},
	note = {arXiv: 1407.3561},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
	file = {arXiv\:1407.3561 PDF:/home/luizirber/Zotero/storage/3H6U2HKH/Benet - 2014 - IPFS - Content Addressed, Versioned, P2P File Syst.pdf:application/pdf;arXiv.org Snapshot:/home/luizirber/Zotero/storage/K2B3CSWH/1407.html:text/html}
}

@article{ondov_mash:_2016,
	title = {Mash: fast genome and metagenome distance estimation using {MinHash}},
	volume = {17},
	issn = {1474-760X},
	shorttitle = {Mash},
	url = {http://dx.doi.org/10.1186/s13059-016-0997-x},
	doi = {10.1186/s13059-016-0997-x},
	abstract = {Mash extends the MinHash dimensionality-reduction technique to include a pairwise mutation distance and P value significance test, enabling the efficient clustering and search of massive sequence collections. Mash reduces large sequences and sequence sets to small, representative sketches, from which global mutation distances can be rapidly estimated. We demonstrate several use cases, including the clustering of all 54,118 NCBI RefSeq genomes in 33 CPU h; real-time database search using assembled or unassembled Illumina, Pacific Biosciences, and Oxford Nanopore data; and the scalable clustering of hundreds of metagenomic samples by composition. Mash is freely released under a BSD license (                  https://github.com/marbl/mash                                  ).},
	urldate = {2017-04-26},
	journal = {Genome Biology},
	author = {Ondov, Brian D. and Treangen, Todd J. and Melsted, Páll and Mallonee, Adam B. and Bergman, Nicholas H. and Koren, Sergey and Phillippy, Adam M.},
	year = {2016},
	keywords = {metagenomics, Comparative genomics, Genomic distance, Alignment, Sequencing, Nanopore},
	pages = {132},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/X8SZW28A/Ondov et al. - 2016 - Mash fast genome and metagenome distance estimati.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/KXJTV6EH/s13059-016-0997-x.html:text/html}
}

@article{solomon_fast_2016,
	title = {Fast search of thousands of short-read sequencing experiments},
	volume = {34},
	copyright = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1087-0156},
	url = {https://www.nature.com/nbt/journal/v34/n3/full/nbt.3442.html},
	doi = {10.1038/nbt.3442},
	abstract = {The amount of sequence information in public repositories is growing at a rapid rate. Although these data are likely to contain clinically important information that has not yet been uncovered, our ability to effectively mine these repositories is limited. Here we introduce Sequence Bloom Trees (SBTs), a method for querying thousands of short-read sequencing experiments by sequence, 162 times faster than existing approaches. The approach searches large data archives for all experiments that involve a given sequence. We use SBTs to search 2,652 human blood, breast and brain RNA-seq experiments for all 214,293 known transcripts in under 4 days using less than 239 MB of RAM and a single CPU. Searching sequence archives at this scale and in this time frame is currently not possible using existing tools.},
	language = {en},
	number = {3},
	urldate = {2017-04-26},
	journal = {Nature Biotechnology},
	author = {Solomon, Brad and Kingsford, Carl},
	month = mar,
	year = {2016},
	keywords = {data mining, Genome informatics, Transcriptomics},
	pages = {300--302},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/25SGV6UJ/Solomon and Kingsford - 2016 - Fast search of thousands of short-read sequencing .pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/GUENRU37/nbt.3442.html:text/html}
}

@article{driscoll_making_1989,
	title = {Making data structures persistent},
	volume = {38},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/0022000089900342},
	doi = {10.1016/0022-0000(89)90034-2},
	abstract = {This paper is a study of persistence in data structures. Ordinary data structures are ephemeral in the sense that a change to the structure destroys the old version, leaving only the new version available for use. In contrast, a persistent structure allows access to any version, old or new, at any time. We develop simple, systematic, and efficient techniques for making linked data structures persistent. We use our techniques to devise persistent forms of binary search trees with logarithmic access, insertion, and deletion times and O(1) space bounds for insertion and deletion.},
	number = {1},
	urldate = {2017-04-26},
	journal = {Journal of Computer and System Sciences},
	author = {Driscoll, James R. and Sarnak, Neil and Sleator, Daniel D. and Tarjan, Robert E.},
	month = feb,
	year = {1989},
	pages = {86--124},
	file = {ScienceDirect Full Text PDF:/home/luizirber/Zotero/storage/MZSWD9D2/Driscoll et al. - 1989 - Making data structures persistent.pdf:application/pdf;ScienceDirect Snapshot:/home/luizirber/Zotero/storage/4QC8XUKH/0022000089900342.html:text/html}
}

@article{noauthor_closure_2011,
	title = {Closure of the {NCBI} {SRA} and implications for the long-term future of genomics data storage},
	volume = {12},
	issn = {1474-760X},
	url = {http://dx.doi.org/10.1186/gb-2011-12-3-402},
	doi = {10.1186/gb-2011-12-3-402},
	urldate = {2017-04-26},
	journal = {Genome Biology},
	year = {2011},
	pages = {402},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/P2MKQFNT/2011 - Closure of the NCBI SRA and implications for the l.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/TCM6HJHM/gb-2011-12-3-402.html:text/html}
}

@article{kyrpides_microbiome_2016,
	title = {Microbiome {Data} {Science}: {Understanding} {Our} {Microbial} {Planet}},
	volume = {24},
	issn = {0966-842X, 1878-4380},
	shorttitle = {Microbiome {Data} {Science}},
	url = {http://www.cell.com/trends/microbiology/abstract/S0966-842X(16)00048-2},
	doi = {10.1016/j.tim.2016.02.011},
	language = {English},
	number = {6},
	urldate = {2017-04-26},
	journal = {Trends in Microbiology},
	author = {Kyrpides, Nikos C. and Eloe-Fadrosh, Emiley A. and Ivanova, Natalia N.},
	month = jun,
	year = {2016},
	pmid = {27197692},
	keywords = {metagenomics, Microbiome, data science, data integration, data standards, data interoperability},
	pages = {425--427},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/R9KC582X/Kyrpides et al. - 2016 - Microbiome Data Science Understanding Our Microbi.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/UNTSUF77/S0966-842X(16)00048-2.html:text/html}
}

@article{popic_gattaca:_2017,
	title = {{GATTACA}: {Lightweight} {Metagenomic} {Binning} {With} {Compact} {Indexing} {Of} {Kmer} {Counts} {And} {MinHash}-based {Panel} {Selection}},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	shorttitle = {{GATTACA}},
	url = {http://biorxiv.org/content/early/2017/04/26/130997},
	doi = {10.1101/130997},
	abstract = {We introduce GATTACA, a framework for rapid and accurate binning of metagenomic contigs from a single or multiple metagenomic samples into clusters associated with individual species. The clusters are computed using co-abundance profiles within a set of reference metagnomes; unlike previous methods, GATTACA estimates these profiles from k-mer counts stored in a highly compact index. On multiple synthetic and real benchmark datasets, GATTACA produces clusters that correspond to distinct bacterial species with an accuracy that matches earlier methods, while being up to 20x faster when the reference panel index can be computed offline and 6x faster for online co-abundance estimation. Leveraging the MinHash technique to quickly compare metagenomic samples, GATTACA also provides an efficient way to identify publicly-available metagenomic data that can be incorporated into the set of reference metagenomes to further improve binning accuracy. Thus, enabling easy indexing and reuse of publicly-available metagenomic datasets, GATTACA makes accurate metagenomic analyses accessible to a much wider range of researchers.},
	language = {en},
	urldate = {2017-04-26},
	journal = {bioRxiv},
	author = {Popic, Victoria and Kuleshov, Volodymyr and Snyder, Michael and Batzoglou, Serafim},
	month = apr,
	year = {2017},
	pages = {130997},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/DW9QNDKR/Popic et al. - 2017 - GATTACA Lightweight Metagenomic Binning With Comp.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/CRBXUQMS/130997.html:text/html}
}

@inproceedings{broder_resemblance_1997,
	title = {On the resemblance and containment of documents},
	url = {http://ieeexplore.ieee.org/abstract/document/666900/},
	urldate = {2017-04-26},
	booktitle = {Compression and {Complexity} of {Sequences} 1997. {Proceedings}},
	publisher = {IEEE},
	author = {Broder, Andrei Z.},
	year = {1997},
	pages = {21--29},
	file = {[PDF] dec.com:/home/luizirber/Zotero/storage/54BT73Q4/Broder - 1997 - On the resemblance and containment of documents.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/W2UW5JUX/666900.html:text/html}
}

@article{christiani_set_2016,
	title = {Set {Similarity} {Search} {Beyond} {MinHash}},
	url = {http://arxiv.org/abs/1612.07710},
	abstract = {We consider the problem of approximate set similarity search under Braun-Blanquet similarity \$B({\textbackslash}mathbf\{x\}, {\textbackslash}mathbf\{y\}) = {\textbar}{\textbackslash}mathbf\{x\} {\textbackslash}cap {\textbackslash}mathbf\{y\}{\textbar} / {\textbackslash}max({\textbar}{\textbackslash}mathbf\{x\}{\textbar}, {\textbar}{\textbackslash}mathbf\{y\}{\textbar})\$. The \$(b\_2, b\_2)\$-approximate Braun-Blanquet similarity search problem is to preprocess a collection of sets \$P\$ such that, given a query set \${\textbackslash}mathbf\{q\}\$, if there exists \${\textbackslash}mathbf\{x\} {\textbackslash}in P\$ with \$B({\textbackslash}mathbf\{q\}, {\textbackslash}mathbf\{x\}) {\textbackslash}geq b\_1\$, then we can efficiently return \${\textbackslash}mathbf\{x\}' {\textbackslash}in P\$ with \$B({\textbackslash}mathbf\{q\}, {\textbackslash}mathbf\{x\}') {\textgreater} b\_2\$. We present a simple data structure that solves this problem with space usage \$O(n{\textasciicircum}\{1+{\textbackslash}rho\}{\textbackslash}log n + {\textbackslash}sum\_\{{\textbackslash}mathbf\{x\} {\textbackslash}in P\}{\textbar}{\textbackslash}mathbf\{x\}{\textbar})\$ and query time \$O({\textbar}{\textbackslash}mathbf\{q\}{\textbar}n{\textasciicircum}\{{\textbackslash}rho\} {\textbackslash}log n)\$ where \$n = {\textbar}P{\textbar}\$ and \${\textbackslash}rho = {\textbackslash}log(1/b\_1)/{\textbackslash}log(1/b\_2)\$. Making use of existing lower bounds for locality-sensitive hashing by O'Donnell et al. (TOCT 2014) we show that this value of \${\textbackslash}rho\$ is tight across the parameter space, i.e., for every choice of constants \$0 {\textless} b\_2 {\textless} b\_1 {\textless} 1\$. In the case where all sets have the same size our solution strictly improves upon the value of \${\textbackslash}rho\$ that can be obtained through the use of state-of-the-art data-independent techniques in the Indyk-Motwani locality-sensitive hashing framework (STOC 1998) such as Broder's MinHash (CCS 1997) for Jaccard similarity and Andoni et al.'s cross-polytope LSH (NIPS 2015) for cosine similarity. Surprisingly, even though our solution is data-independent, for a large part of the parameter space we outperform the currently best data-dependent method by Andoni and Razenshteyn (STOC 2015).},
	urldate = {2017-04-26},
	journal = {arXiv:1612.07710 [cs]},
	author = {Christiani, Tobias and Pagh, Rasmus},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.07710},
	keywords = {Computer Science - Data Structures and Algorithms},
	file = {arXiv\:1612.07710 PDF:/home/luizirber/Zotero/storage/FR5TVQRZ/Christiani and Pagh - 2016 - Set Similarity Search Beyond MinHash.pdf:application/pdf;arXiv.org Snapshot:/home/luizirber/Zotero/storage/NPA6H553/1612.html:text/html}
}

@article{titus_brown_sourmash:_2016,
	title = {sourmash: a library for {MinHash} sketching of {DNA}},
	volume = {1},
	shorttitle = {sourmash},
	url = {http://joss.theoj.org/papers/10.21105/joss.00027},
	doi = {10.21105/joss.00027},
	number = {5},
	urldate = {2017-04-26},
	journal = {The Journal of Open Source Software},
	author = {Titus Brown, C. and Irber, Luiz},
	month = sep,
	year = {2016}
}

@article{yu_hyperminhash:_2017,
	title = {{HyperMinHash}: {Jaccard} index sketching in {LogLog} space},
	shorttitle = {{HyperMinHash}},
	url = {http://arxiv.org/abs/1710.08436},
	abstract = {In this extended abstract, we describe and analyse a streaming probabilistic sketch, HYPERMINHASH, to estimate the Jaccard index (or Jaccard similarity coefficient) over two sets \$A\$ and \$B\$. HyperMinHash can be thought of as a compression of standard MinHash by building off of a HyperLogLog count-distinct sketch. Given Jaccard index \${\textbackslash}delta\$, using \$k\$ buckets of size \$O({\textbackslash}log(l) + {\textbackslash}log{\textbackslash}log({\textbar}A {\textbackslash}cup B{\textbar}))\$ (in practice, typically 2 bytes) per set, HyperMinHash streams over \$A\$ and \$B\$ and generates an estimate of the Jaccard index \${\textbackslash}delta\$ with error \$O(1/l + {\textbackslash}sqrt\{k/{\textbackslash}delta\})\$. This improves on the best previously known sketch, MinHash, which requires the same number of storage units (buckets), but using \$O({\textbackslash}log({\textbar}A {\textbackslash}cup B{\textbar}))\$ bit per bucket. For instance, our new algorithm allows estimating Jaccard indices of 0.01 for set cardinalities on the order of \$10{\textasciicircum}\{19\}\$ with relative error of around 5{\textbackslash}\% using 64KiB of memory; the previous state-of-the-art MinHash can only estimate Jaccard indices for cardinalities of \$10{\textasciicircum}\{10\}\$ with the same memory consumption. Alternately, one can think of HyperMinHash as an augmentation of b-bit MinHash that enables streaming updates, unions, and cardinality estimation (and thus intersection cardinality by way of Jaccard), while using \${\textbackslash}log{\textbackslash}log\$ extra bits.},
	urldate = {2017-10-25},
	journal = {arXiv:1710.08436 [cs]},
	author = {Yu, Y. William and Weber, Griffin},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.08436},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases},
	file = {arXiv\:1710.08436 PDF:/home/luizirber/Zotero/storage/EBWLBT9Y/Yu and Weber - 2017 - HyperMinHash Jaccard index sketching in LogLog sp.pdf:application/pdf;arXiv.org Snapshot:/home/luizirber/Zotero/storage/SFTPDBV9/1710.html:text/html}
}

@article{tsitsiklis_power_2011,
	title = {On the power of (even a little) centralization in distributed processing},
	volume = {39},
	number = {1},
	journal = {ACM SIGMETRICS Performance Evaluation Review},
	author = {Tsitsiklis, John N. and Xu, Kuang},
	year = {2011},
	pages = {121--132},
	file = {Fulltext:/home/luizirber/Zotero/storage/WFW8MH2I/Tsitsiklis and Xu - 2011 - On the power of (even a little) centralization in .pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/8YXAUHAY/citation.html:text/html}
}

@inproceedings{sun_allsome_2017,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{AllSome} {Sequence} {Bloom} {Trees}},
	isbn = {978-3-319-56969-7 978-3-319-56970-3},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-56970-3_17},
	doi = {10.1007/978-3-319-56970-3_17},
	abstract = {The ubiquity of next generation sequencing has transformed the size and nature of many databases, pushing the boundaries of current indexing and searching methods. One particular example is a database of 2,652 human RNA-seq experiments uploaded to the Sequence Read Archive. Recently, Solomon and Kingsford proposed the Sequence Bloom Tree data structure and demonstrated how it can be used to accurately identify SRA samples that have a transcript of interest potentially expressed. In this paper, we propose an improvement called the AllSome Sequence Bloom Tree. Results show that our new data structure significantly improves performance, reducing the tree construction time by 52.7\% and query time by 39–85\%, with a price of up to 3x memory consumption during queries. Notably, it can query a batch of 198,074 queries in under 8 h (compared to around two days previously) and a whole set of {\textbackslash}(k{\textbackslash})-mers from a sequencing experiment (about 27 mil {\textbackslash}(k{\textbackslash})-mers) in under 11 min.},
	language = {en},
	urldate = {2017-10-27},
	booktitle = {Research in {Computational} {Molecular} {Biology}},
	publisher = {Springer, Cham},
	author = {Sun, Chen and Harris, Robert S. and Chikhi, Rayan and Medvedev, Paul},
	month = may,
	year = {2017},
	pages = {272--286},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/UWYW7PX3/Sun et al. - 2017 - AllSome Sequence Bloom Trees.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/L6WQNVBP/978-3-319-56970-3_17.html:text/html}
}

@article{tennessen_prodege:_2016,
	title = {{ProDeGe}: a computational protocol for fully automated decontamination of genomes},
	volume = {10},
	copyright = {© 2015 Nature Publishing Group},
	issn = {1751-7362},
	shorttitle = {{ProDeGe}},
	url = {https://www.nature.com/ismej/journal/v10/n1/abs/ismej2015100a.html},
	doi = {10.1038/ismej.2015.100},
	abstract = {Single amplified genomes and genomes assembled from metagenomes have enabled the exploration of uncultured microorganisms at an unprecedented scale. However, both these types of products are plagued by contamination. Since these genomes are now being generated in a high-throughput manner and sequences from them are propagating into public databases to drive novel scientific discoveries, rigorous quality controls and decontamination protocols are urgently needed. Here, we present ProDeGe (Protocol for fully automated Decontamination of Genomes), the first computational protocol for fully automated decontamination of draft genomes. ProDeGe classifies sequences into two classes—clean and contaminant—using a combination of homology and feature-based methodologies. On average, 84\% of sequence from the non-target organism is removed from the data set (specificity) and 84\% of the sequence from the target organism is retained (sensitivity). The procedure operates successfully at a rate of {\textasciitilde}0.30 CPU core hours per megabase of sequence and can be applied to any type of genome sequence.},
	language = {en},
	number = {1},
	urldate = {2017-10-27},
	journal = {The ISME Journal},
	author = {Tennessen, Kristin and Andersen, Evan and Clingenpeel, Scott and Rinke, Christian and Lundberg, Derek S. and Han, James and Dangl, Jeff L. and Ivanova, Natalia and Woyke, Tanja and Kyrpides, Nikos and Pati, Amrita},
	month = jan,
	year = {2016},
	pages = {269--272},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/AH4M7YLK/Tennessen et al. - 2016 - ProDeGe a computational protocol for fully automa.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/6MIBPXNC/ismej2015100a.html:text/html}
}

@article{lux_acdc_2016,
	title = {acdc – {Automated} {Contamination} {Detection} and {Confidence} estimation for single-cell genome data},
	volume = {17},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-016-1397-7},
	doi = {10.1186/s12859-016-1397-7},
	abstract = {A major obstacle in single-cell sequencing is sample contamination with foreign DNA. To guarantee clean genome assemblies and to prevent the introduction of contamination into public databases, considerable quality control efforts are put into post-sequencing analysis. Contamination screening generally relies on reference-based methods such as database alignment or marker gene search, which limits the set of detectable contaminants to organisms with closely related reference species. As genomic coverage in the tree of life is highly fragmented, there is an urgent need for a reference-free methodology for contaminant identification in sequence data.},
	urldate = {2017-10-27},
	journal = {BMC Bioinformatics},
	author = {Lux, Markus and Krüger, Jan and Rinke, Christian and Maus, Irena and Schlüter, Andreas and Woyke, Tanja and Sczyrba, Alexander and Hammer, Barbara},
	month = dec,
	year = {2016},
	keywords = {Binning, Clustering, Contamination detection, Machine learning, Quality control, Single-cell sequencing},
	pages = {543},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/QMUF4WCP/Lux et al. - 2016 - acdc – Automated Contamination Detection and Confi.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/YD6CJII2/s12859-016-1397-7.html:text/html}
}

@article{baquero_pure_2017,
	title = {Pure {Operation}-{Based} {Replicated} {Data} {Types}},
	url = {http://arxiv.org/abs/1710.04469},
	abstract = {Distributed systems designed to serve clients across the world often make use of geo-replication to attain low latency and high availability. Conflict-free Replicated Data Types (CRDTs) allow the design of predictable multi-master replication and support eventual consistency of replicas that are allowed to transiently diverge. CRDTs come in two flavors: state-based, where a state is changed locally and shipped and merged into other replicas; operation-based, where operations are issued locally and reliably causal broadcast to all other replicas. However, the standard definition of op-based CRDTs is very encompassing, allowing even sending the full-state, and thus imposing storage and dissemination overheads as well as blurring the distinction from state-based CRDTs. We introduce pure op-based CRDTs, that can only send operations to other replicas, drawing a clear distinction from state-based ones. Data types with commutative operations can be trivially implemented as pure op-based CRDTs using standard reliable causal delivery; whereas data types having non-commutative operations are implemented using a PO-Log, a partially ordered log of operations, and making use of an extended API, i.e., a Tagged Causal Stable Broadcast (TCSB), that provides extra causality information upon delivery and later informs when delivered messages become causally stable, allowing further PO-Log compaction. The framework is illustrated by a catalog of pure op-based specifications for classic CRDTs, including counters, multi-value registers, add-wins and remove-wins sets.},
	urldate = {2017-11-01},
	journal = {arXiv:1710.04469 [cs]},
	author = {Baquero, Carlos and Almeida, Paulo Sergio and Shoker, Ali},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.04469},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Databases},
	file = {arXiv\:1710.04469 PDF:/home/luizirber/Zotero/storage/8GN86T9N/Baquero et al. - 2017 - Pure Operation-Based Replicated Data Types.pdf:application/pdf;arXiv.org Snapshot:/home/luizirber/Zotero/storage/TRPL76U4/1710.html:text/html}
}

@inproceedings{solomon_improved_2017,
	title = {Improved {Search} of {Large} {Transcriptomic} {Sequencing} {Databases} {Using} {Split} {Sequence} {Bloom} {Trees}},
	booktitle = {International {Conference} on {Research} in {Computational} {Molecular} {Biology}},
	publisher = {Springer},
	author = {Solomon, Brad and Kingsford, Carl},
	year = {2017},
	pages = {257--271},
	file = {Fulltext:/home/luizirber/Zotero/storage/YK8FPVVH/Solomon and Kingsford - 2017 - Improved Search of Large Transcriptomic Sequencing.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/QLGG56Y3/978-3-319-56970-3_16.html:text/html}
}

@article{crainiceanu_bloofi:_2015,
	title = {Bloofi: {Multidimensional} {Bloom} {Filters}},
	volume = {54},
	issn = {03064379},
	shorttitle = {Bloofi},
	url = {http://arxiv.org/abs/1501.01941},
	doi = {10.1016/j.is.2015.01.002},
	abstract = {Bloom filters are probabilistic data structures commonly used for approximate membership problems in many areas of Computer Science (networking, distributed systems, databases, etc.). With the increase in data size and distribution of data, problems arise where a large number of Bloom filters are available, and all them need to be searched for potential matches. As an example, in a federated cloud environment, each cloud provider could encode the information using Bloom filters and share the Bloom filters with a central coordinator. The problem of interest is not only whether a given element is in any of the sets represented by the Bloom filters, but which of the existing sets contain the given element. This problem cannot be solved by just constructing a Bloom filter on the union of all the sets. Instead, we effectively have a multidimensional Bloom filter problem: given an element, we wish to receive a list of candidate sets where the element might be. To solve this problem, we consider 3 alternatives. Firstly, we can naively check many Bloom filters. Secondly, we propose to organize the Bloom filters in a hierarchical index structure akin to a B+ tree, that we call Bloofi. Finally, we propose another data structure that packs the Bloom filters in such a way as to exploit bit-level parallelism, which we call Flat-Bloofi. Our theoretical and experimental results show that Bloofi and Flat-Bloofi provide scalable and efficient solutions alternatives to search through a large number of Bloom filters.},
	urldate = {2017-12-27},
	journal = {Information Systems},
	author = {Crainiceanu, Adina and Lemire, Daniel},
	month = dec,
	year = {2015},
	note = {arXiv: 1501.01941},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases},
	pages = {311--324},
	file = {arXiv\:1501.01941 PDF:/home/luizirber/Zotero/storage/2886ZUWC/Crainiceanu and Lemire - 2015 - Bloofi Multidimensional Bloom Filters.pdf:application/pdf;arXiv.org Snapshot:/home/luizirber/Zotero/storage/PIQQJEYJ/1501.html:text/html}
}

@misc{noauthor_bitfunnel:_2017,
	title = {{BitFunnel}: {A} signature-based search engine},
	copyright = {MIT},
	shorttitle = {{BitFunnel}},
	url = {https://github.com/BitFunnel/BitFunnel},
	urldate = {2017-12-27},
	publisher = {BitFunnel},
	month = dec,
	year = {2017},
	note = {original-date: 2016-04-01T22:44:40Z},
	keywords = {search, search-engine, search-in-text},
	file = {Snapshot:/home/luizirber/Zotero/storage/2GZPWS6M/BitFunnel.html:text/html}
}

@article{bradley_real-time_2017,
	title = {Real-time search of all bacterial and viral genomic data},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/12/15/234955},
	doi = {10.1101/234955},
	abstract = {Genome sequencing of pathogens is now ubiquitous in microbiology, and the sequence archives are effectively no longer searchable for arbitrary sequences. Furthermore, the exponential increase of these archives is likely to be further spurred by automated diagnostics. To unlock their use for scientific research and real-time surveillance we have combined knowledge about bacterial genetic variation with ideas used in web-search, to build a DNA search engine for microbial data that can grow incrementally. We indexed the complete global corpus of bacterial and viral whole genome sequence data (447,833 genomes), using four orders of magnitude less storage than previous methods. The method allows future scaling to millions of genomes. This renders the global archive accessible to sequence search, which we demonstrate with three applications: ultra-fast search for resistance genes MCR1-3, analysis of host-range for 2827 plasmids, and quantification of the rise of antibiotic resistance prevalence in the sequence archives.},
	language = {en},
	urldate = {2018-01-08},
	journal = {bioRxiv},
	author = {Bradley, Phelim and Bakker, Henk den and Rocha, Eduardo and McVean, Gil and Iqbal, Zamin},
	month = dec,
	year = {2017},
	pages = {234955},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/C2943RMW/Bradley et al. - 2017 - Real-time search of all bacterial and viral genomi.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/W6P7IVAM/234955.html:text/html}
}

@article{muggli_succinct_2017,
	title = {Succinct {De} {Bruijn} {Graph} {Construction} for {Massive} {Populations} {Through} {Space}-{Efficient} {Merging}},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/early/2017/12/06/229641},
	doi = {10.1101/229641},
	abstract = {Recently, there has been significant amount of effort in developing space-efficient and succinct data structures for storing and building the traditional de Bruijn graph and its variants, including the colored de Bruijn graph. However, a problem not yet considered is developing a means to merge succinct representations of the de Bruijn graph---a challenge is necessary for constructing the de Bruijn graph on very-large datasets. We create VARIMERGE, for building the colored de Bruijn graph on a very-large dataset through partitioning the data into smaller subsets, building the colored de Bruijn graph using a FM-index based representation, and merging these representations in an iterative format. This last step is an algorithmic challenge for which we present an algorithm in this paper. Lastly, we demonstrate the utility of VARIMERGE by demonstrating: a four-fold reduction in working space when constructing an 8,000 color dataset, and the construction of population graph two orders of magnitude larger than previous reported methods.},
	language = {en},
	urldate = {2018-01-08},
	journal = {bioRxiv},
	author = {Muggli, Martin D. and Boucher, Christina},
	month = dec,
	year = {2017},
	pages = {229641},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/DFPRZKWC/Muggli and Boucher - 2017 - Succinct De Bruijn Graph Construction for Massive .pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/Z6LPELP8/229641.html:text/html}
}

@article{pandey_mantis:_2017,
	title = {Mantis: {A} {Fast}, {Small}, and {Exact} {Large}-{Scale} {Sequence} {Search} {Index}},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {Mantis},
	url = {https://www.biorxiv.org/content/early/2017/11/10/217372},
	doi = {10.1101/217372},
	abstract = {Motivation: Sequence-level searches on large collections of RNA-seq experiments, such as the NIH Sequence Read Archive (SRA), would enable one to ask many questions about the expression or variation of a given transcript in a population. Bloom filter-based indexes and variants, such as the Sequence Bloom Tree, have been proposed in the past to solve this problem. However, these approaches suffer from fundamental limitations of the Bloom filter, resulting in slow build and query times, less-than-optimal space usage, and large numbers of false positives. Results: This paper introduces Mantis, a space-efficient data structure that can be used to index thousands of raw-read experiments and facilitate large-scale sequence searches on those experiments. Mantis uses counting quotient filters instead of Bloom filters, enabling rapid index builds and queries, small indexes, and exact results, i.e., no false positives or negatives. Furthermore, Mantis is also a colored de Bruijn graph representation, so it supports fast graph traversal and other topological analyses in addition to large-scale sequence-level searches. In our performance evaluation, index construction with Mantis is 4.4x faster and yields a 20\% smaller index than the state-of-the-art split sequence Bloom tree (SSBT). For queries, Mantis is 6x-108x faster than SSBT and has no false positives or false negatives. For example, Mantis was able to search for all 200,400 known human transcripts in an index of 2652 human blood, breast, and brain RNA-seq experiments in one hour and 22 minutes; SBT took close to 4 days and AllSomeSBT took about eight hours. Mantis is written in C++11 and is available at https://github.com/splatlab/mantis.},
	language = {en},
	urldate = {2018-01-19},
	journal = {bioRxiv},
	author = {Pandey, Prashant and Almodaresi, Fatemeh and Bender, Michael A. and Ferdman, Michael and Johnson, Rob and Patro, Rob},
	month = nov,
	year = {2017},
	pages = {217372},
	file = {Full Text PDF:/home/luizirber/Zotero/storage/MPX4YPS4/Pandey et al. - 2017 - Mantis A Fast, Small, and Exact Large-Scale Seque.pdf:application/pdf;Snapshot:/home/luizirber/Zotero/storage/M78QFA4Y/217372.html:text/html}
}