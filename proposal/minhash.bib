
@inproceedings{heule_hyperloglog_2013,
	title = {{HyperLogLog} in practice: Algorithmic engineering of a state of the art cardinality estimation algorithm},
	shorttitle = {{HyperLogLog} in practice},
	pages = {683--692},
	booktitle = {Proceedings of the 16th International Conference on Extending Database Technology},
	publisher = {{ACM}},
	author = {Heule, Stefan and Nunkesser, Marc and Hall, Alexander},
	date = {2013},
}

@inproceedings{alon_space_1996,
	title = {The space complexity of approximating the frequency moments},
	url = {http://dl.acm.org/citation.cfm?id=237823},
	pages = {20--29},
	booktitle = {Proceedings of the twenty-eighth annual {ACM} symposium on Theory of computing},
	publisher = {{ACM}},
	author = {Alon, Noga and Matias, Yossi and Szegedy, Mario},
	urldate = {2015-09-02},
	date = {1996},
}

@article{giroire_directions_nodate,
	title = {Directions to use probabilistic algorithms for cardinality for {DNA} analysis},
	abstract = {Probabilistic algorithms for cardinality (see for example [1]) allow to estimate the number of distinct words of very large multisets. Best of them are very fast (only few tens of {CPU} operations per element) c and use constant memory (standard error of √M attained using M units of memory) to be compared with the linear memory used by exact algorithms. Hence they allow to do multiple experiments in few minutes with few {KiloBytes} on files of several {GigaBytes} that would be unfeasible with exact counting algorithms. Typically they are used for applications in the area of databases (see [2]) or networking (see [3] or [4]). Such algorithms are used here to analyze base correlation in human genome. The correlation is measured by the number of distinct subwords of fixed size k (10 bases for example) in a {DNA} piece of size N. The idea is that a sequence with few distinct subwords is more corrolated than a sequence of same size with more distinct subwords. Three different angles of study are introduced:- Are all possible words (4 k subwords of size k) present in the genome or, on the contrary, are a lot of patterns forbidden?- Is the genome homogeneus or are some areas more corrolated than others? In the late case, is it possible to recognize or have location hints, in a fast and easy way, for regions of different natures such as repetitions, coding or not coding regions?- What is the arrival rate of distinct subwords in the genome when considered as a sequence read},
	pages = {2006},
	journaltitle = {Journées Ouvertes Biologie Informatique Mathématiques},
	author = {Giroire, Frédéric},
}

@article{flajolet_hyperloglog:_2008,
	title = {{HyperLogLog}: the analysis of a near-optimal cardinality estimation algorithm},
	url = {http://www.dmtcs.org/dmtcs-ojs/index.php/proceedings/article/viewArticle/914},
	shorttitle = {{HyperLogLog}},
	number = {1},
	journaltitle = {{DMTCS} Proceedings},
	author = {Flajolet, Philippe and Fusy, Éric and Gandouet, Olivier and Meunier, Frédéric},
	urldate = {2015-09-29},
	date = {2008},
}

@article{holley_bloom_2016,
	title = {Bloom Filter Trie: an alignment-free and reference-free data structure for pan-genome storage},
	volume = {11},
	issn = {1748-7188},
	url = {http://dx.doi.org/10.1186/s13015-016-0066-8},
	doi = {10.1186/s13015-016-0066-8},
	shorttitle = {Bloom Filter Trie},
	abstract = {High throughput sequencing technologies have become fast and cheap in the past years. As a result, large-scale projects started to sequence tens to several thousands of genomes per species, producing a high number of sequences sampled from each genome. Such a highly redundant collection of very similar sequences is called a pan-genome. It can be transformed into a set of sequences “colored” by the genomes to which they belong. A colored de Bruijn graph (C-{DBG}) extracts from the sequences all colored k-mers, strings of length k, and stores them in vertices.},
	pages = {3},
	journaltitle = {Algorithms for Molecular Biology},
	shortjournal = {Algorithms for Molecular Biology},
	author = {Holley, Guillaume and Wittler, Roland and Stoye, Jens},
	urldate = {2016-04-25},
	date = {2016},
	keywords = {Bloom filter, Colored de bruijn graph, Index, Pan-genome, Population genomics, Similar genomes, Succinct data structure, Trie, compression},
}

@article{mohamadi_ntcard:_nodate,
	title = {{ntCard}: a streaming algorithm for cardinality estimation in genomics data},
	url = {https://academic.oup.com/bioinformatics/article/doi/10.1093/bioinformatics/btw832/2832780/ntCard-a-streaming-algorithm-for-cardinality},
	doi = {10.1093/bioinformatics/btw832},
	shorttitle = {{ntCard}},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Mohamadi, Hamid and Khan, Hamza and Birol, Inanc},
	urldate = {2017-04-25},
}

@article{melsted_kmerstream:_2014,
	title = {{KmerStream}: streaming algorithms for k -mer abundance estimation},
	volume = {30},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/30/24/3541/2422237/KmerStream-streaming-algorithms-for-k-mer},
	doi = {10.1093/bioinformatics/btu713},
	shorttitle = {{KmerStream}},
	pages = {3541--3547},
	number = {24},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Melsted, Páll and Halldórsson, Bjarni V.},
	urldate = {2017-04-25},
	date = {2014-12-15},
}

@article{benet_ipfs_2014,
	title = {{IPFS} - Content Addressed, Versioned, P2P File System},
	url = {http://arxiv.org/abs/1407.3561},
	abstract = {The {InterPlanetary} File System ({IPFS}) is a peer-to-peer distributed file system that seeks to connect all computing devices with the same system of files. In some ways, {IPFS} is similar to the Web, but {IPFS} could be seen as a single {BitTorrent} swarm, exchanging objects within one Git repository. In other words, {IPFS} provides a high throughput content-addressed block storage model, with content-addressed hyper links. This forms a generalized Merkle {DAG}, a data structure upon which one can build versioned file systems, blockchains, and even a Permanent Web. {IPFS} combines a distributed hashtable, an incentivized block exchange, and a self-certifying namespace. {IPFS} has no single point of failure, and nodes do not need to trust each other.},
	journaltitle = {{arXiv}:1407.3561 [cs]},
	author = {Benet, Juan},
	urldate = {2017-04-26},
	date = {2014-07-14},
	eprinttype = {arxiv},
	eprint = {1407.3561},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
}

@article{ondov_mash:_2016,
	title = {Mash: fast genome and metagenome distance estimation using {MinHash}},
	volume = {17},
	issn = {1474-760X},
	url = {http://dx.doi.org/10.1186/s13059-016-0997-x},
	doi = {10.1186/s13059-016-0997-x},
	shorttitle = {Mash},
	abstract = {Mash extends the {MinHash} dimensionality-reduction technique to include a pairwise mutation distance and P value significance test, enabling the efficient clustering and search of massive sequence collections. Mash reduces large sequences and sequence sets to small, representative sketches, from which global mutation distances can be rapidly estimated. We demonstrate several use cases, including the clustering of all 54,118 {NCBI} {RefSeq} genomes in 33 {CPU} h; real-time database search using assembled or unassembled Illumina, Pacific Biosciences, and Oxford Nanopore data; and the scalable clustering of hundreds of metagenomic samples by composition. Mash is freely released under a {BSD} license (                  https://github.com/marbl/mash                                  ).},
	pages = {132},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {Ondov, Brian D. and Treangen, Todd J. and Melsted, Páll and Mallonee, Adam B. and Bergman, Nicholas H. and Koren, Sergey and Phillippy, Adam M.},
	urldate = {2017-04-26},
	date = {2016},
	keywords = {metagenomics, Comparative genomics, Genomic distance, Alignment, Sequencing, Nanopore},
}

@article{solomon_fast_2016,
	title = {Fast search of thousands of short-read sequencing experiments},
	volume = {34},
	rights = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1087-0156},
	url = {https://www.nature.com/nbt/journal/v34/n3/full/nbt.3442.html},
	doi = {10.1038/nbt.3442},
	abstract = {The amount of sequence information in public repositories is growing at a rapid rate. Although these data are likely to contain clinically important information that has not yet been uncovered, our ability to effectively mine these repositories is limited. Here we introduce Sequence Bloom Trees ({SBTs}), a method for querying thousands of short-read sequencing experiments by sequence, 162 times faster than existing approaches. The approach searches large data archives for all experiments that involve a given sequence. We use {SBTs} to search 2,652 human blood, breast and brain {RNA}-seq experiments for all 214,293 known transcripts in under 4 days using less than 239 {MB} of {RAM} and a single {CPU}. Searching sequence archives at this scale and in this time frame is currently not possible using existing tools.},
	pages = {300--302},
	number = {3},
	journaltitle = {Nature Biotechnology},
	shortjournal = {Nat Biotech},
	author = {Solomon, Brad and Kingsford, Carl},
	urldate = {2017-04-26},
	date = {2016-03},
	langid = {english},
	keywords = {data mining, Genome informatics, Transcriptomics},
}

@article{driscoll_making_1989,
	title = {Making data structures persistent},
	volume = {38},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/0022000089900342},
	doi = {10.1016/0022-0000(89)90034-2},
	abstract = {This paper is a study of persistence in data structures. Ordinary data structures are ephemeral in the sense that a change to the structure destroys the old version, leaving only the new version available for use. In contrast, a persistent structure allows access to any version, old or new, at any time. We develop simple, systematic, and efficient techniques for making linked data structures persistent. We use our techniques to devise persistent forms of binary search trees with logarithmic access, insertion, and deletion times and O(1) space bounds for insertion and deletion.},
	pages = {86--124},
	number = {1},
	journaltitle = {Journal of Computer and System Sciences},
	shortjournal = {Journal of Computer and System Sciences},
	author = {Driscoll, James R. and Sarnak, Neil and Sleator, Daniel D. and Tarjan, Robert E.},
	urldate = {2017-04-26},
	date = {1989-02-01},
}

@article{noauthor_closure_2011,
	title = {Closure of the {NCBI} {SRA} and implications for the long-term future of genomics data storage},
	volume = {12},
	issn = {1474-760X},
	url = {http://dx.doi.org/10.1186/gb-2011-12-3-402},
	doi = {10.1186/gb-2011-12-3-402},
	pages = {402},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	urldate = {2017-04-26},
	date = {2011},
}

@article{kyrpides_microbiome_2016,
	title = {Microbiome Data Science: Understanding Our Microbial Planet},
	volume = {24},
	issn = {0966-842X, 1878-4380},
	url = {http://www.cell.com/trends/microbiology/abstract/S0966-842X(16)00048-2},
	doi = {10.1016/j.tim.2016.02.011},
	shorttitle = {Microbiome Data Science},
	pages = {425--427},
	number = {6},
	journaltitle = {Trends in Microbiology},
	shortjournal = {Trends in Microbiology},
	author = {Kyrpides, Nikos C. and Eloe-Fadrosh, Emiley A. and Ivanova, Natalia N.},
	urldate = {2017-04-26},
	date = {2016-06-01},
	pmid = {27197692},
	keywords = {metagenomics, Microbiome, data science, data integration, data standards, data interoperability},
}

@article{popic_gattaca:_2017,
	title = {{GATTACA}: Lightweight Metagenomic Binning With Compact Indexing Of Kmer Counts And {MinHash}-based Panel Selection},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {http://biorxiv.org/content/early/2017/04/26/130997},
	doi = {10.1101/130997},
	shorttitle = {{GATTACA}},
	abstract = {We introduce {GATTACA}, a framework for rapid and accurate binning of metagenomic contigs from a single or multiple metagenomic samples into clusters associated with individual species. The clusters are computed using co-abundance profiles within a set of reference metagnomes; unlike previous methods, {GATTACA} estimates these profiles from k-mer counts stored in a highly compact index. On multiple synthetic and real benchmark datasets, {GATTACA} produces clusters that correspond to distinct bacterial species with an accuracy that matches earlier methods, while being up to 20x faster when the reference panel index can be computed offline and 6x faster for online co-abundance estimation. Leveraging the {MinHash} technique to quickly compare metagenomic samples, {GATTACA} also provides an efficient way to identify publicly-available metagenomic data that can be incorporated into the set of reference metagenomes to further improve binning accuracy. Thus, enabling easy indexing and reuse of publicly-available metagenomic datasets, {GATTACA} makes accurate metagenomic analyses accessible to a much wider range of researchers.},
	pages = {130997},
	journaltitle = {{bioRxiv}},
	author = {Popic, Victoria and Kuleshov, Volodymyr and Snyder, Michael and Batzoglou, Serafim},
	urldate = {2017-04-26},
	date = {2017-04-26},
	langid = {english},
}

@inproceedings{broder_resemblance_1997,
	title = {On the resemblance and containment of documents},
	url = {http://ieeexplore.ieee.org/abstract/document/666900/},
	pages = {21--29},
	booktitle = {Compression and Complexity of Sequences 1997. Proceedings},
	publisher = {{IEEE}},
	author = {Broder, Andrei Z.},
	urldate = {2017-04-26},
	date = {1997},
}

@article{christiani_set_2016,
	title = {Set Similarity Search Beyond {MinHash}},
	url = {http://arxiv.org/abs/1612.07710},
	abstract = {We consider the problem of approximate set similarity search under Braun-Blanquet similarity \$B({\textbackslash}mathbf\{x\}, {\textbackslash}mathbf\{y\}) = {\textbar}{\textbackslash}mathbf\{x\} {\textbackslash}cap {\textbackslash}mathbf\{y\}{\textbar} / {\textbackslash}max({\textbar}{\textbackslash}mathbf\{x\}{\textbar}, {\textbar}{\textbackslash}mathbf\{y\}{\textbar})\$. The \$(b\_2, b\_2)\$-approximate Braun-Blanquet similarity search problem is to preprocess a collection of sets \$P\$ such that, given a query set \${\textbackslash}mathbf\{q\}\$, if there exists \${\textbackslash}mathbf\{x\} {\textbackslash}in P\$ with \$B({\textbackslash}mathbf\{q\}, {\textbackslash}mathbf\{x\}) {\textbackslash}geq b\_1\$, then we can efficiently return \${\textbackslash}mathbf\{x\}' {\textbackslash}in P\$ with \$B({\textbackslash}mathbf\{q\}, {\textbackslash}mathbf\{x\}') {\textgreater} b\_2\$. We present a simple data structure that solves this problem with space usage \$O(n{\textasciicircum}\{1+{\textbackslash}rho\}{\textbackslash}log n + {\textbackslash}sum\_\{{\textbackslash}mathbf\{x\} {\textbackslash}in P\}{\textbar}{\textbackslash}mathbf\{x\}{\textbar})\$ and query time \$O({\textbar}{\textbackslash}mathbf\{q\}{\textbar}n{\textasciicircum}\{{\textbackslash}rho\} {\textbackslash}log n)\$ where \$n = {\textbar}P{\textbar}\$ and \${\textbackslash}rho = {\textbackslash}log(1/b\_1)/{\textbackslash}log(1/b\_2)\$. Making use of existing lower bounds for locality-sensitive hashing by O'Donnell et al. ({TOCT} 2014) we show that this value of \${\textbackslash}rho\$ is tight across the parameter space, i.e., for every choice of constants \$0 {\textless} b\_2 {\textless} b\_1 {\textless} 1\$. In the case where all sets have the same size our solution strictly improves upon the value of \${\textbackslash}rho\$ that can be obtained through the use of state-of-the-art data-independent techniques in the Indyk-Motwani locality-sensitive hashing framework ({STOC} 1998) such as Broder's {MinHash} ({CCS} 1997) for Jaccard similarity and Andoni et al.'s cross-polytope {LSH} ({NIPS} 2015) for cosine similarity. Surprisingly, even though our solution is data-independent, for a large part of the parameter space we outperform the currently best data-dependent method by Andoni and Razenshteyn ({STOC} 2015).},
	journaltitle = {{arXiv}:1612.07710 [cs]},
	author = {Christiani, Tobias and Pagh, Rasmus},
	urldate = {2017-04-26},
	date = {2016-12-22},
	eprinttype = {arxiv},
	eprint = {1612.07710},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@article{titus_brown_sourmash:_2016,
	title = {sourmash: a library for {MinHash} sketching of {DNA}},
	volume = {1},
	url = {http://joss.theoj.org/papers/10.21105/joss.00027},
	doi = {10.21105/joss.00027},
	shorttitle = {sourmash},
	number = {5},
	journaltitle = {The Journal of Open Source Software},
	author = {Titus Brown, C. and Irber, Luiz},
	urldate = {2017-04-26},
	date = {2016-09-14}
}

@article{yu_hyperminhash:_2017,
	title = {{HyperMinHash}: Jaccard index sketching in {LogLog} space},
	url = {http://arxiv.org/abs/1710.08436},
	shorttitle = {{HyperMinHash}},
	abstract = {In this extended abstract, we describe and analyse a streaming probabilistic sketch, {HYPERMINHASH}, to estimate the Jaccard index (or Jaccard similarity coefficient) over two sets \$A\$ and \$B\$. {HyperMinHash} can be thought of as a compression of standard {MinHash} by building off of a {HyperLogLog} count-distinct sketch. Given Jaccard index \${\textbackslash}delta\$, using \$k\$ buckets of size \$O({\textbackslash}log(l) + {\textbackslash}log{\textbackslash}log({\textbar}A {\textbackslash}cup B{\textbar}))\$ (in practice, typically 2 bytes) per set, {HyperMinHash} streams over \$A\$ and \$B\$ and generates an estimate of the Jaccard index \${\textbackslash}delta\$ with error \$O(1/l + {\textbackslash}sqrt\{k/{\textbackslash}delta\})\$. This improves on the best previously known sketch, {MinHash}, which requires the same number of storage units (buckets), but using \$O({\textbackslash}log({\textbar}A {\textbackslash}cup B{\textbar}))\$ bit per bucket. For instance, our new algorithm allows estimating Jaccard indices of 0.01 for set cardinalities on the order of \$10{\textasciicircum}\{19\}\$ with relative error of around 5{\textbackslash}\% using 64KiB of memory; the previous state-of-the-art {MinHash} can only estimate Jaccard indices for cardinalities of \$10{\textasciicircum}\{10\}\$ with the same memory consumption. Alternately, one can think of {HyperMinHash} as an augmentation of b-bit {MinHash} that enables streaming updates, unions, and cardinality estimation (and thus intersection cardinality by way of Jaccard), while using \${\textbackslash}log{\textbackslash}log\$ extra bits.},
	journaltitle = {{arXiv}:1710.08436 [cs]},
	author = {Yu, Y. William and Weber, Griffin},
	urldate = {2017-10-25},
	date = {2017-10-23},
	eprinttype = {arxiv},
	eprint = {1710.08436},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases},
}

@article{tsitsiklis_power_2011,
	title = {On the power of (even a little) centralization in distributed processing},
	volume = {39},
	pages = {121--132},
	number = {1},
	journaltitle = {{ACM} {SIGMETRICS} Performance Evaluation Review},
	author = {Tsitsiklis, John N. and Xu, Kuang},
	date = {2011},
}

@inproceedings{sun_allsome_2017,
	title = {{AllSome} Sequence Bloom Trees},
	isbn = {978-3-319-56969-7 978-3-319-56970-3},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-56970-3_17},
	doi = {10.1007/978-3-319-56970-3_17},
	series = {Lecture Notes in Computer Science},
	abstract = {The ubiquity of next generation sequencing has transformed the size and nature of many databases, pushing the boundaries of current indexing and searching methods. One particular example is a database of 2,652 human {RNA}-seq experiments uploaded to the Sequence Read Archive. Recently, Solomon and Kingsford proposed the Sequence Bloom Tree data structure and demonstrated how it can be used to accurately identify {SRA} samples that have a transcript of interest potentially expressed. In this paper, we propose an improvement called the {AllSome} Sequence Bloom Tree. Results show that our new data structure significantly improves performance, reducing the tree construction time by 52.7\% and query time by 39–85\%, with a price of up to 3x memory consumption during queries. Notably, it can query a batch of 198,074 queries in under 8 h (compared to around two days previously) and a whole set of {\textbackslash}(k{\textbackslash})-mers from a sequencing experiment (about 27 mil {\textbackslash}(k{\textbackslash})-mers) in under 11 min.},
	eventtitle = {International Conference on Research in Computational Molecular Biology},
	pages = {272--286},
	booktitle = {Research in Computational Molecular Biology},
	publisher = {Springer, Cham},
	author = {Sun, Chen and Harris, Robert S. and Chikhi, Rayan and Medvedev, Paul},
	urldate = {2017-10-27},
	date = {2017-05-03},
	langid = {english},
}

@article{tennessen_prodege:_2016,
	title = {{ProDeGe}: a computational protocol for fully automated decontamination of genomes},
	volume = {10},
	rights = {© 2015 Nature Publishing Group},
	issn = {1751-7362},
	url = {https://www.nature.com/ismej/journal/v10/n1/abs/ismej2015100a.html},
	doi = {10.1038/ismej.2015.100},
	shorttitle = {{ProDeGe}},
	abstract = {Single amplified genomes and genomes assembled from metagenomes have enabled the exploration of uncultured microorganisms at an unprecedented scale. However, both these types of products are plagued by contamination. Since these genomes are now being generated in a high-throughput manner and sequences from them are propagating into public databases to drive novel scientific discoveries, rigorous quality controls and decontamination protocols are urgently needed. Here, we present {ProDeGe} (Protocol for fully automated Decontamination of Genomes), the first computational protocol for fully automated decontamination of draft genomes. {ProDeGe} classifies sequences into two classes—clean and contaminant—using a combination of homology and feature-based methodologies. On average, 84\% of sequence from the non-target organism is removed from the data set (specificity) and 84\% of the sequence from the target organism is retained (sensitivity). The procedure operates successfully at a rate of {\textasciitilde}0.30 {CPU} core hours per megabase of sequence and can be applied to any type of genome sequence.},
	pages = {269--272},
	number = {1},
	journaltitle = {The {ISME} Journal},
	shortjournal = {{ISME} J},
	author = {Tennessen, Kristin and Andersen, Evan and Clingenpeel, Scott and Rinke, Christian and Lundberg, Derek S. and Han, James and Dangl, Jeff L. and Ivanova, Natalia and Woyke, Tanja and Kyrpides, Nikos and Pati, Amrita},
	urldate = {2017-10-27},
	date = {2016-01},
	langid = {english},
}

@article{lux_acdc_2016,
	title = {acdc – Automated Contamination Detection and Confidence estimation for single-cell genome data},
	volume = {17},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-016-1397-7},
	doi = {10.1186/s12859-016-1397-7},
	abstract = {A major obstacle in single-cell sequencing is sample contamination with foreign {DNA}. To guarantee clean genome assemblies and to prevent the introduction of contamination into public databases, considerable quality control efforts are put into post-sequencing analysis. Contamination screening generally relies on reference-based methods such as database alignment or marker gene search, which limits the set of detectable contaminants to organisms with closely related reference species. As genomic coverage in the tree of life is highly fragmented, there is an urgent need for a reference-free methodology for contaminant identification in sequence data.},
	pages = {543},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Lux, Markus and Krüger, Jan and Rinke, Christian and Maus, Irena and Schlüter, Andreas and Woyke, Tanja and Sczyrba, Alexander and Hammer, Barbara},
	urldate = {2017-10-27},
	date = {2016-12-20},
	keywords = {Binning, Clustering, Contamination detection, Machine learning, Quality control, Single-cell sequencing},
}

@article{baquero_pure_2017,
	title = {Pure Operation-Based Replicated Data Types},
	url = {http://arxiv.org/abs/1710.04469},
	abstract = {Distributed systems designed to serve clients across the world often make use of geo-replication to attain low latency and high availability. Conflict-free Replicated Data Types ({CRDTs}) allow the design of predictable multi-master replication and support eventual consistency of replicas that are allowed to transiently diverge. {CRDTs} come in two flavors: state-based, where a state is changed locally and shipped and merged into other replicas; operation-based, where operations are issued locally and reliably causal broadcast to all other replicas. However, the standard definition of op-based {CRDTs} is very encompassing, allowing even sending the full-state, and thus imposing storage and dissemination overheads as well as blurring the distinction from state-based {CRDTs}. We introduce pure op-based {CRDTs}, that can only send operations to other replicas, drawing a clear distinction from state-based ones. Data types with commutative operations can be trivially implemented as pure op-based {CRDTs} using standard reliable causal delivery; whereas data types having non-commutative operations are implemented using a {PO}-Log, a partially ordered log of operations, and making use of an extended {API}, i.e., a Tagged Causal Stable Broadcast ({TCSB}), that provides extra causality information upon delivery and later informs when delivered messages become causally stable, allowing further {PO}-Log compaction. The framework is illustrated by a catalog of pure op-based specifications for classic {CRDTs}, including counters, multi-value registers, add-wins and remove-wins sets.},
	journaltitle = {{arXiv}:1710.04469 [cs]},
	author = {Baquero, Carlos and Almeida, Paulo Sergio and Shoker, Ali},
	urldate = {2017-11-01},
	date = {2017-10-12},
	eprinttype = {arxiv},
	eprint = {1710.04469},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Databases},
}

@inproceedings{solomon_improved_2017,
	title = {Improved Search of Large Transcriptomic Sequencing Databases Using Split Sequence Bloom Trees},
	pages = {257--271},
	booktitle = {International Conference on Research in Computational Molecular Biology},
	publisher = {Springer},
	author = {Solomon, Brad and Kingsford, Carl},
	date = {2017},
}

@article{crainiceanu_bloofi:_2015,
	title = {Bloofi: Multidimensional Bloom Filters},
	volume = {54},
	issn = {03064379},
	url = {http://arxiv.org/abs/1501.01941},
	doi = {10.1016/j.is.2015.01.002},
	shorttitle = {Bloofi},
	abstract = {Bloom filters are probabilistic data structures commonly used for approximate membership problems in many areas of Computer Science (networking, distributed systems, databases, etc.). With the increase in data size and distribution of data, problems arise where a large number of Bloom filters are available, and all them need to be searched for potential matches. As an example, in a federated cloud environment, each cloud provider could encode the information using Bloom filters and share the Bloom filters with a central coordinator. The problem of interest is not only whether a given element is in any of the sets represented by the Bloom filters, but which of the existing sets contain the given element. This problem cannot be solved by just constructing a Bloom filter on the union of all the sets. Instead, we effectively have a multidimensional Bloom filter problem: given an element, we wish to receive a list of candidate sets where the element might be. To solve this problem, we consider 3 alternatives. Firstly, we can naively check many Bloom filters. Secondly, we propose to organize the Bloom filters in a hierarchical index structure akin to a B+ tree, that we call Bloofi. Finally, we propose another data structure that packs the Bloom filters in such a way as to exploit bit-level parallelism, which we call Flat-Bloofi. Our theoretical and experimental results show that Bloofi and Flat-Bloofi provide scalable and efficient solutions alternatives to search through a large number of Bloom filters.},
	pages = {311--324},
	journaltitle = {Information Systems},
	author = {Crainiceanu, Adina and Lemire, Daniel},
	urldate = {2017-12-27},
	date = {2015-12},
	eprinttype = {arxiv},
	eprint = {1501.01941},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases},
}

@software{noauthor_bitfunnel:_2017,
	title = {{BitFunnel}: A signature-based search engine},
	rights = {{MIT}},
	url = {https://github.com/BitFunnel/BitFunnel},
	shorttitle = {{BitFunnel}},
	publisher = {{BitFunnel}},
	urldate = {2017-12-27},
	date = {2017-12-27},
	note = {original-date: 2016-04-01T22:44:40Z},
	keywords = {search, search-engine, search-in-text},
}

@article{bradley_real-time_2017,
	title = {Real-time search of all bacterial and viral genomic data},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/12/15/234955},
	doi = {10.1101/234955},
	abstract = {Genome sequencing of pathogens is now ubiquitous in microbiology, and the sequence archives are effectively no longer searchable for arbitrary sequences. Furthermore, the exponential increase of these archives is likely to be further spurred by automated diagnostics. To unlock their use for scientific research and real-time surveillance we have combined knowledge about bacterial genetic variation with ideas used in web-search, to build a {DNA} search engine for microbial data that can grow incrementally. We indexed the complete global corpus of bacterial and viral whole genome sequence data (447,833 genomes), using four orders of magnitude less storage than previous methods. The method allows future scaling to millions of genomes. This renders the global archive accessible to sequence search, which we demonstrate with three applications: ultra-fast search for resistance genes {MCR}1-3, analysis of host-range for 2827 plasmids, and quantification of the rise of antibiotic resistance prevalence in the sequence archives.},
	pages = {234955},
	journaltitle = {{bioRxiv}},
	author = {Bradley, Phelim and Bakker, Henk den and Rocha, Eduardo and {McVean}, Gil and Iqbal, Zamin},
	urldate = {2018-01-08},
	date = {2017-12-15},
	langid = {english},
}

@article{muggli_succinct_2017,
	title = {Succinct De Bruijn Graph Construction for Massive Populations Through Space-Efficient Merging},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/early/2017/12/06/229641},
	doi = {10.1101/229641},
	abstract = {Recently, there has been significant amount of effort in developing space-efficient and succinct data structures for storing and building the traditional de Bruijn graph and its variants, including the colored de Bruijn graph. However, a problem not yet considered is developing a means to merge succinct representations of the de Bruijn graph---a challenge is necessary for constructing the de Bruijn graph on very-large datasets. We create {VARIMERGE}, for building the colored de Bruijn graph on a very-large dataset through partitioning the data into smaller subsets, building the colored de Bruijn graph using a {FM}-index based representation, and merging these representations in an iterative format. This last step is an algorithmic challenge for which we present an algorithm in this paper. Lastly, we demonstrate the utility of {VARIMERGE} by demonstrating: a four-fold reduction in working space when constructing an 8,000 color dataset, and the construction of population graph two orders of magnitude larger than previous reported methods.},
	pages = {229641},
	journaltitle = {{bioRxiv}},
	author = {Muggli, Martin D. and Boucher, Christina},
	urldate = {2018-01-08},
	date = {2017-12-06},
	langid = {english},
}

@article{pandey_mantis:_2017,
	title = {Mantis: A Fast, Small, and Exact Large-Scale Sequence Search Index},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/11/10/217372},
	doi = {10.1101/217372},
	shorttitle = {Mantis},
	abstract = {Motivation: Sequence-level searches on large collections of {RNA}-seq experiments, such as the {NIH} Sequence Read Archive ({SRA}), would enable one to ask many questions about the expression or variation of a given transcript in a population. Bloom filter-based indexes and variants, such as the Sequence Bloom Tree, have been proposed in the past to solve this problem. However, these approaches suffer from fundamental limitations of the Bloom filter, resulting in slow build and query times, less-than-optimal space usage, and large numbers of false positives. Results: This paper introduces Mantis, a space-efficient data structure that can be used to index thousands of raw-read experiments and facilitate large-scale sequence searches on those experiments. Mantis uses counting quotient filters instead of Bloom filters, enabling rapid index builds and queries, small indexes, and exact results, i.e., no false positives or negatives. Furthermore, Mantis is also a colored de Bruijn graph representation, so it supports fast graph traversal and other topological analyses in addition to large-scale sequence-level searches. In our performance evaluation, index construction with Mantis is 4.4x faster and yields a 20\% smaller index than the state-of-the-art split sequence Bloom tree ({SSBT}). For queries, Mantis is 6x-108x faster than {SSBT} and has no false positives or false negatives. For example, Mantis was able to search for all 200,400 known human transcripts in an index of 2652 human blood, breast, and brain {RNA}-seq experiments in one hour and 22 minutes; {SBT} took close to 4 days and {AllSomeSBT} took about eight hours. Mantis is written in C++11 and is available at https://github.com/splatlab/mantis.},
	pages = {217372},
	journaltitle = {{bioRxiv}},
	author = {Pandey, Prashant and Almodaresi, Fatemeh and Bender, Michael A. and Ferdman, Michael and Johnson, Rob and Patro, Rob},
	urldate = {2018-01-19},
	date = {2017-11-10},
	langid = {english},
}

@article{langmead_cloud_2018,
	title = {Cloud computing for genomic data analysis and collaboration},
	rights = {2018 Nature Publishing Group},
	issn = {1471-0064},
	url = {https://www.nature.com/articles/nrg.2017.113},
	doi = {10.1038/nrg.2017.113},
	abstract = {Next-generation sequencing has made major strides in the past decade. Studies based on large sequencing data sets are growing in number, and public archives for raw sequencing data have been doubling in size every 18 months. Leveraging these data requires researchers to use large-scale computational resources. Cloud computing, a model whereby users rent computers and storage from large data centres, is a solution that is gaining traction in genomics research. Here, we describe how cloud computing is used in genomics for research and large-scale collaborations, and argue that its elasticity, reproducibility and privacy features make it ideally suited for the large-scale reanalysis of publicly available archived data, including privacy-protected data.},
	journaltitle = {Nature Reviews Genetics},
	author = {Langmead, Ben and Nellore, Abhinav},
	urldate = {2018-02-09},
	date = {2018-01-30},
	langid = {english},
}

@article{sczyrba_critical_2017,
	title = {Critical Assessment of Metagenome Interpretation—a benchmark of metagenomics software},
	volume = {14},
	rights = {2017 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.4458},
	doi = {10.1038/nmeth.4458},
	abstract = {Methods for assembly, taxonomic profiling and binning are key to interpreting metagenome data, but a lack of consensus about benchmarking complicates performance assessment. The Critical Assessment of Metagenome Interpretation ({CAMI}) challenge has engaged the global developer community to benchmark their programs on highly complex and realistic data sets, generated from ∼700 newly sequenced microorganisms and ∼600 novel viruses and plasmids and representing common experimental setups. Assembly and genome binning programs performed well for species represented by individual genomes but were substantially affected by the presence of related strains. Taxonomic profiling and binning programs were proficient at high taxonomic ranks, with a notable performance decrease below family level. Parameter settings markedly affected performance, underscoring their importance for program reproducibility. The {CAMI} results highlight current challenges but also provide a roadmap for software selection to answer specific research questions.},
	pages = {1063--1071},
	number = {11},
	journaltitle = {Nature Methods},
	author = {Sczyrba, Alexander and Hofmann, Peter and Belmann, Peter and Koslicki, David and Janssen, Stefan and Dröge, Johannes and Gregor, Ivan and Majda, Stephan and Fiedler, Jessika and Dahms, Eik and Bremges, Andreas and Fritz, Adrian and Garrido-Oter, Ruben and Jørgensen, Tue Sparholt and Shapiro, Nicole and Blood, Philip D. and Gurevich, Alexey and Bai, Yang and Turaev, Dmitrij and {DeMaere}, Matthew Z. and Chikhi, Rayan and Nagarajan, Niranjan and Quince, Christopher and Meyer, Fernando and Balvočiūtė, Monika and Hansen, Lars Hestbjerg and Sørensen, Søren J. and Chia, Burton K. H. and Denis, Bertrand and Froula, Jeff L. and Wang, Zhong and Egan, Robert and Don Kang, Dongwan and Cook, Jeffrey J. and Deltel, Charles and Beckstette, Michael and Lemaitre, Claire and Peterlongo, Pierre and Rizk, Guillaume and Lavenier, Dominique and Wu, Yu-Wei and Singer, Steven W. and Jain, Chirag and Strous, Marc and Klingenberg, Heiner and Meinicke, Peter and Barton, Michael D. and Lingner, Thomas and Lin, Hsin-Hung and Liao, Yu-Chieh and Silva, Genivaldo Gueiros Z. and Cuevas, Daniel A. and Edwards, Robert A. and Saha, Surya and Piro, Vitor C. and Renard, Bernhard Y. and Pop, Mihai and Klenk, Hans-Peter and Göker, Markus and Kyrpides, Nikos C. and Woyke, Tanja and Vorholt, Julia A. and Schulze-Lefert, Paul and Rubin, Edward M. and Darling, Aaron E. and Rattei, Thomas and {McHardy}, Alice C.},
	urldate = {2018-02-16},
	date = {2017-11},
	langid = {english},
}

@article{mcintyre_comprehensive_2017,
	title = {Comprehensive benchmarking and ensemble approaches for metagenomic classifiers},
	volume = {18},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-017-1299-7},
	doi = {10.1186/s13059-017-1299-7},
	abstract = {One of the main challenges in metagenomics is the identification of microorganisms in clinical and environmental samples. While an extensive and heterogeneous set of computational tools is available to classify microorganisms using whole-genome shotgun sequencing data, comprehensive comparisons of these methods are limited.},
	pages = {182},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {{McIntyre}, Alexa B. R. and Ounit, Rachid and Afshinnekoo, Ebrahim and Prill, Robert J. and Hénaff, Elizabeth and Alexander, Noah and Minot, Samuel S. and Danko, David and Foox, Jonathan and Ahsanuddin, Sofia and Tighe, Scott and Hasan, Nur A. and Subramanian, Poorani and Moffat, Kelly and Levy, Shawn and Lonardi, Stefano and Greenfield, Nick and Colwell, Rita R. and Rosen, Gail L. and Mason, Christopher E.},
	urldate = {2018-02-16},
	date = {2017-09-21},
	keywords = {Metagenomics, Classification, Comparison, Ensemble methods, Meta-classification, Pathogen detection, Shotgun sequencing, Taxonomy},
}

@inproceedings{indyk_approximate_1998,
	location = {New York, {NY}, {USA}},
	title = {Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality},
	isbn = {978-0-89791-962-3},
	url = {http://doi.acm.org/10.1145/276698.276876},
	doi = {10.1145/276698.276876},
	series = {{STOC} '98},
	shorttitle = {Approximate Nearest Neighbors},
	pages = {604--613},
	booktitle = {Proceedings of the Thirtieth Annual {ACM} Symposium on Theory of Computing},
	publisher = {{ACM}},
	author = {Indyk, Piotr and Motwani, Rajeev},
	urldate = {2018-03-06},
	date = {1998},
}

@article{flajolet_birthday_1992,
	title = {Birthday Paradox, Coupon Collectors, Caching Algorithms and Self-Organizing Search},
	volume = {39},
	doi = {10.1016/0166-218X(92)90177-C},
	abstract = {This paper introduces a unified framework for the analysis of a class of random allocation processes thdt include: (i) the birthday paradox; (ii) the coupon collector problem* (iii) least-tecently-used ({LRU}) caching in memory management systems under the independent reference model; (iv) the move-to-front heuristic of self-organizing search. All analyses are relative to general nonuniform f robability distributions. Our approach to these problems comprises two stages. First, the probabilistic phenozna ct’ interest are described by means of regular languages extended by addition of the shu!‘Re product. Next, systematic translation mechanisms are used to derive integral representations for expectations and probability distributions.},
	pages = {207--229},
	journaltitle = {Discrete Applied Mathematics},
	author = {Flajolet, Philippe and Gardy, Danièle and Thimonier, Loÿs},
	date = {1992},
}

@article{downey_computational_1999,
	title = {Computational tractability: The view from mars},
	volume = {69},
	shorttitle = {Computational tractability},
	pages = {73--97},
	journaltitle = {Bulletin of the {EATCS}},
	author = {Downey, Rodney G. and Fellows, Michael R. and Stege, Ulrike},
	date = {1999},
}

@report{sparka_p2kmv:_2018,
	title = {P2KMV: A Privacy-preserving Counting Sketch for Efficient and Accurate Set Intersection Cardinality Estimations},
	url = {http://eprint.iacr.org/2018/234},
	shorttitle = {P2KMV},
	abstract = {In this paper, we propose P2KMV, a novel privacy-preserving counting sketch, based on the k minimum values algorithm. With P2KMV, we offer a versatile privacy-enhanced technology for obtaining statistics, following the principle of data minimization, and aiming for the sweet spot between privacy, accuracy, and computational efficiency. As our main contribution, we develop methods to perform set operations, which facilitate cardinality estimates under strong privacy requirements. Most notably, we propose an efficient, privacy-preserving algorithm to estimate the set intersection cardinality. P2KMV provides plausible deniability for all data items contained in the sketch. We discuss the algorithm's privacy guarantees as well as the accuracy of the obtained estimates. An experimental evaluation confirms our analytical expectations and provides insights regarding parameter choices.},
	number = {234},
	author = {Sparka, Hagen and Tschorsch, Florian and Scheuermann, Björn},
	urldate = {2018-04-02},
	date = {2018},
	keywords = {applications, privacy},
}

@article{ju_tahcoroll:_2017,
	title = {{TahcoRoll}: An Efficient Approach for Signature Profiling in Genomic Data through Variable-Length k-mers},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/12/06/229708},
	doi = {10.1101/229708},
	shorttitle = {{TahcoRoll}},
	abstract = {K-mer profiling has been one of the trending approaches to analyze read data generated by high-throughput sequencing technologies. The tasks of k-mer profiling include, but are not limited to, counting the frequencies and determining the occurrences of short sequences in a dataset. The notion of k-mer has been extensively used to build de Bruijn graphs in genome or transcriptome assembly, which requires examining all possible k-mers presented in the dataset. Recently, an alternative way of profiling has been proposed, which constructs a set of representative k-mers as genomic markers and profiles their occurrences in the sequencing data. This technique has been applied in both transcript quantification through {RNA}-Seq and taxonomic classification of metagenomic reads. Most of these applications use a set of fixed-size k-mers since the majority of existing k-mer counters are inadequate to process genomic sequences with variable-length k-mers. However, choosing the appropriate k is challenging, as it varies for different applications. As a pioneer work to profile a set of variable-length k-mers, we propose {TahcoRoll} in order to enhance the Aho-Corasick algorithm. More specifically, we use one bit to represent each nucleotide, and integrate the rolling hash technique to construct an efficient in-memory data structure for this task. Using both synthetic and real datasets, results show that {TahcoRoll} outperforms existing approaches in either or both time and memory efficiency without using any disk space. In addition, compared to the most efficient state-of-the-art k-mer counters, such as {KMC} and {MSBWT}, {TahcoRoll} is the only approach that can process long read data from both {PacBio} and Oxford Nanopore on a commodity desktop computer. The source code of {TahcoRoll} is implemented in C++14, and available at https://github.com/chelseaju/{TahcoRoll}.git.},
	pages = {229708},
	journaltitle = {{bioRxiv}},
	author = {Ju, Chelsea Jui-Ting and Jiang, Jyun-Yu and Li, Ruirui and Li, Zeyu and Wang, Wei},
	urldate = {2018-04-02},
	date = {2017-12-06},
	langid = {english},
}

@article{ertl_new_2017,
	title = {New cardinality estimation algorithms for {HyperLogLog} sketches},
	url = {http://arxiv.org/abs/1702.01284},
	abstract = {This paper presents new methods to estimate the cardinalities of data sets recorded by {HyperLogLog} sketches. A theoretically motivated extension to the original estimator is presented that eliminates the bias for small and large cardinalities. Based on the maximum likelihood principle a second unbiased method is derived together with a robust and efficient numerical algorithm to calculate the estimate. The maximum likelihood approach can also be applied to more than a single {HyperLogLog} sketch. In particular, it is shown that it gives more precise cardinality estimates for union, intersection, or relative complements of two sets that are both represented by {HyperLogLog} sketches compared to the conventional technique using the inclusion-exclusion principle. All the new methods are demonstrated and verified by extensive simulations.},
	journaltitle = {{arXiv}:1702.01284 [cs]},
	author = {Ertl, Otmar},
	urldate = {2018-04-02},
	date = {2017-02-04},
	eprinttype = {arxiv},
	eprint = {1702.01284},
	keywords = {Computer Science - Data Structures and Algorithms, 68W15, 68W25, 62-07, E.1, H.2.8, I.1.2},
}

@article{deorowicz_kmer-db:_2018,
	title = {Kmer-db: instant evolutionary distance estimation},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/early/2018/02/12/263590},
	doi = {10.1101/263590},
	shorttitle = {Kmer-db},
	abstract = {Summary: Kmer-db is a new tool for estimating evolutionary relationship on the basis of k-mers extracted from genomes or sequencing reads. Thanks to an efficient data structure and parallel implementation, our software estimates distances between 40,715 pathogens in less than 4 minutes (on a modern workstation), 44 times faster than Mash, its main competitor. Availability and Implementation: https://github.com/refresh-bio/kmer-db},
	pages = {263590},
	journaltitle = {{bioRxiv}},
	author = {Deorowicz, Sebastian and Gudys, Adam and Dlugosz, Maciej and Kokot, Marek and Danek, Agnieszka},
	urldate = {2018-04-02},
	date = {2018-02-12},
	langid = {english},
}

@article{zhu_lsh_2016,
	title = {{LSH} ensemble: internet-scale domain search},
	volume = {9},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=2994509.2994534},
	doi = {10.14778/2994509.2994534},
	shorttitle = {{LSH} ensemble},
	abstract = {We study the problem of domain search where a domain is a set of distinct values from an unspeciﬁed universe. We use Jaccard set containment score, deﬁned as {\textbar}Q ∩ X{\textbar}/{\textbar}Q{\textbar}, as the measure of relevance of a domain X to a query domain Q. Our choice of Jaccard set containment over Jaccard similarity as a measure of relevance makes our work particularly suitable for searching Open Data and data on the web, as Jaccard similarity is known to have poor performance over sets with large diﬀerences in their domain sizes. We demonstrate that the domains found in several real-life Open Data and web data repositories show a power-law distribution over their domain sizes.},
	pages = {1185--1196},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	author = {Zhu, Erkang and Nargesian, Fatemeh and Pu, Ken Q. and Miller, Renée J.},
	urldate = {2018-04-02},
	date = {2016-08-01},
	langid = {english},
}

@article{wang_hashing_2014,
	title = {Hashing for Similarity Search: A Survey},
	url = {http://arxiv.org/abs/1408.2927},
	shorttitle = {Hashing for Similarity Search},
	abstract = {Similarity search (nearest neighbor search) is a problem of pursuing the data items whose distances to a query item are the smallest from a large database. Various methods have been developed to address this problem, and recently a lot of efforts have been devoted to approximate search. In this paper, we present a survey on one of the main solutions, hashing, which has been widely studied since the pioneering work locality sensitive hashing. We divide the hashing algorithms two main categories: locality sensitive hashing, which designs hash functions without exploring the data distribution and learning to hash, which learns hash functions according the data distribution, and review them from various aspects, including hash function design and distance measure and search scheme in the hash coding space.},
	journaltitle = {{arXiv}:1408.2927 [cs]},
	author = {Wang, Jingdong and Shen, Heng Tao and Song, Jingkuan and Ji, Jianqiu},
	urldate = {2018-04-02},
	date = {2014-08-13},
	eprinttype = {arxiv},
	eprint = {1408.2927},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases, Computer Science - Computer Vision and Pattern Recognition},
}

@article{shrivastava_optimal_2017,
	title = {Optimal Densification for Fast and Accurate Minwise Hashing},
	url = {http://arxiv.org/abs/1703.04664},
	abstract = {Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification{\textasciitilde}{\textbackslash}cite\{Proc:{OneHashLSH}\_ICML14,Proc:Shrivastava\_UAI14\} have shown that it is possible to compute \$k\$ minwise hashes, of a vector with \$d\$ nonzeros, in mere \$(d + k)\$ computations, a significant improvement over the classical \$O(dk)\$. These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.},
	journaltitle = {{arXiv}:1703.04664 [cs]},
	author = {Shrivastava, Anshumali},
	urldate = {2018-04-02},
	date = {2017-03-14},
	eprinttype = {arxiv},
	eprint = {1703.04664},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Learning},
}

@article{jiang_comparison_2012,
	title = {Comparison of metagenomic samples using sequence signatures},
	volume = {13},
	issn = {1471-2164},
	url = {https://doi.org/10.1186/1471-2164-13-730},
	doi = {10.1186/1471-2164-13-730},
	abstract = {Sequence signatures, as defined by the frequencies of k-tuples (or k-mers, k-grams), have been used extensively to compare genomic sequences of individual organisms, to identify cis-regulatory modules, and to study the evolution of regulatory sequences. Recently many next-generation sequencing ({NGS}) read data sets of metagenomic samples from a variety of different environments have been generated. The assembly of these reads can be difficult and analysis methods based on mapping reads to genes or pathways are also restricted by the availability and completeness of existing databases. Sequence-signature-based methods, however, do not need the complete genomes or existing databases and thus, can potentially be very useful for the comparison of metagenomic samples using {NGS} read data. Still, the applications of sequence signature methods for the comparison of metagenomic samples have not been well studied.},
	pages = {730},
	journaltitle = {{BMC} Genomics},
	shortjournal = {{BMC} Genomics},
	author = {Jiang, Bai and Song, Kai and Ren, Jie and Deng, Minghua and Sun, Fengzhu and Zhang, Xuegong},
	urldate = {2018-04-02},
	date = {2012-12-27},
	keywords = {Dissimilarity Measure, Galapagos Island, Metagenomic Dataset, Metagenomic Sample, Sequencing Depth},
}

@article{aflitos_cnidaria:_2015,
	title = {Cnidaria: fast, reference-free clustering of raw and assembled genome and transcriptome {NGS} data},
	volume = {16},
	issn = {1471-2105},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4630969/},
	doi = {10.1186/s12859-015-0806-7},
	shorttitle = {Cnidaria},
	abstract = {Background
Identification of biological specimens is a requirement for a range of applications. Reference-free methods analyse unprocessed sequencing data without relying on prior knowledge, but generally do not scale to arbitrarily large genomes and arbitrarily large phylogenetic distances.

Results
We present Cnidaria, a practical tool for clustering genomic and transcriptomic data with no limitation on genome size or phylogenetic distances. We successfully simultaneously clustered 169 genomic and transcriptomic datasets from 4 kingdoms, achieving 100 \% identification accuracy at supra-species level and 78 \% accuracy at the species level.

Conclusion
{CNIDARIA} allows for fast, resource-efficient comparison and identification of both raw and assembled genome and transcriptome data. This can help answer both fundamental (e.g. in phylogeny, ecological diversity analysis) and practical questions (e.g. sequencing quality control, primer design).

Electronic supplementary material
The online version of this article (doi:10.1186/s12859-015-0806-7) contains supplementary material, which is available to authorized users.},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Aflitos, Saulo Alves and Severing, Edouard and Sanchez-Perez, Gabino and Peters, Sander and de Jong, Hans and de Ridder, Dick},
	urldate = {2018-04-02},
	date = {2015-11-02},
	pmid = {26525298},
	pmcid = {PMC4630969},
}

@article{mustafa_dynamic_2018,
	title = {Dynamic compression schemes for graph coloring},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/03/17/239806},
	doi = {10.1101/239806},
	abstract = {Technological advancements in high-throughput {DNA} sequencing have led to an exponential growth of sequencing data being produced and stored as a byproduct of biomedical research. Despite its public availability, a majority of this data remains hard to query to the research community due to a lack of efficient data representation and indexing solutions. One of the available techniques to represent read data is a condensed form as an assembly graph. Such a representation contains all sequence information but does not store contextual information and metadata. We present two new approaches for a compressed representation of a graph coloring: a lossless compression scheme based on a novel application of wavelet tries as well as a highly accurate lossy compression based on a set of Bloom filters. Both strategies retain a coloring with dynamically changing graph topology. We present construction and merge procedures for both methods and evaluate their performance on a wide range of different datasets. By dropping the requirement of a fully lossless compression and using the topological information of the underlying graph, we can reduce memory requirements by up to three orders of magnitude. Representing individual colors as independently stored modules, our approaches are fully dynamic and can be efficiently parallelized. These properties allow for an easy upscaling to the problem sizes common to the biomedical domain. We provide prototype implementations in C++, summaries of our experiments as well as links to all datasets publicly at https://github.com/ratschlab/graph\_annotation.},
	pages = {239806},
	journaltitle = {{bioRxiv}},
	author = {Mustafa, Harun and Schilken, Ingo and Karasikov, Mikhail and Eickhoff, Carsten and Ratsch, Gunnar and Kahles, Andre},
	urldate = {2018-04-02},
	date = {2018-03-17},
	langid = {english},
}

@article{zappia_clustering_2018,
	title = {Clustering trees: a visualisation for evaluating clusterings at multiple resolutions},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/05/22/274035},
	doi = {10.1101/274035},
	shorttitle = {Clustering trees},
	abstract = {Clustering techniques are widely used in the analysis of large data sets to group together samples with similar properties. For example, clustering is often used in the field of single-cell {RNA}- sequencing in order to identify different cell types present in a tissue sample. There are many algorithms for performing clustering and the results can vary substantially. In particular, the number of groups present in a data set is often unknown and the number of clusters identified by an algorithm can change based on the parameters used. To explore and examine the impact of varying clustering resolution we present clustering trees. This visualisation shows the relationships between clusters at multiple resolutions allowing researchers to see how samples move as the number of clusters increases. In addition, meta-information can be overlaid on the tree to inform the choice of resolution and guide in identification of clusters. We illustrate the features of clustering trees using a series of simulations as well as two real examples, the classical iris dataset and a complex single-cell {RNA}-sequencing dataset. Clustering trees can be produced using the clustree R package available from {CRAN} (https://{CRAN}.R-project.org/package=clustree) and developed on {GitHub} (https://github.com/lazappi/clustree).},
	pages = {274035},
	journaltitle = {{bioRxiv}},
	author = {Zappia, Luke and Oshlack, Alicia},
	urldate = {2018-06-18},
	date = {2018-05-22},
	langid = {english},
	keywords = {sbt},
}

@online{bovee_finch:_2018,
	title = {Finch: a tool adding dynamic abundance filtering to genomic {MinHashing}},
	url = {http://joss.theoj.org},
	shorttitle = {Finch},
	abstract = {The Journal of Open Source Software, a {\textless}strong{\textgreater}developer friendly{\textless}/strong{\textgreater} journal for research software packages.},
	titleaddon = {The Journal of Open Source Software},
	author = {Bovee, Roderick and Greenfield, Nick},
	urldate = {2018-06-26},
	date = {2018-02-01},
	langid = {english},
	doi = {10.21105/joss.00505},
}

@article{kucherov_algorithms_2018,
	title = {Algorithms for biosequence search: past, present and future},
	url = {http://arxiv.org/abs/1808.01038},
	shorttitle = {Algorithms for biosequence search},
	abstract = {The paper surveys the evolution of main algorithmic ideas used to compare and search biological sequences, including current trends and future prospects.},
	journaltitle = {{arXiv}:1808.01038 [q-bio]},
	author = {Kucherov, Gregory},
	urldate = {2018-08-07},
	date = {2018-08-02},
	eprinttype = {arxiv},
	eprint = {1808.01038},
	keywords = {Quantitative Biology - Genomics},
}

@inproceedings{yang_histosketch:_2018,
	title = {{HistoSketch}: Fast Similarity-Preserving Sketching of Streaming Histograms with Concept Drift},
	isbn = {978-1-5386-3835-4},
	url = {doi.ieeecomputersociety.org/10.1109/ICDM.2017.64},
	doi = {10.1109/ICDM.2017.64},
	shorttitle = {{HistoSketch}},
	abstract = {Histogram-based similarity has been widely adopted in many machine learning tasks. However, measuring histogram similarity is a challenging task for streaming data, where the elements of a histogram are observed in a streaming manner. First, the ever-growing cardinality of histogram elements makes any similarity computation inefficient. Second, the concept-drift issue in the data streams also impairs the accurate assessment of the similarity. In this paper, we propose to overcome the above challenges with {HistoSketch}, a fast similarity-preserving sketching method for streaming histograms with concept drift. Specifically, {HistoSketch} is designed to incrementally maintain a set of compact and fixed-size sketches of streaming histograms to approximate similarity between the histograms, with the special consideration of gradually forgetting the outdated histogram elements. We evaluate {HistoSketch} on multiple classification tasks using both synthetic and real-world datasets. The results show that our method is able to efficiently approximate similarity for streaming histograms and quickly adapt to concept drift. Compared to full streaming histograms gradually forgetting the outdated histogram elements, {HistoSketch} is able to dramatically reduce the classification time (with a 7500x speedup) with only a modest loss in accuracy (about 3.5\%).},
	pages = {545--554},
	booktitle = {2017 {IEEE} International Conference on Data Mining ({ICDM})},
	author = {Yang, D. and Li, B. and Rettig, L. and Cudre-Mauroux, P.},
	urldate = {2018-08-08},
	date = {2018-11},
	keywords = {learning (artificial intelligence), pattern classification},
}

@article{piro_ganon:_2018,
	title = {ganon: continuously up-to-date with database growth for precise short read classification in metagenomics},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/early/2018/08/31/406017},
	doi = {10.1101/406017},
	shorttitle = {ganon},
	abstract = {The exponential growth of assembled genome sequences greatly benefits metagenomic studies, providing a broader catalog of reference organisms in a variety of studies. However, due to the massive amount of sequences, tools are struggling to manage such data and their frequent updates. Processing and indexing currently available repositories is no longer possible on standard infrastructures and can take days and hundreds of {GB} even on large servers. Few methods have acknowledged such issues thus far and even though many can theoretically handle large amounts of references, time-memory requirements are prohibitive for a real application. As a result, many tools still rely on static outdated datasets and clearly underperform under the currently available wealth of genome sequences. The content and taxonomic distribution of such databases is also a crucial factor, introducing bias when not properly managed. Motivated by those limitations we created ganon, a k-mer based short read classification tool which uses Interleaved Bloom Filters in conjunction with a taxonomic clustering and a k-mer counting-filtering scheme. Ganon provides an efficient method for indexing references and keeping them updated, requiring under 25 minutes to index the complete (i.e. high quality) archaeal-bacterial genomes and under 3 hours for all archaeal-bacterial genomes (30GB and 380GB of raw data, respectively). Ganon can keep such indices up-to-date in a fraction of the time necessary to create them allowing researchers to permanently work on the most recent references. Using data from the {CAMI} challenge, ganon shows strongly improved precision (in some cases more than double) while having equal or better sensitivity compared to state-of-the-art tools. Ganon's memory footprint is less than half of similar tools using the same reference set. Using a larger set of references with a broader diversity, ganon classified 25\% more reads achieving 97\% precision at species level. Ganon supports taxonomy and assembly level classification as well as multiple indices for a hierarchical classification. The software is open-source and available at: https://gitlab.com/rki\_bioinformatics/ganon},
	pages = {406017},
	journaltitle = {{bioRxiv}},
	author = {Piro, Vitor C. and Dadi, Temesgen Hailemariam and Seiler, Enrico and Reinert, Knut and Renard, Bernhard Y.},
	urldate = {2018-09-01},
	date = {2018-08-31},
	langid = {english},
}

@article{rowe_streaming_2018,
	title = {Streaming histogram sketching for rapid microbiome analytics},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/09/04/408070},
	doi = {10.1101/408070},
	abstract = {Motivation: The growth in publically available microbiome data in recent years has yielded an invaluable resource for genomic research; allowing for the design of new studies, augmentation of novel datasets and reanalysis of published works. This vast amount of microbiome data, as well as the widespread proliferation of microbiome research and the looming era of clinical metagenomics, means there is an urgent need to develop analytics that can process huge amounts of data in a short amount of time. To address this need, we propose a new method for the compact representation of microbiome sequencing data using similarity-preserving sketches of streaming k-mer spectra. These sketches allow for dissimilarity estimation, rapid microbiome catalogue searching, and classification of microbiome samples in near real-time. Results: We apply streaming histogram sketching to microbiome samples as a form of dimensionality reduction, creating a compressed "histosketch" that can be used to efficiently represent microbiome k-mer spectra. Using public microbiome datasets, we show that histosketches can be clustered by sample type using pairwise Jaccard similarity estimation, consequently allowing for rapid microbiome similarity searches via a locality sensitive hashing indexing scheme. Furthermore, we show that histosketches can be used to train machine learning classifiers to accurately label microbiome samples. Specifically, using a collection of 108 novel microbiome samples from a cohort of premature neonates, we trained and tested a Random Forest Classifier that could accurately predict whether the neonate had received antibiotic treatment (95\% accuracy, precision 97\%) and could subsequently be used to classify microbiome data streams in less than 12 seconds. We provide our implementation, Histosketching Using Little K-mers ({HULK}), which can histosketch a typical 2GB microbiome in 50 seconds on a standard laptop using 4 cores, with the sketch occupying 3000 bytes of disk space. Availability: Our implementation ({HULK}) is written in Go and is available at: https://github.com/will-rowe/hulk ({MIT} License).},
	pages = {408070},
	journaltitle = {{bioRxiv}},
	author = {Rowe, Will {PM} and Carrieri, Anna Paola and Alcon-Giner, Cristina and Caim, Shabhonam and Shaw, Alex and Sim, Kathleen and Kroll, J. Simon and Hall, Lindsay and Pyzer-Knapp, Edward O. and Winn, Martyn D.},
	urldate = {2018-09-05},
	date = {2018-09-04},
	langid = {english},
}

@article{wood_kraken:_2014,
	title = {Kraken: ultrafast metagenomic sequence classification using exact alignments},
	volume = {15},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/gb-2014-15-3-r46},
	doi = {10.1186/gb-2014-15-3-r46},
	shorttitle = {Kraken},
	abstract = {Kraken is an ultrafast and highly accurate program for assigning taxonomic labels to metagenomic {DNA} sequences. Previous programs designed for this task have been relatively slow and computationally expensive, forcing researchers to use faster abundance estimation programs, which only classify small subsets of metagenomic data. Using exact alignment of k-mers, Kraken achieves classification accuracy comparable to the fastest {BLAST} program. In its fastest mode, Kraken classifies 100 base pair reads at a rate of over 4.1 million reads per minute, 909 times faster than Megablast and 11 times faster than the abundance estimation program {MetaPhlAn}. Kraken is available at http://ccb.jhu.edu/software/kraken/.},
	pages = {R46},
	number = {3},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {Wood, Derrick E. and Salzberg, Steven L.},
	urldate = {2018-09-22},
	date = {2014-03-03},
}

@article{freelon_computational_2018,
	title = {Computational research in the post-{API} age},
	url = {https://osf.io/preprints/socarxiv/56f4q/},
	doi = {10.31235/osf.io/56f4q},
	abstract = {Briefly describes the implications of the emerging post-{API} age for social science research methods.},
	journaltitle = {{SocArXiv}},
	author = {Freelon, Deen},
	urldate = {2018-09-28},
	date = {2018-08-20},
}

@online{noauthor_k-mer_nodate,
	title = {k-mer counting, part I: Introduction {\textbar} {BioInfoLogics}},
	url = {https://bioinfologics.github.io/post/2018/09/17/k-mer-counting-part-i-introduction/},
	urldate = {2018-09-28},
}

@article{chu_improving_2018,
	title = {Improving on hash-based probabilistic sequence classification using multiple spaced seeds and multi-index Bloom filters},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/10/05/434795},
	doi = {10.1101/434795},
	abstract = {Alignment-free classification of sequences against collections of sequences has enabled high-throughput processing of sequencing data in many bioinformatics analysis pipelines. Originally hash-table based, much work has been done to improve and reduce the memory requirement of indexing of k-mer sequences with probabilistic indexing strategies. These efforts have led to lower memory highly efficient indexes, but often lack sensitivity in the face of sequencing errors or polymorphism because they are k-mer based. To address this, we designed a new memory efficient data structure that can tolerate mismatches using multiple spaced seeds, called a multi-index Bloom Filter. Implemented as part of {BioBloom} Tools, we demonstrate our algorithm in two applications, read binning for targeted assembly and taxonomic read assignment. Our tool shows a higher sensitivity and specificity for read-binning than {BWA} {MEM} at an order of magnitude less time. For taxonomic classification, we show higher sensitivity than {CLARK}-S at an order of magnitude less time while using half the memory.},
	pages = {434795},
	journaltitle = {{bioRxiv}},
	author = {Chu, Justin and Mohamadi, Hamid and Erhan, Emre and Tse, Jeffery and Chiu, Readman and Yeo, Sarah and Birol, Inanc},
	urldate = {2018-10-05},
	date = {2018-10-05},
	langid = {english},
}

@article{lemire_roaring_2018,
	title = {Roaring Bitmaps: Implementation of an Optimized Software Library},
	volume = {48},
	issn = {00380644},
	url = {http://arxiv.org/abs/1709.07821},
	doi = {10.1002/spe.2560},
	shorttitle = {Roaring Bitmaps},
	abstract = {Compressed bitmap indexes are used in systems such as Git or Oracle to accelerate queries. They represent sets and often support operations such as unions, intersections, differences, and symmetric differences. Several important systems such as Elasticsearch, Apache Spark, Netflix's Atlas, {LinkedIn}'s Pivot, Metamarkets' Druid, Pilosa, Apache Hive, Apache Tez, Microsoft Visual Studio Team Services and Apache Kylin rely on a specific type of compressed bitmap index called Roaring. We present an optimized software library written in C implementing Roaring bitmaps: {CRoaring}. It benefits from several algorithms designed for the single-instruction-multiple-data ({SIMD}) instructions available on commodity processors. In particular, we present vectorized algorithms to compute the intersection, union, difference and symmetric difference between arrays. We benchmark the library against a wide range of competitive alternatives, identifying weaknesses and strengths in our software. Our work is available under a liberal open-source license.},
	pages = {867--895},
	number = {4},
	journaltitle = {Software: Practice and Experience},
	author = {Lemire, Daniel and Kaser, Owen and Kurz, Nathan and Deri, Luca and O'Hara, Chris and Saint-Jacques, François and Ssi-Yan-Kai, Gregory},
	urldate = {2018-10-07},
	date = {2018-04},
	eprinttype = {arxiv},
	eprint = {1709.07821},
	keywords = {Computer Science - Databases},
}

@online{noauthor_dream-yara:_nodate,
	title = {{DREAM}-Yara: an exact read mapper for very large databases with short update time {\textbar} Bioinformatics {\textbar} Oxford Academic},
	url = {https://academic.oup.com/bioinformatics/article/34/17/i766/5093228},
	urldate = {2018-10-10},
}

@article{piro_ganon:_2018-1,
	title = {ganon: continuously up-to-date with database growth for precise short read classification in metagenomics},
	url = {http://biorxiv.org/lookup/doi/10.1101/406017},
	doi = {10.1101/406017},
	shorttitle = {ganon},
	abstract = {The exponential growth of assembled genome sequences greatly benefits metagenomic studies, providing a broader catalog of reference organisms in a variety of studies. However, due to the massive amount of sequences, tools are struggling to manage such data and their frequent updates. Processing and indexing currently available repositories is no longer possible on standard infrastructures and can take days and hundreds of {GB} even on large servers. Few methods have acknowledged such issues thus far and even though many can theoretically handle large amounts of references, time-memory requirements are prohibitive for a real application. As a result, many tools still rely on static outdated datasets and clearly underperform under the currently available wealth of genome sequences. The content and taxonomic distribution of such databases is also a crucial factor, introducing bias when not properly managed. Motivated by those limitations we created ganon, a k-mer based short read classification tool which uses Interleaved Bloom Filters in conjunction with a taxonomic clustering and a k-mer counting-filtering scheme. Ganon provides an efficient method for indexing references and keeping them updated, requiring under 25 minutes to index the complete (i.e. high quality) archaeal-bacterial genomes and under 3 hours for all archaeal-bacterial genomes (30GB and 380GB of raw data, respectively). Ganon can keep such indices up-to-date in a fraction of the time necessary to create them allowing researchers to permanently work on the most recent references. Using data from the {CAMI} challenge, ganon shows strongly improved precision (in some cases more than double) while having equal or better sensitivity compared to state-of-the-art tools. Ganon's memory footprint is less than half of similar tools using the same reference set. Using a larger set of references with a broader diversity, ganon classified 25\% more reads achieving 97\% precision at species level. Ganon supports taxonomy and assembly level classification as well as multiple indices for a hierarchical classification. The software is open-source and available at: https://gitlab.com/rki\_bioinformatics/ganon},
	author = {Piro, Vitor C and Dadi, Temesgen Hailemariam and Seiler, Enrico and Reinert, Knut and Renard, Bernhard Y},
	urldate = {2018-10-10},
	date = {2018-08-31},
	langid = {english},
}

@online{noauthor_using_nodate,
	title = {Using Multidimensional Bloom filters to Search {RNAseq} Libraries - (i)},
	url = {http://homolog.us/blogs/blog/2016/12/13/multidimensional-bloom-i/},
	urldate = {2018-10-10},
}

@article{zhao_bindash_nodate,
	title = {{BinDash}, software for fast genome distance estimation on a typical personal laptop},
	url = {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty651/5058094},
	doi = {10.1093/bioinformatics/bty651},
	abstract = {{AbstractMotivation}.  The number of genomes (including meta-genomes) is increasing at an accelerating pace. In the near future, we may need to estimate pairwise},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Zhao, {XiaoFei}},
	urldate = {2018-10-10},
	langid = {english},
}

@article{sivadasan_kmerlight:_2016,
	title = {Kmerlight: fast and accurate k-mer abundance estimation},
	url = {http://arxiv.org/abs/1609.05626},
	shorttitle = {Kmerlight},
	abstract = {k-mers (nucleotide strings of length k) form the basis of several algorithms in computational genomics. In particular, k-mer abundance information in sequence data is useful in read error correction, parameter estimation for genome assembly, digital normalization etc. We give a streaming algorithm Kmerlight for computing the k-mer abundance histogram from sequence data. Our algorithm is fast and uses very small memory footprint. We provide analytical bounds on the error guarantees of our algorithm. Kmerlight can efficiently process genome scale and metagenome scale data using standard desktop machines. Few applications of abundance histograms computed by Kmerlight are also shown. We use abundance histogram for de novo estimation of repetitiveness in the genome based on a simple probabilistic model that we propose. We also show estimation of k-mer error rate in the sampling using abundance histogram. Our algorithm can also be used for abundance estimation in a general streaming setting. The Kmerlight tool is written in C++ and is available for download and use from https://github.com/nsivad/kmerlight.},
	journaltitle = {{arXiv}:1609.05626 [cs]},
	author = {Sivadasan, Naveen and Srinivasan, Rajgopal and Goyal, Kshama},
	urldate = {2018-10-15},
	date = {2016-09-19},
	eprinttype = {arxiv},
	eprint = {1609.05626},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@online{noauthor_hulk_nodate,
	title = {Hulk and histosketch - Daniel's blog},
	url = {https://standage.github.io/a-brief-review-of-hulk-and-histosketch.html},
	urldate = {2018-10-19},
}

@article{nasko_refseq_2018,
	title = {{RefSeq} database growth influences the accuracy of k-mer-based lowest common ancestor species identification},
	volume = {19},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-018-1554-6},
	doi = {10.1186/s13059-018-1554-6},
	abstract = {In order to determine the role of the database in taxonomic sequence classification, we examine the influence of the database over time on k-mer-based lowest common ancestor taxonomic classification. We present three major findings: the number of new species added to the {NCBI} {RefSeq} database greatly outpaces the number of new genera; as a result, more reads are classified with newer database versions, but fewer are classified at the species level; and Bayesian-based re-estimation mitigates this effect but struggles with novel genomes. These results suggest a need for new classification approaches specially adapted for large databases.},
	pages = {165},
	number = {1},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {Nasko, Daniel J. and Koren, Sergey and Phillippy, Adam M. and Treangen, Todd J.},
	urldate = {2018-10-30},
	date = {2018-10-30},
}

@article{kirk_functional_2018,
	title = {Functional classification of long non-coding {RNAs} by k -mer content},
	volume = {50},
	rights = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/s41588-018-0207-8},
	doi = {10.1038/s41588-018-0207-8},
	abstract = {{SEEKR} is a method that deconstructs linear sequence relationships between {lncRNAs} and evaluates similarity on the basis of abundance of short motifs called k-mers. {LncRNAs} of related function often have similar k-mer profiles despite lacking linear homology.},
	pages = {1474},
	number = {10},
	journaltitle = {Nature Genetics},
	author = {Kirk, Jessime M. and Kim, Susan O. and Inoue, Kaoru and Smola, Matthew J. and Lee, David M. and Schertzer, Megan D. and Wooten, Joshua S. and Baker, Allison R. and Sprague, Daniel and Collins, David W. and Horning, Christopher R. and Wang, Shuo and Chen, Qidi and Weeks, Kevin M. and Mucha, Peter J. and Calabrese, J. Mauro},
	urldate = {2018-10-31},
	date = {2018-10},
}

@article{bar-joseph_fast_2001,
	title = {Fast optimal leaf ordering for hierarchical clustering},
	volume = {17},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/17/suppl_1/S22/261423},
	doi = {10.1093/bioinformatics/17.suppl_1.S22},
	abstract = {Abstract.  We present the first practical algorithm for the optimal
 linear leaf ordering of trees that are generated by hierarchical
 clustering. Hierarchical},
	pages = {S22--S29},
	issue = {suppl\_1},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Bar-Joseph, Ziv and Gifford, David K. and Jaakkola, Tommi S.},
	urldate = {2018-10-31},
	date = {2001-06-01},
	langid = {english},
}

@article{mullner_modern_2011,
	title = {Modern hierarchical, agglomerative clustering algorithms},
	url = {http://arxiv.org/abs/1109.2378},
	abstract = {This paper presents algorithms for hierarchical, agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a "stepwise dendrogram", a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting efficiently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes.},
	journaltitle = {{arXiv}:1109.2378 [cs, stat]},
	author = {Müllner, Daniel},
	urldate = {2018-10-31},
	date = {2011-09-12},
	eprinttype = {arxiv},
	eprint = {1109.2378},
	keywords = {Computer Science - Data Structures and Algorithms, Statistics - Machine Learning, 62H30, I.5.3},
}

@article{popic_hybrid_2017,
	title = {A hybrid cloud read aligner based on {MinHash} and kmer voting that preserves privacy},
	volume = {8},
	issn = {2041-1723},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5440850/},
	doi = {10.1038/ncomms15311},
	abstract = {Low-cost clouds can alleviate the compute and storage burden of the genome sequencing data explosion. However, moving personal genome data analysis to the cloud can raise serious privacy concerns. Here, we devise a method named Balaur, a privacy preserving read mapper for hybrid clouds based on locality sensitive hashing and kmer voting. Balaur can securely outsource a substantial fraction of the computation to the public cloud, while being highly competitive in accuracy and speed with non-private state-of-the-art read aligners on short read data. We also show that the method is significantly faster than the state of the art in long read mapping. Therefore, Balaur can enable institutions handling massive genomic data sets to shift part of their analysis to the cloud without sacrificing accuracy or exposing sensitive information to an untrusted third party., Outsourcing computation for genomic data processing offers the ability to allocate massive computing power and storage on demand. Here, Popic and Batzoglou develop a hybrid cloud aligner for sequence read mapping that preserves privacy with competitive accuracy and speed.},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Popic, Victoria and Batzoglou, Serafim},
	urldate = {2018-11-01},
	date = {2017-05-16},
	pmid = {28508884},
	pmcid = {PMC5440850},
}

@inproceedings{leiserson_work-efficient_2010,
	location = {Thira, Santorini, Greece},
	title = {A work-efficient parallel breadth-first search algorithm (or how to cope with the nondeterminism of reducers)},
	isbn = {978-1-4503-0079-7},
	url = {http://portal.acm.org/citation.cfm?doid=1810479.1810534},
	doi = {10.1145/1810479.1810534},
	abstract = {We have developed a multithreaded implementation of breadth-ﬁrst search ({BFS}) of a sparse graph using the Cilk++ extensions to C++. Our {PBFS} program on a single processor runs as quickly as a standard C++ breadth-ﬁrst search implementation. {PBFS} achieves high work-efﬁciency by using a novel implementation of a multiset data structure, called a “bag,” in place of the {FIFO} queue usually employed in serial breadth-ﬁrst search algorithms. For a variety of benchmark input graphs whose diameters are signiﬁcantly smaller than the number of vertices — a condition met by many real-world graphs — {PBFS} demonstrates good speedup with the number of processing cores.},
	eventtitle = {the 22nd {ACM} symposium},
	pages = {303},
	booktitle = {Proceedings of the 22nd {ACM} symposium on Parallelism in algorithms and architectures - {SPAA} '10},
	publisher = {{ACM} Press},
	author = {Leiserson, Charles E. and Schardl, Tao B.},
	urldate = {2018-11-01},
	date = {2010},
	langid = {english},
}

@article{pandey_mantis:_2018,
	title = {Mantis: A Fast, Small, and Exact Large-Scale Sequence-Search Index},
	volume = {7},
	issn = {2405-4712},
	url = {https://www.cell.com/cell-systems/abstract/S2405-4712(18)30239-4},
	doi = {10.1016/j.cels.2018.05.021},
	shorttitle = {Mantis},
	pages = {201--207.e4},
	number = {2},
	journaltitle = {Cell Systems},
	shortjournal = {cels},
	author = {Pandey, Prashant and Almodaresi, Fatemeh and Bender, Michael A. and Ferdman, Michael and Johnson, Rob and Patro, Rob},
	urldate = {2018-11-02},
	date = {2018-08-22},
	pmid = {29936185},
	keywords = {de Bruijn graph, Bloom filter, color equivalence classes, counting quotient filter, experiment discovery, Mantis, {RNA} sequencing, sequence Bloom tree, sequence search},
}

@online{noauthor_[1803.01969]_nodate,
	title = {[1803.01969] Moment-Based Quantile Sketches for Efficient High Cardinality Aggregation Queries},
	url = {https://arxiv.org/abs/1803.01969},
	urldate = {2018-11-04},
}

@article{tai_sketching_2017,
	title = {Sketching Linear Classifiers over Data Streams},
	url = {http://arxiv.org/abs/1711.02305},
	abstract = {We introduce a new sub-linear space sketch---the Weight-Median Sketch---for learning compressed linear classifiers over data streams while supporting the efficient recovery of large-magnitude weights in the model. This enables memory-limited execution of several statistical analyses over streams, including online feature selection, streaming data explanation, relative deltoid detection, and streaming estimation of pointwise mutual information. Unlike related sketches that capture the most frequently-occurring features (or items) in a data stream, the Weight-Median Sketch captures the features that are most discriminative of one stream (or class) compared to another. The Weight-Median Sketch adopts the core data structure used in the Count-Sketch, but, instead of sketching counts, it captures sketched gradient updates to the model parameters. We provide a theoretical analysis that establishes recovery guarantees for batch and online learning, and demonstrate empirical improvements in memory-accuracy trade-offs over alternative memory-budgeted methods, including count-based sketches and feature hashing.},
	journaltitle = {{arXiv}:1711.02305 [cs, stat]},
	author = {Tai, Kai Sheng and Sharan, Vatsal and Bailis, Peter and Valiant, Gregory},
	urldate = {2018-11-04},
	date = {2017-11-07},
	eprinttype = {arxiv},
	eprint = {1711.02305},
	keywords = {Computer Science - Data Structures and Algorithms, Statistics - Machine Learning, Computer Science - Machine Learning},
}

@article{harris_improved_2018,
	title = {Improved Representation of Sequence Bloom Trees},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/12/19/501452},
	doi = {10.1101/501452},
	abstract = {Algorithmic solutions to index and search biological databases are a fundamental part of bioinformatics, providing underlying components to many end-user tools. Inexpensive next generation sequencing has filled publicly available databases such as the Sequence Read Archive beyond the capacity of traditional indexing methods. Recently, the Sequence Bloom Tree ({SBT}) and its derivatives were proposed as a way to efficiently index such data for queries about transcript presence. We build on the {SBT} framework to construct the {HowDe}-{SBT} data structure, which uses a novel partitioning of information to reduce the construction and query time as well as the size of the index. We evaluate {HowDe}-{SBT} by both proving theoretical bounds on its performance and using real {RNA}-seq data. Compared to previous {SBT} methods, {HowDe}-{SBT} can construct the index in less than half the time, and with less than 32\% of the space, and can answer small-batch queries nine times faster.},
	pages = {501452},
	journaltitle = {{bioRxiv}},
	author = {Harris, Robert S. and Medvedev, Paul},
	urldate = {2018-12-20},
	date = {2018-12-19},
	langid = {english},
}

@article{baker_dashing:_2018,
	title = {Dashing: Fast and Accurate Genomic Distances with {HyperLogLog}},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/12/20/501726},
	doi = {10.1101/501726},
	shorttitle = {Dashing},
	abstract = {Dashing is a fast and accurate software tool for estimating similarities of genomes or sequencing datasets. It uses the {HyperLogLog} sketch together with cardinality estimation methods that specialize in set unions and intersections. Dashing sketches genomes more rapidly than previous {MinHash}-based methods while providing greater accuracy across a wide range of input sizes and sketch sizes. It can sketch and calculate pairwise distances for over 87K genomes in under 6 minutes. Dashing is open source and available at https://github.com/dnbaker/dashing.},
	pages = {501726},
	journaltitle = {{bioRxiv}},
	author = {Baker, Daniel N. and Langmead, Benjamin},
	urldate = {2018-12-20},
	date = {2018-12-20},
	langid = {english},
}

@article{benson_genbank_2017,
	title = {{GenBank}},
	volume = {45},
	issn = {0305-1048},
	url = {https://academic.oup.com/nar/article/45/D1/D37/2605704},
	doi = {10.1093/nar/gkw1070},
	abstract = {Abstract.  {GenBank}® (www.ncbi.nlm.nih.gov/genbank/) is a comprehensive database that contains publicly available nucleotide sequences for 370 000 formally descr},
	pages = {D37--D42},
	issue = {D1},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res},
	author = {Benson, Dennis A. and Cavanaugh, Mark and Clark, Karen and Karsch-Mizrachi, Ilene and Lipman, David J. and Ostell, James and Sayers, Eric W.},
	urldate = {2018-12-26},
	date = {2017-01-04},
	langid = {english},
}

@article{bloom_space/time_1970,
	title = {Space/Time Trade-offs in Hash Coding with Allowable Errors},
	volume = {13},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/362686.362692},
	doi = {10.1145/362686.362692},
	abstract = {In this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash-coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency.
The new methods are intended to reduce the amount of space required to contain the hash-coded information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods.
In such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to “catch” the small fraction of errors associated with the new methods. An example is discussed which illustrates possible areas of application for the new methods.
Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time.},
	pages = {422--426},
	number = {7},
	journaltitle = {Commun. {ACM}},
	author = {Bloom, Burton H.},
	urldate = {2019-04-07},
	date = {1970-07},
	keywords = {hash addressing, hash coding, retrieval efficiency, retrieval trade-offs, scatter storage, searching, storage efficiency, storage layout},
}

@article{irber_junior_efficient_2016,
	title = {Efficient cardinality estimation for k-mers in large {DNA} sequencing data sets},
	rights = {All rights reserved},
	url = {http://www.biorxiv.org/content/early/2016/06/07/056846.abstract},
	pages = {056846},
	journaltitle = {{bioRxiv}},
	author = {Irber Junior, Luiz Carlos and Brown, C. Titus},
	urldate = {2017-03-02},
	date = {2016},
}

@article{crusoe_khmer_2015,
	title = {The khmer software package: enabling efficient nucleotide sequence analysis},
	volume = {4},
	rights = {All rights reserved},
	url = {https://f1000research.com/articles/4-900/v1},
	shorttitle = {The khmer software package},
	journaltitle = {F1000Research},
	author = {Crusoe, Michael R. and Alameldin, Hussien F. and Awad, Sherine and Boucher, Elmar and Caldwell, Adam and Cartwright, Reed and Charbonneau, Amanda and Constantinides, Bede and Edvenson, Greg and Fay, Scott and {others}},
	urldate = {2017-03-02},
	date = {2015},
}

@article{pell_scaling_2012,
	title = {Scaling metagenome sequence assembly with probabilistic de Bruijn graphs},
	volume = {109},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/109/33/13272},
	doi = {10.1073/pnas.1121464109},
	abstract = {Deep sequencing has enabled the investigation of a wide range of environmental microbial ecosystems, but the high memory requirements for de novo assembly of short-read shotgun sequencing data from these complex populations are an increasingly large practical barrier. Here we introduce a memory-efficient graph representation with which we can analyze the k-mer connectivity of metagenomic samples. The graph representation is based on a probabilistic data structure, a Bloom filter, that allows us to efficiently store assembly graphs in as little as 4 bits per k-mer, albeit inexactly. We show that this data structure accurately represents {DNA} assembly graphs in low memory. We apply this data structure to the problem of partitioning assembly graphs into components as a prelude to assembly, and show that this reduces the overall memory requirements for de novo assembly of metagenomes. On one soil metagenome assembly, this approach achieves a nearly 40-fold decrease in the maximum memory requirements for assembly. This probabilistic graph representation is a significant theoretical advance in storing assembly graphs and also yields immediate leverage on metagenomic assembly.},
	pages = {13272--13277},
	number = {33},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Pell, Jason and Hintze, Arend and Canino-Koning, Rosangela and Howe, Adina and Tiedje, James M. and Brown, C. Titus},
	urldate = {2015-10-25},
	date = {2012-08-14},
	langid = {english},
	pmid = {22847406},
	keywords = {compression, metagenomics},
}
