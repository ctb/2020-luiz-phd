<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# Compositional analysis with scaled minhash and gather

\chaptermark{Scaled}

## Introduction

### MinHash sketch: similarity and containment

A data sketch is a representative proxy for the original data focused on queries
for specific properties.
It can also be viewed as a probabilistic data structure (in contrast with
deterministic data structures),
since it uses hashing techniques to provide statistical guarantees on the
precision of the answer for a query.
This allows a memory/accuracy trade-off:
using more memory leads to more accurate results,
but in memory-constrained situations it still bound results to an expected error rate.

The MinHash sketch [@broder_resemblance_1997] was developed at Altavista in the context of document clustering and deduplication.
It provides an estimate of the Jaccard similarity
(called **resemblance** in the original article)
and the **containment** of two documents,
estimating how much of document $A$ is contained in document $B$.
These estimates depend on two processes:
Converting documents to sets ("Shingling"),
and transforming large sets into short signatures,
while preserving similarity ("Min-Hashing").
In the original use case the *$w$-shingling* $\Omega$ of a document $D$ is defined as the set of all
continuous subsequence of $w$ words contained in $D$.
*Min-hashing* is the process of creating $W = \{\,h(x) \mid \forall x \in \Omega\,\}$,
where $h(x)$ is an uniform hash function,
and then either
a) keeping the $n$ smallest elements ($\mathbf{MIN}_n(W)$), or
b) keeping elements that are $0 \mod m$ ($\mathbf{MOD}_m(W)$).

$\mathbf{MIN}_n(W)$ is fixed-sized (length $n$) and supports similarity estimation,
but doesn't support containment.
$\mathbf{MOD}_m(W)$ supports both similarity and containment,
with the drawback of not being fixed-sized anymore,
growing with the complexity of the document.

Mash [@ondov_mash:_2016] was the first implementation of MinHash in the genomic context,
relating the $w$-shingling of a document to the $k$-mer composition of genomic
datasets,
and using the $\mathbf{MIN}_n(W)$ fixed-sized formulation for the signature.
Mash needs extra information (the genome size for the organism being queried) to account for genomic complexity in datasets.
This extra information is required because using a fixed-size MinHash leads to
different degrees of accuracy when comparing across highly-diverged organisms
(bacteria to animals, for example), and it is even more extreme when taking more
complex datasets into account (like metagenomes).

### minhash and mod hash (broder paper)

### mash and genomic minhash

## Scaled MinHash

### Comparison with mod hash

MinHash containment: mash works only on datasets of similar genome size/complexity
ModHash is a solution (more complex dataset will have larger sketch), but falls apart
with very small dataset (might not contain enough hashes for meaningful results)

(intro CMash here)

## Decomposition of queries with Gather

gather algorithm: Given a dataset and a collection of signatures $DB$
```
query = scaled_minhash(dataset)

matches = []
while len(query) > threshold:
  best_contained = find_best_contained(query, collection)
  matches.append(best_match)
  query = remove_intersection(query, best_match)

summarize_matches(matches)
```

Operations:
```
def find_best_contained(query, DB):
  best_score = 0.0
  best_match = None

  for sig in DB:
    if score := query.containment(sig) > best_score:
      best_score = score
      best_match = sig

  return best_match
```

```
def subtract(query, best_match):
  # possible 
```

### Limitations

- increased size (compared with minhash, which is constant size)
- detection threshold (viruses)

## Implementation

### smol

`smol` is a minimal implementation for the Scaled MinHash sketch and the gather method for simulation and verifying results with more featureful tools.
There are two compatible versions,
one in Python and another in Rust,
due to performance requirements when processing large datasets (like metagenomes).
Both versions of the Scaled MinHash implementations use each language standard library sets
(`set` for Python, `HashSet` for Rust)
for storing hashes and efficient set operations (intersection and difference).
Experiments used the Rust version for calculating Scaled MinHash sketches,
and the Python version for running gather and reporting containment scores.
Since they serialize the sketches to a compatible JSON format,
they can be used interchangeably and while computing Scaled MinHash sketches is
orders of magnitude faster in Rust,
for gather running time are similar and in the order of seconds.

The Python version has two external dependencies:
`screed` for sequence parsing,
and `mmh3` for the MurmurHash3 hash function.
Other modules from the standard library are used for JSON serialization (`json`)
and command line parsing (`argparse`).

The Rust version has four direct external dependencies:
`needletail` for sequence parsing and normalization
(similar to what `screed` does in the Python version),
`murmurhash3` for the MurmurHash3 hash function,
`serde_json` for JSON serialization and `structopt` for command line parsing.

### exact

`exact` is an exact k-mer counting implementation in Rust.


### Experiments

<!--
- containment score experiments
- sketch sizes
-->

Experiments are implemented in `snakemake` workflows and use `conda` for
managing dependencies,
allowing reproducibility of the results with one command:
`snakemake --use-conda`.
This will download all data,
install dependencies and generate the data used for analysis.

The analysis is contained in a Jupyter Notebook,
and can be executed in any place where it is supported,
including in a local installation or using Binder,
a service that deploy a live Jupyter environment in cloud instances.
Instructions are available at https://github.com/luizirber/phd
<!-- TODO: replace with zenodo archival DOI -->

## Evaluation

### Run gather on a collection of signatures (>100)

(show method works, even if slow -> leading to introduction of other indices)

(use scaled=1 and scaled=1000, show results are similar.
Use small datasets, since scaled=1 will be huge...)

(maybe use podar dataset?)

(compare with cmash/mash screen here?)













































<!--
#### Bloom Filter: Set membership

The Bloom Filter [@bloom_space/time_1970] allows set membership queries.
Inserting an element in a Bloom Filter is a two-step process: first use multiple
hash functions on the element,
and then for each bit set in the hashed value update the same position in the
bit array.
Querying an element involves calculating the multiple hash values for the
element and then checking if the bits are set in the bit array.
This guarantees that a false negative is impossible
(if the element was inserted,
a bit would be set),
but can report false positives if there are collisions in the hash values.

khmer [@crusoe_khmer_2015] implements a variation where longer (and multiple) bit arrays are used,
and hash functions are derived from a composed hashing strategy $h_i(x) = h(x) \mod p_i$,
where each bit array has a distinct length $p_i$ (and $p$ is a prime number),
with $h(x)$ being a more CPU-intensive hash function.

Bloom Filters are used extensively in bioinformatics,
including lossy representation of assembly graphs [@pell_scaling_2012] and as
a filtering step in processing pipelines [@ondov_mash:_2016].

#### HyperLogLog: Cardinality estimation

The HyperLogLog sketch [@flajolet_hyperloglog:_2008] estimates the number of unique elements in a dataset.
It is designed to lower the variability of a more basic estimator:
given a run of $\rho$ zeros in a binary sequence,
it estimates the cardinality of the dataset to be $2^{\rho}$.
It achieves this by splitting the binary sequence in two:
the lower bits define an index for $m$ registers,
and each register contain the longest run of zeros seen for that index.
The HyperLogLog estimator $E(D)$ is an harmonic mean of the registers $M$,
with a correction factor $\alpha_m$ for the number of registers:
$$E(D) = \alpha_m m^2 \left( \sum_{j=0}^{m-1} 2^{-M[j]} \right)^{-1}$$
More recent methods [@heule_hyperloglog_2013] improve the cardinality estimator
by further refining the estimate based on empirical data.

In genomic contexts, the khmer [@crusoe_khmer_2015] implementation of HyperLogLog [@irber_junior_efficient_2016]
uses the $k$-mer composition of a genomic dataset and the \emph{murmurhash3} hash function,
together with the improved estimator from [@heule_hyperloglog_2013].
Dashing [@baker_dashing:_2018] is a recent method that supports both similarity and cardinality estimation using HyperLogLog,
based on a better estimator for union and intersection of HyperLogLog sketches by [@ertl_new_2017].
-->
