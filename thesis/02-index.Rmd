# Efficient indexing of collections of signatures

\chaptermark {MHBT}

## Introduction

<!-- TODO
 - Public genome databases
 - Exponential growth
 - Challenges in indexing, searching and updating the indices for collections of datasets
 - Methods for indexing genomic datasets
-->

Searching for matches in large collection of datasets is challenging when hundreds of thousands of
them are available,
especially if they are partitioned and the data is not all present at the same place,
or too large to even be stored in a single system.
<!-- cite some methods, including SBT and Mantis -->

<!-- 
CTBQ: Additional points to raise: in-memory representation of sketches may be too big (!!),
goal here is on disk storage/low minimum memory for "extremely large data" situation.
Also/in addition, want ability to do incremental loading of things.
Note we are not talking here about situations where the indices themselves are too big to download,
could maybe include forward pointer to chp4.

Note, in this chapter you could also include distinction in performance between SBT and LCA DB,
to whit: large scaled works well with LCA (small DB, ~tolerable memory, load all at once, then quite fast)
but low scaled may work (much) better with SBT.
-->

[@marchet_data_2019] classifies indexing strategies for querying large collections of
sequencing datasets into $k$-mer aggregative methods and color aggregative methods.
$k$-mer aggregative methods index the $k$-mer composition for each individual dataset,
and then build structures for retrieving the datasets where the query $k$-mers are present.
Color aggregative methods index the full $k$-mer composition for all datasets,
and then assign a color representing an intersection of datasets where a $k$-mer is present.
This allows reduced space requirements,
since each $k$-mer is stored only once,
but needs extra structures for storing what datasets each color represents.

Both strategies allow the same class of queries,
but with different trade-offs and optimizations:
$k$-mer aggregative methods favor threshold queries
("what datasets contain more 60% of the query $k$-mers?")
while color aggregative methods tend to be more efficient for specific $k$-mer
queries ("what datasets contain this query $k$-mer?").

<!-- how to dive into hierarchical index and inverted index below? -->

<!-- ctb mar 31

CTBQ: Additional points to raise: in-memory representation of sketches may be
too big (!!), goal here is on disk storage/low minimum memory for "extremely
large data" situation. Also/in addition, want ability to do incremental loading
of things. Note we are not talking here about situations where the indices
themselves are too big to download, could maybe include forward pointer to chp4.

Note, in this chapter you could also include distinction in performance between
SBT and LCA DB, to whit: large scaled works well with LCA (small DB, ~tolerable
memory, load all at once, then quite fast) but low scaled may work (much) better
with SBT.
-->

### Hierarchical index

<!-- 'k-mer aggregative methods in (marchet 2019)' -->

Bloofi [@crainiceanu_bloofi:_2015] is a hierarchical index structure that
extends the Bloom Filter basic query to collections of Bloom Filters.
Instead of calculating the union of all Bloom Filters in the collection
(which would allow answering if an element is present in any of them)
it defines a tree structure where the original Bloom Filters are leaves,
and internal nodes are the union of all the Bloom Filters in their subtrees.
Searching is based on a breadth-first search,
with pruning when no matches are found at an internal level.
Bloofi can also be partitioned in a network,
with network nodes containing a subtree of the original tree and only being
accessed if the search requires it.

The Sequence Bloom Tree [@solomon_fast_2016] adapts Bloofi for genomic contexts,
rephrasing the problem as experiment discovery:
given a query sequence $Q$ and a threshold $\theta$,
which experiments contain at least $\theta$ of the original query $Q$?
Experiments are encoded in Bloom Filters containing the $k$-mer composition of transcriptomes,
and queries are transcripts.

Further developments focused on clustering similar datasets to prune search
early [@sun_allsome_2017] and developing more efficient representations for the
internal nodes [@solomon_improved_2017] [@harris_improved_2018] to use less
storage space and memory.

<!--
example figure for SBT:
http://www.texample.net/tikz/examples/merge-sort-recursion-tree/
-->

### Inverted index

<!-- 'color- aggregative methods in (marchet 2019)' -->

An inverted index is a mapping from words in a text back to its location inside
the text.
An example is the index in the back of a book,
containing a list of topics and in which page they are present.
Information retrieval system use inverted index to find the occurrences of
words in a text [@ziviani_compression_2000].

When indexing the $k$-mer decomposition of genomic datasets,
the inverted index is a map of all hashes in the collection back to
the dataset from where they originated.
Just as words can appear more than once in a text,
hashes show up in more than one signature,
so the inverted index maps a hash to a list of datasets.

kraken has a similar index,
but uses a taxonomic ID (taxon) for each dataset.
Datasets can share the same ID,
if they belong to the same taxon.
Moreover,
if a hash is present in more than one dataset
kraken also reduces the list of taxons to the lowest common ancestor (LCA),
which leads to reduced memory usage.
[@nasko_refseq_2018] explores how this LCA approach leads to decreased precision and sensitivity over time,
since more datasets are added to reference databases and the chance of a k-mer being present
in multiple datasets increases.

Efficient storage of the list of signatures IDs can also be achieved via representation of the list as colors,
where a color can represent one dataset or multiple datasets (if a hash is present in many of them).
Mantis [@pandey_mantis:_2018] uses this hash to color mapping
(and an auxiliary color table) to achieve reduced memory usage.

## sourmash

<!-- TODO
- intro sourmash as a more featureful implementation of Scaled MinHash sketches
- focused on user experience via CLI and Python API
-->

### Specialized indices for Scaled MinHash sketches
<!-- TODO
- mash screen has a similar index, but it is constructed on-the-fly using the
  distinct hashes in a sketch collection as keys,
  and the values are mapped to a hash occurrence counter in a query metagenome.

- sourmash LCA index is the opposite: it stores the hashes, but allow
  reconstructing the original sketch collection.
-->

<!--
  CAMI 2 refseq, 141k signatures, scaled=2000
  LCA: 18.3 GB
  SBT: 524 MB
    sig.name(): 5078 MB
-->

### Converting between indices

Both MHBT and LCA index can recover the original sketch collection.
In the MHBT case,
it outputs all the leaf nodes.
In the LCA index,
it reconstruct each sketch from the hash -> dataset ID mapping.
This allows trade-offs between storage efficiency,
distribution,
updating and query performance.

## Results

### Efficient similarity and containment queries

#### Index construction and updating

\todoin{

- 
- resource usage (time, cpu, mem)

}

#### Querying

\todoin{

- discuss COST (cost of single thread)
- sourmash indices benefit from more data. Inverted index
}


## Discussion

### Limitations

### Future directions

\todoin{
- Other indices
  * Fundamentally a scaled minhash is a subset of the k-mer composition of a
  dataset, so any index from https://www.biorxiv.org/content/10.1101/866756v1
  ("Data structures based on k-mers for querying large collections of sequencing datasets")
  can be used to scale,
  and given that gather is defined over a collection of signatures the indices
  can also be used to improve gather performance.

- sourmash is currently single threaded, but that's an implementation detail.
  Parallel queries are possible (in a shared read-only index)
}

### Summary/Conclusion

## Methods

### Implementation

sourmash is implemented in Rust for the core performance-critical functionality,
and exposes a C API that is wrapped in Python for higher-level features.
This includes a Python API for interactive use,
as well as a command-line interface for more traditional bioinformatics workflows.



### Experiments
