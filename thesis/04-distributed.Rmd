# Distributed signature calculation with wort

\chaptermark {wort}

## Introduction

Signature calculation in sourmash is a low memory and streaming process,
since a signature retains a much smaller subset of the original data
and once a chunk of the data is processed it is not necessary to keep it or refer to it again.
The calculation process is frequently I/O bound on reading data,
unless very fast access to the data is available,
when it becomes CPU-bound by the hashing algorithm.
sourmash signatures are also typically small,
with input sizes in the order of hundreds of times larger than the final signature.
This creates opportunities for approaches where distributed workers do all the expensive data access,
and report back a small amount of computed data.
Systems like \emph{Folding@home} [@beberg_folding_2009] focus on small amounts of input data and heavy computational requirements instead,
but have similar architectures where a central server distributes tasks to workers and collect results.

On top of fast access to one dataset,
indexing large public genomic databases also requires considering another axis:
since there are millions of datasets in these databases,
embarrassingly parallel approaches to access the data are necessary too,
because even with the fastest possible connection it would still take too long to process them all serially.
Given that processing a dataset into a signature can be done independently of other datasets,
a system that can spawn a large number of jobs can potentially keep up with the rate of new dataset inclusion,
and also make it feasible to calculate signatures for the current datasets.

In this sense,
getting access to the data and downloading it is the main bottleneck,
especially considering that high-speed network connections are expensive.
At the same time,
given the embarrassingly parallel nature of signature calculation,
multiple workers can be spawned in different computational infrastructure
(academic clusters, public cloud instances, and even browsers)
with potentially independent network connections.
Then the limitation is how fast data can be served from the public genomic database being indexed;
this is inherently a limitation of centralized data storage.
Cloud solutions disguise these limitations by creating a single access point and rerouting requests to multiple servers,
but accessing it outside the cloud provider infrastructure incurs additional costs.

This chapter explores these distributed architectures and methods for
crowdsourcing signature calculation for large public genomic databases.

## Methods

### Data sources and selection

#### Sequence Read Archive

The Sequence Read Archive [@leinonen_sequence_2011] stores raw sequence data for sequencing experiments,
and is part of the International Nucleotide Sequence Database Collaboration (INSDC)
with the European Nucleotide Archive (ENA) [@leinonen_european_2011] and the DNA Database of Japan (DDBJ) [@kaminuma_ddbj_2010].
Submission to any of them are shared with the others,
although with different methods for accessing and potentially distinct data formats.

Given the large amount of data available and the biological diversity of the datasets,
the initial prototype focused on single genome microbial datasets (excluding metagenomes).
The datasets were selected with a query to the Sequence Read Archive to retrieve enough metadata for processing (dataset ID, size and download location),
but this metadadata is not stored in the signatures.
This avoids synchronization problems when the metadata is updated in the original source,
and users should check them for more information.

#### IMG

The Integrated Microbial Genomes and Microbiomes (IMG/M) [@chen_imgm_2019] contains assembled genomes
and metagenomes for sequencing done at the Department of Energy's Joint Genome Institute,
focusing on environmental samples.
Because the sequencing data is already assembled,
it is closer to GenBank and RefSeq than the Sequence Read Archive,
which hosts raw sequence data.

Through a "Facilities Integrating Collaborations for User Science" (FICUS) grant we also had access to the JGI IMG/M database,
as well as to the NERSC supercomputers for computational resources.
There were 65 thousand single genome datasets corresponding to the taxons represented in the database,
and metadata was provided independently of online access to IMG.
It represented a snapshot of the data available on the database in October 2017.
The sequencing data was available in a shared storage unit,
and processing was done through NERSC's Cori scheduling system.

### The worker

Based on the initially retrieved metadata,
each dataset was processed by a worker.
Each worker runs an instance of `sourmash`,
with additional software for accessing the data or uploading it to more permanent storage,
depending on what database is being calculated.

For IMG the data was available in a shared filesystem,
so `sourmash compute` could access it directly.

For the SRA the worker uses `fastq-dump` (from the `sra toolkit`) to download the data and stream it through a UNIX pipe into a `sourmash compute` process.
Since the data is being streamed,
it is not being stored locally by the worker,
so if a signature need to be recalculated the data needs to be downloaded again.
Signatures would only need to be recalculated if the `sourmash compute` parameters are changed,
and the initial parameters tried to accommodate for it and cover more use cases,
which leads to a larger signature (but still a fraction of the size of the original data).

### Coordination

The first prototype (named `soursigs`, available at https://github.com/dib-lab/soursigs/)
used the `snakemake` workflow management system [@koster_snakemakescalable_2012] to:

- query metadata from SRA and choose datasets;

- for each dataset, spawn a worker job to process it and generate a signature;

- upload the signature to permanent storage.

This system was able to process 412k datasets and 28 TB of data over two weeks,
<!-- link to soursigs posts?
https://blog.luizirber.org/2016/12/28/soursigs-arch-1/
http://ivory.idyll.org/blog/2017-sourmash-sra-microbial-wgs.html
 -->
but processing was still limited to one system executing all the workers jobs.
Even with the system being a cluster and providing access to many compute nodes,
the external network connection was still shared between all nodes.

This initial prototype only read a small part of the streaming dataset,
since the original scientific goal was to identify novel datasets after comparing them to a reference database.
Due to the random nature of shotgun sequencing
<!-- TODO  intro syrah, connect to scaled -->
This was also necessary because at this point the Scaled MinHash introduced in chapter 1 was not available yet,
and experiments with `soursigs` led to many of the insights that led to the Scaled MinHash development.

### Storage

Signatures were initially stored locally in the system where they were processed
(NERSC Cori for IMG, MSU HPCC for SRA).
None serve as permanent or public storage,
since they are restricted systems and access can be revoked once projects are concluded.

For convenience,
data was uploaded to Amazon S3,
which makes it easier to share the data but also has drawbacks:

- Storage and data access incur additional costs,
  especially if the data becomes a popular resource and is used frequently.

- Even with all the infrastructure and level of service that AWS provides,
  it is a single point of failure and is not easy to mirror in other places without generating new URLs
  or using some sort of load balancer.

Other data repositories like Zenodo are better for archival purposes,
but not as appropriate for a system that is frequently updated.

## From soursigs to wort

<!-- how and why did soursigs evolve into wort? -->
<!-- mirror structure from previous section, showing how things changed? -->

<!--
CTBQ: which limitation is being overcome? It's ok to repeat and be specific.
-->
`wort` was created to fix some of the issues with the initial `soursigs` prototype
and allow more computational systems to participate in the process,
building an heterogeneous system that doesn't depend on similar resources being available for each worker.
The two major changes were:

- encapsulate workers in a Docker container [@boettiger_introduction_2015],
  which is also executable by Singularity [@kurtzer_singularity_2017].
  Docker containers are useful for cloud instances,
  but are usually not available in academic clusters.
  Similarly, Singularity tends to be present in academic clusters,
  but is not as convenient in cloud infrastructures.
  While containers are more convenient for distribution,
  all the software dependencies are also available via bioconda [@gruning_bioconda_2018] and conda-forge.

- Use a messaging queue for job submission that can be accessed by workers in any computational system.

Another goal for `wort` is creating a basic API for common tasks that allow external interactions without needing knowledge about internal implementation.
Since the internal details are not exposed it allows refactoring and reimplementing the infrastructure with other technologies not used in the original version.
In this sense,
the original version is implemented in a more traditional (and centralized) approach,
with a web application written in Flask serving the API and sending job requests to an Amazon Web Services SQS queue.
Workers connect to the queue and grab jobs for execution,
and upload results to Amazon S3.

```{r wortArch, eval=TRUE, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, cache=TRUE, out.width="500px", fig.width=4, fig.cap="wort architecture"}
knitr::include_graphics('figure/arch.png')
```

<!-- drop the "a little centralization goes a long way" paper here -->
The coordination of the system is still centralized,
but this new infrastructure allows workers to be executed in any system that has Docker or Singularity support,
including most academic clusters and cloud providers,
as well as local workstations and personal laptops.
Since signature calculation is I/O bound,
CPU resources consumption is proportional to how fast the network connection can provide data,
and the CPU is idle waiting for data most of the time.

Since signature calculation jobs are independent of each other,
the single point of failure of this approach is the centralized coordination.
At the same time,
developing a system is easier if there is no coordination required between most parts of the system,
and given the exploratory nature of this experimental system having an initial implementation that can be used to clarify use cases and figure out the requirements and interfaces needed is very valuable.

Over time the implementation can become less dependant on the original platform used (AWS),
and move to more decentralized approaches.
<!-- It's easier to test and refactor than coming up with the perfect design upfront -->
Because the original version is already working,
refactoring the internal implementation while keeping all tests working allow improvements to the system without breaking the public API and clients using `wort`.

## Results

As of early June 2020,
there are 1.2 million datasets in the SRA that fit the initial goal of
calculating signatures for microbial genomes.
These datasets represent around 14% of the 8.3 million public datasets in the SRA,
<!--
Total public datasets:
https://www.ncbi.nlm.nih.gov/sra/?term=cluster_public%5Bprop%5D
-->
although this proportion is not representative of the total data size:
this microbial subset contains 352.5 TB of data,
around 2.2% of the 16 PB of publicly-accessible data.

Around 16 thousand datasets weren't processed correctly,
due to missing data in the SRA or because they are too large for the time
allocated for workers and didn't finish downloading in time.
Excluding these 16k datasets,
the signatures represent 332 TB of the original data,
stored in 2.4 TB of signature files.

It is worth noting that signatures were calculated with a $\mathbf{scaled}=1000$,
but $332/2.4=138$,
more than expected if considering only final file size.
Although each signature is using this scaled value,
there are 3 Scaled MinHash sketches for each dataset (for $k={21,31,51}$),
and each sketch is also tracking abundance for each hash,
which doubles the storage requirements.

`wort` focuses on providing an API for triggering further processing,
and additional logic for updating the signature collection was implemented as
scripts accessing the exposed API.
They only need access to the public API,
and don't need to be running in the same machine as `wort`.
As an example,
an external script that downloads metadata for datasets recently added to the
SRA is executed everyday for the last two years


## Discussion

<!--
We set out to solve a problem...

We designed a system to take advantage of custom features of the task:
asymmetric, etc.

This system was centralized in the following way, but otherwise relied on independent workers.

The system worked pretty well within its limitations. Processed XX YY ZZ etc etc.

Challenges of centralization. Challenges of single database to
access. Are there better solutions that rely on the central people to
do things differently, e.g. allow docker containers running on their
hardware? (Kind of what the cloud allows, sure. But you can make the
point that no, not really, you're always limited by the centralized
nature of the design.) So in chp4 we explore a different more
decentralized approach.

CTB: Attacks on this system by bad faith workers? Access restrictions on queue.

Maybe reference "linda tuple space" in the paper somewhere?

CTBQ: What points do you want to make here?
-->

## Conclusion

<!--
Distributed workers can be an efficient and effective solution but only insofar as the centralized database can scale to hand data to them fast enough.

TODO:
* pull text down from above into discussion as you think it makes sense
* smoothen out additional text from CTB discussion, with extra points
* move on to chp5
-->
