<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# Compositional analysis with scaled minhash and gather

\chaptermark{Gather}

## Introduction

<!--
 CTBQ: What are the goals of compositional analysis in biological systems?
-->

## Decomposition of queries with gather
<!--
CTB suggestion: put gather in {\tt form} or {\sf form} to distinguish it.
-->

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[ht]
   \DontPrintSemicolon
   \SetKwInOut{Input}{Input}
   \SetKwInOut{Output}{Output}
   \SetKwBlock{Loop}{Loop}{}
   \SetKwFunction{FindBestContained}{FindBestContained}
   \SetKwFunction{Subtract}{Subtract}
   \SetKwFunction{AppendToMatches}{AppendToMatches}
   \Input{query $Q$}
   \Input{a collection $C$ of reference datasets}
   \Input{a containment threshold $T$}
   \Output{a list of matches $M$ from $C$ contained in $Q$}
   \BlankLine
   $M \leftarrow \emptyset$\;
   $Q' \leftarrow Q$\;
   \Loop {
       $best = \FindBestContained(Q', C, T)$\;
       \If{$best = \emptyset$ }{
           break\;
       }
       $\AppendToMatches(M, best)$\;
       $Q' \leftarrow \Subtract(M, Q')$\;
   }
   \KwRet{matches}
   \caption{The gather method}
\end{algorithm} 
 
\begin{algorithm}[ht]
  \DontPrintSemicolon
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \SetKwFunction{containment}{containment}
  \SetKwBlock{Loop}{Loop}{}
  \Input{query $Q$}
  \Input{a list $C$ of reference datasets}
  \Input{a containment threshold $T$}
  \Output{the match $m$ for $m \in C$ with best containment in $Q$, or $\emptyset$ if no match above threshold}
  \BlankLine
  $best\_containment \leftarrow T$\;
  $m \leftarrow \emptyset$\;
  \For{$c \in C$}{
     $containment \leftarrow \containment(c,Q)$\;
     \If{$containment \ge best\_containment$ }{
       $best\_containment \leftarrow containment$\;
       $m \leftarrow c$\;
     }
  }
  \KwRet{$m$}
  \caption{a \emph{FindBestContained} implementation for a list}
\end{algorithm} 

### CAMI challenges

Critical Assessment of Metagenome Intepretation (CAMI) is a community-driven initiative,
and brings together tool developers to create standard and reproducible
benchmarking methods.
It is divided in three big groups:
assembly,
binning and profiling.

Since there is a standard output format that tools need to implement,
performance comparisons can be
<!--

Intro: what is CAMI, what it provides:
- challenges provide gold standards (what is expected to be in the sample)
- standardized tools for profiling performance comparisons (OPAL)

-->

#### CAMI 1 (low, medium, high)

<!--
- Already published, but results are for older/outdated tools
- still a good benchmark (because gold standards are available)
- low and medium datasets have viruses, which are not in sourmash indices
-->

#### CAMI 2 mouse gut toy challenge

<!--
- new CAMI challenges, with short and long reads (pacbio)
- toy challenge is calibration: gold standard available
- CAMI provides results for other tools

CAMI 2 refseq database also doesn't have viruses...

- sourmash gather:
-->

### Limitations

<!--
Viruses (scaled minhash too small).
  mash screen solves this by going for sensitivity (at the cost of precision),
  possible solution: scaled+num hashes, but would only allow mash screen-like method
-->

## Implementation

### smol

`smol` is a minimal implementation for the Scaled MinHash sketch and the gather method for simulation and verifying results with more featureful tools.
There are two compatible versions,
one in Python and another in Rust,
due to performance requirements when processing large datasets (like metagenomes).
Both versions of the Scaled MinHash implementations use each language standard library sets
(`set` for Python, `HashSet` for Rust)
for storing hashes and efficient set operations (intersection and difference).
Experiments used the Rust version for calculating Scaled MinHash sketches,
and the Python version for running gather and reporting containment scores.
Since they serialize the sketches to a compatible JSON format,
they can be used interchangeably and while computing Scaled MinHash sketches is
orders of magnitude faster in Rust,
for gather running time are similar and in the order of seconds.

The Python version has two external dependencies:
`screed` for sequence parsing,
and `mmh3` for the MurmurHash3 hash function.
Other modules from the standard library are used for JSON serialization (`json`)
and command line parsing (`argparse`).

The Rust version has four direct external dependencies:
`needletail` for sequence parsing and normalization
(similar to what `screed` does in the Python version),
`murmurhash3` for the MurmurHash3 hash function,
`serde_json` for JSON serialization and `structopt` for command line parsing.

## Evaluation

(cmash and mash screen can do `gather` too,
but still require the original data available.
Scaled MinHash allows using the sketches for gather,
which are a fraction of the original data in size.
Additional benefit:
if there are collection updates you don't need to recalculate the query sketch,
and both cmash and mash screen would need the original data for the query
again)

### Implementation for experiments

Experiments are implemented in `snakemake` workflows and use `conda` for
managing dependencies,
allowing reproducibility of the results with one command:
`snakemake --use-conda`.
This will download all data,
install dependencies and generate the data used for analysis.

The analysis is contained in a Jupyter Notebook,
and can be executed in any place where it is supported,
including in a local installation or using Binder,
a service that deploy a live Jupyter environment in cloud instances.
Instructions are available at https://github.com/luizirber/phd
<!-- TODO: replace with zenodo archival DOI -->

<!-- TODO: related work section? It's sort of covered with cmash and mash screen
already -->

## Conclusion and Future Work
