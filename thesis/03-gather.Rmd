# Compositional analysis with Scaled MinHash and gather

\chaptermark{Gather}

## Introduction

<!--
 - What are the goals of compositional analysis in biological systems?
-->

Taxonomic profiling is a particular instance of this general problem.
The goal of taxonomic profiling is to find the identity and relative abundance of microbial community elements
at a specific taxonomic rank (species, genus, family),
especially in metagenomic samples [@sczyrba_critical_2017].

Existing taxonomic profilers use different methods to solve this problem,
including aligning sequences to a reference database [@huson_megan_2016],
using marker genes derived from known organisms from reference databases [@segata_metagenomic_2012]
or coupled with unknown organisms clustered from metagenomes [@milanese_microbial_2019],
and exact $k$-mer matching using fixed $k$ and lowest common ancestor (LCA) for resolving
$k$-mer assignments matching multiple taxons from a reference database
[@wood_kraken:_2014] or variable $k$ and assigning multiple taxons per sequence,
with an option to reduce it further to the LCA [@kim_centrifuge_2016].

Once each sequence (from raw reads or assembled contigs) has a taxonomic assignment,
these methods resolve the final identity and abundance for each member of the community by summarizing the assignments to a specific taxonomic level.
Taxonomic profiling is fundamentally limited by the availability of reference datasets to be used for assignments,
and reporting what percentage of the sample is unassigned is important to assess results,
especially in under characterized environments such as oceans and soil.

### Decomposition of queries with gather

Methods summarizing taxonomic assignments from sequences in the query metagenome to calculate the profile for the community follow a bottom-up approach.
`gather` is a new method following a top-down approach:
starting from the $k$-mer composition of the query,
it iteratively finds a match in a collection of datasets with the largest _containment_ of the query (most elements in common),
and create a new query by _removing elements_ in the match from the original query.
The process stop when the new query doesn't have any more matches in the collection,
or a user-provided minimum detection threshold is reached.
This approach differs from previous methods because the co-occurrence of $k$-mers
in a match is considered a strong signal that they are coming from the same organism in the original sample,
and is used instead of the LCA-based methods to resolve ambiguities in the taxonomic assignment of a sequence (or its $k$-mers).

Any data structure supporting both the _containment_
$C(A, B) = \frac{\vert A \cap B \vert }{\vert A \vert}$
and _remove elements_ operations can be used as a query with `gather`.
For example,
a _set_ of the $k$-mer composition of the query supports element removal,
and calculating containment can be done with regular set operations.
Approximate membership query (AMQ) sketches like the _Counting Quotient Filter_ [@pandey_general-purpose_2017] can also be used,
with the benefit of reduced storage and memory usage.
Moreover,
the collection of datasets can be implemented with any data structure that can do containment comparisons with the query data structure,
including implicit representations like an inverted index from hashed $k$-mers to dataset IDs (as detailed in subsection [2.1.2](#inverted-index)).

<!-- TODO crossref with section/subsection is not using all the numbers (only the chapter)... -->

### Implementing gather with Scaled MinHash sketches

_Scaled MinHash_ sketches (section [1.2](#scaled-minhash)) are a subset of the $k$-mer composition $M$ of a dataset,
with the guarantee that if a hash $w'=h(m)$ of $k$-mer $m \in M$ is present in the _Scaled MinHash_ sketch with scaled parameter $s$
$$\mathbf{SCALED}_s(W) = \{\,w \leq \frac{H}{s} \mid \forall w \in W\,\}$$
where $W = \{\,h(m) \mid \forall m \in M\,\}$,
$h(x)$ is an uniform hash function and $H$ is the maximum value possible for $h(.)$,
the same hash $w'$ will be present in sketches for other datasets also containing the $k$-mer $m$,
as long as they have the same parameter $s$ or can be downsampled to the same $s$.
This is not guaranteed for regular _MinHash_ sketches, <!-- TODO because fixed size explanation -->
and is what allows removing elements from the _Scaled MinHash_ sketch of a query once a match is found.
Since the containment of two datasets can also be estimated directly from their _Scaled MinHash_ sketches,
they are viable data structures for `gather`,
especially since they are only a small fraction of the original dataset size and easier to store,
manipulate and share.

_Scaled MinHash_ sketches can be stored in any data structure for representing the $k$-mer composition $M$ of a dataset [@marchet_data_2019],
and as a subset of $M$ they can also be indexed by approaches for the full $k$-mer composition.
`sourmash` [@pierce_large-scale_2019] defines the MinHash Bloom Tree (_MHBT_) index,
a $k$-mer aggregative method with explicit representation of datasets based on hierarchical indices and a specialization of the Sequence Bloom Tree [@solomon_fast_2016],
as well as the _LCA_ index,
a color-aggregative method with implicit representation of the datasets based on inverted indices.

Compared to previous taxonomic profiling methods,
_Scaled MinHash_ can also be seen as a mix of two other approaches:
It uses exact $k$-mer matching and assignment,
and the $k$-mers selected by the MinHashing process are equivalent to implicitly-defined markers.
It differs from previous approaches because only a subset of the $k$-mer composition is used for matching,
and traditional gene markers are explicitly chosen due to sequence conservation and low mutation rates,
while MinHashing $k$-mers generates a randomized,
but consistent across datasets,
set of marker $k$-mers.

### Taxonomic profiling with gather

Taxonomic profiling in `sourmash` is built as an extra step on top of the `gather` algorithm.
`gather` returns assignments to a dataset in a collection,
and based on that assignment the extra step associates a taxonomic ID
(based on some dataset identifier)
and a taxonomic lineage (a path from root to taxonomic ID) derived from a specific taxonomy.
After a lineage is available,
each taxonomic rank is summarized based on the abundances under it.
This information is used to build a profile in the CAMI profiling format,
but can also be adapted for other uses and tools.

<!-- TODO expand a bit -->

<!-- TODO: a diagram here, leave the algorithm for the methods section -->
<!-- Future paper TODO: demonstrate gather on top of other approaches? kraken/mantis/kProcessor? -->

## Results

### CAMI challenges

The Critical Assessment of Metagenome Intepretation (CAMI) [@sczyrba_critical_2017] is a community-driven initiative
bringing together tool developers and users to create standards for reproducibly benchmarking metagenomic methods.
Challenges are organized around datasets representing microbial communities of interest in metagenomics,
like marine,
high-strain and rhizosphere datasets.
Sequencing data is generated by CAMISIM [@fritz_camisim_2019],
a microbial community and metagenome simulator using a gold standard with a known community composition
to model different aspects
(diversity levels, abundances and sequencing technologies features)
of these datasets.

Each challenge typically includes three tasks:
assembly,
taxonomic profiling and binning (at taxon or genome levels).
Since there is a standard output format that tools need to implement,
performance comparisons can be streamlined.
CAMI provides a set of tools for computing performance metrics for each group:
MetaQUAST for assembly,
AMBER for binning,
and OPAL [@meyer_assessing_2019] for taxonomic profiling evaluation.

`gather` can be used for the taxonomic profiling task,
where the goal is finding what organisms are present in a metagenome sampled from a microbial community,
and what are their relative abundances.
Taxonomic profiling is based on a predetermined taxonomy of known organisms,
as well as a collection of genomes for each organism.
It differs from taxonomic classification,
where each read or sequence in the metagenome is given a taxonomic assignment,
and from binning,
which aims to cluster reads or sequences into bins,
possibly representing unknown organisms.

The first set of CAMI challenges happened in 2015 and results were published in 2017.
Since then more tools were developed and improved,
as well as reference databases growing in size and diversity.
Reproducing the running environment used by the original tools is challenging,
even with all the focus on reproducibility by the organizers and community.

<!--- TODO: bring back later, CAMI II is more interesting and enough for discussing results?

#### The first set of CAMI challenges

The initial CAMI challenges [@sczyrba_critical_2017] included three datasets based on genome sequences from
689 bacterial and archaeal isolates (cultured organisms) and 598 sequences derived from plasmids,
viruses and other circular elements.
Each challenge dataset simulates 150-bp paired-end reads with Illumina HighSeq error profiles,
with varying levels of complexity:

 - _low_, a single 15-Gbp sample with 40 genomes and 20 circular elements;

 - _medium_, two samples with 132 genomes and 100 circular elements, totalling 40-Gbp;

 - _high_, a five-sample time series with 596 genomes and 478 circular elements), totalling 75-Gbp.

All datasets also simulate realistic characteristics from sequenced metagenomes,
including species with strain-level diversity,
presence of viruses, plasmids and other circular elements,
and genomes covering distinct evolutionary distances,
with the goal of measuring how these characteristics impact the performance of each method.
-->

<!-- Urgent TODO: medium and high datasets!  -->

<!-- TODO: comment results from CAMI low -->

<!--
```{r gatherCAMIlowTable, eval=TRUE, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, cache=TRUE, out.width="100%", auto_pdf=TRUE, fig.cap="CAMI I Low table"}
knitr::include_graphics('figure/cami_i_low_table.png')
```

```{r gatherCAMIlowSpider, eval=TRUE, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, cache=TRUE, out.width="100%", auto_pdf=TRUE, fig.cap="CAMI I Low spider plot"}
knitr::include_graphics('figure/cami_i_low_recall_precision.png')
```
-->

<!-- TODO
- low and medium datasets have viruses, which are not in sourmash indices
-->

#### CAMI II mouse gut metagenome dataset

<!-- TODO more intro -->

The CAMI initiative released new challenges in 2019 (marine, high-strain and pathogen detection)
and 2020 (rhizosphere),
with updated processes for submission,
evaluation and participation.
In addition to short-read sequencing data matching Illumina profiles,
it also includes long-read sequencing data with PacBio and Nanopore profiles,
allowing further benchmarks and comparisons.
CAMI also provides a snapshot of the RefSeq reference genomes for building specialized databases for each tool,
as well with an NCBI Taxonomy to minimize differences in taxonomic reports.
Since challenges only release the gold standard after they are concluded and published,
results for comparison with new methods are still pending.

The CAMI II mouse gut metagenome [@meyer_tutorial_2020] is a toy dataset,
used for preparing and calibrating tools for other CAMI II challenges.
Similar to the concluded challenges from CAMI,
it provides gold standards for expected microbial community composition,
including presence and relative abundance of organisms.
The simulated mouse gut metagenome (_MGM_) was derived from 791 bacterial and archaeal genomes,
representing 8 phyla,
18 classes,
26 orders,
50 families,
157 genera,
and 549 species.
64 samples were generated with _CAMISIM_,
with 91.8 genomes present on each sample on average.
Each sample is 5 GB in size,
and both short-read (Illumina) and long-read (PacBio) sequencing data is available.

Because the official challenges don't have gold standards published yet,
it is currently the only alternative for using the CAMI benchmarking tools to evaluate new methods with updated datasets.
Curated metadata for multiple tools is also available,
and users can submit their tools for inclusion.
All tools currently in the curated metadata repository use the short-read samples.

```{r gatherCAMImgSpider, eval=TRUE, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, cache=TRUE, out.width="75%", auto_pdf=TRUE, fig.cap='(ref:cami2caption)', fig.show="hold", fig.align="center"}
knitr::include_graphics(c('../experiments/profiling/figures/spider_plot_relative.pdf',
                          '../experiments/profiling/figures/ranks_by_tool.pdf',
                          '../experiments/profiling/figures/scores.pdf'))
```

(ref:cami2caption) Updated Fig. 6 from [@meyer_tutorial_2020] including `sourmash`.
**a** Comparison per taxonomic rank of methods in terms of completeness, purity (1% filtered), and L1 norm.
**b** Performance per method at all major taxonomic ranks, with the shaded bands showing the standard deviation of a metric.
In **a** and **b**, completeness, purity, and L1 norm error range between 0 and 1.
The L1 norm error is normalized to this range and is also known as Bray-Curtis distance.
The higher the completeness and purity, and the lower the L1 norm, the better the profiling performance.
**c** Methods rankings and scores obtained for the different metrics over all samples and taxonomic ranks.
For score calculation, all metrics were weighted equally.


<!-- TODO: show results
main points:
- all tools used only the short reads datasets
- highest completeness and purity
- low L1-norm error (second, check this)

- for running times: run kraken2 to serve as a base and compare with running
  times from the tutorial paper?
  - can't really build the standard db, check with minikraken...

  data from tutorial:

		Taxonomic binner Time (hh:mm) Memory (kbytes)
		MetaPhlAn 2.9.21 18:44 5,139,172
		MetaPhlAn 2.2.0 12:30 1,741,304
		Bracken 2.5 (only Bracken) 0:01 24,472
		Bracken 2.5 (Kraken and Bracken) 3:03 39,439,796
		FOCUS 0.31 13:27 5,236,199
		CAMIARKQuikr 1.0.0 16:19 27,391,555
		mOTUs 1.1 19:50 1,251,296
		mOTUs 2.5.1 14:29 3,922,448
		MetaPalette 1.0.0 76:49 27,297,132
		TIPP 2.0.0 151:01 70,789,939
		MetaPhyler 1.25 119:30 2,684,720

  kraken2 (mem doesn't match, so runtime probably doesn't match either...)
    117 seconds per sample, with 8 cores and 8.1 GB of RAM.
		3:03 = 10803s
		117 * 64 = 7488s
		"correction factor" = 10803 / 7488 = 1.44

  trying with metaphlan 2.9.21.
    643 seconds per sample, with 8 cores and 3.46 GB of RAM.
    18:44 = 64844
    643 * 64 = 41152
    "correction factor" = 64844 / 41152 = 1.57

  sourmash
    38251 seconds, with 1 core and 5.62 GB of RAM
    after correction: 38251 * 1.57 = 60054
-->

<!--
The CAMI II RefSeq snapshot for reference genomes doesn't include viruses;
this benefits `sourmash` because viral _Scaled MinHash_ sketches are usually not well supported for containment estimation,
since viral sequences require small scaled values to have enough hashes to be reliable.
-->

## Discussion

`gather` is a new method for decomposing datasets into its components, (...)
<!-- TODO small summary -->

Other containment estimation methods described in Chapter [1](#chp-scaled),
_CMash_ [@koslicki_improving_2019] and _mash screen_ [@ondov_mash_2019],
can also implement `gather`.
Running a search requires access to the original dataset (_mash screen_) for the query,
or a Bloom Filter derived from the original dataset (_CMash_),
and when the collection of reference sketches is updated the Bloom Filter from _CMash_ can be reused,
but _mash screen_ needs access to original dataset again.

Since _Scaled MinHash_ sketches allow using the sketch directly for `gather`,
which are a fraction of the original data in size and also allow enumerating all the elements,
an operation not possible with Bloom Filters,
they can be stored and reused for large collections of sequencing datasets,
including public databases like the Sequence Read Archive [@leinonen_sequence_2011].
A service that calculate these _Scaled MinHash_ sketches and make them available can improve discoverability of these large collections,
as well as support future use cases derived from other _Scaled MinHash_ features.

<!-- Scaling to large collections of references -->
Taxonomic profiling is fundamentally limited by the availability of reference datasets,
even if new reference datasets can be derived from clustering possible organisms based on sequence data in metagenomes [@milanese_microbial_2019].
`gather` as implemented in `sourmash` is a method that can scale to increasingly larger collections of datasets
due to multiple reasons:

  - containment and similarity estimation with _Scaled MinHash_ sketches has
    lower computational requirements than alignment over all reads of a dataset;

  - since _Scaled MinHash_ sketches use a subset of the $k$-mer composition,
    they also scale better than full $k$-mer composition representations,
    requiring less space and reducing the number of elements to be computed;

  - querying multiple databases can be done independently,
    avoiding the need to merge,
    update or reprocess databases when new datasets are available.
    A new database with the new datasets can be constructed and queried together
    with previous ones.

<!-- TODO to make this point I need more info about the other databases used...
I don't think they were calculated from the refseq snapshot
https://github.com/CAMI-challenge/data/issues/2

These aspects allowed the `sourmash` database to be include the largest number
of reference datasets of all methods compared,
-->

<!-- dependency on taxonomic assignments -->
Taxonomic profiling in `sourmash` is implemented as an extra step on top of `gather` results.
Because these steps are independent of the dataset assignment that `gather` generates,
updates to the taxonomy don't require re-executing `gather`,
since the taxonomic information can be derived from the same dataset identifier
(but potentially with a new associated taxonomic ID).
This allows using new taxonomies derived from the same underlying datasets [@parks_standardized_2018],
as well as updates to the original taxonomy used before.

<!-- Benchmarking -->
<!-- TODO
Despite the improvements to the metrics measure by CAMI benchmarks,
Benchmarking is still an unsolved problem,
despite CAMI efforts
-->


<!--
### Limitations

- Viruses (scaled minhash too small).
  mash screen solves this by going for sensitivity (at the cost of precision),
  possible solution: scaled+num hashes, but would only allow mash screen-like method

- gather assigns hashes to best matches ("winner-takes-all"). Other approaches
  will be needed to disambiguate matches further
  -> check species score from centrifuge

- abundance analysis
  -> EM from centrifuge, based on cufflinks/sailfish

- Because it favors the best containment, larger organisms are more likely to be chosen first
  (since they have more hashes/k-mers).
  An additional step could take in consideration the size of the match too?
  This is not quite the similarity, note: We still want containment, but add match size
  to the calculation without necessarily using |A union B|

Scaled MinHash sketches don't preserve information about individual sequences,
and for larger scaled values and short sequences they have increasingly smaller chances of not having any of its $k$-mers (represented as hashes) contained in the final sketch.
-->

### Future directions

In this chapter `gather` is described in terms of taxonomic profiling of metagenomes.
That is one application of the algorithm,
but it can applied to other biological problems too.
If the query is a genome instead of a metagenome,
`gather` can be used to detect possible contamination in the assembled genome by
using a collection of genomes and removing the query genome from it (if it is present).
This allows finding matches that contain the query genome and evaluating if they agree at specific taxonomic levels,
and in case of large divergence (two different phyla are found, for example)
it is likely to indicative that the query genome contains sequences from different organisms.
This is especially useful for quality control and validation of metagenome-assembled genomes (MAGs),
genomes assembled from reads binned and clustered from metagenomes,
as well as a verification during submission of new assembled genomes to public
genomic databases like GenBank.

Improvements to the calculation of _Scaled MinHash_ sketches can also improve
the taxonomic profiling use case.
Exact $k$-mer matching is limited in phylogenetically distant organisms,
since small nucleotide differences lead to distinct $k$-mers,
breaking homology assumptions. <!-- TODO verify/cite? -->
Different approaches for converting the datasets into a set to be hashed (_shingling_) than computing the nucleotide $k$-mer composition,
such as spaced $k$-mers [@leimeister_fast_2014] and minimizers [@roberts_reducing_2004]
and alternative encodings for the nucleotides using 6-frame translation to amino acid [@gish_identification_1993]
or other reduced alphabets [@peterson_reduced_2009],
can allow comparisons on longer evolutionary distances and so improve taxonomic profiling by increasing the sensitivity of the containment estimation.
These improvements don't fundamentally change the `gather` method,
since it would still be based on the same *containment* and *remove element* operations,
but show how `gather` works as a more general method that can leverage characteristics from different building blocks and explore new or improved use cases.

### Conclusion



## Methods

### The gather algorithm

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[ht]
   \DontPrintSemicolon
   \SetKwInOut{Input}{Input}
   \SetKwInOut{Output}{Output}
   \SetKwBlock{Loop}{Loop}{}
   \SetKwFunction{FindBestContainment}{FindBestContainment}
   \SetKwFunction{Remove}{Remove}
   \SetKwFunction{AppendToMatches}{AppendToMatches}
   \Input{query $Q$}
   \Input{a collection $C$ of reference datasets}
   \Input{a containment threshold $T$}
   \Output{a list of matches $M$ from $C$ contained in $Q$}
   \BlankLine
   $M \leftarrow \emptyset$\;
   $Q' \leftarrow Q$\;
   \Loop {
       $(best, M) \leftarrow \FindBestContainment(Q', C, T)$\;
       \If{$M = \emptyset$ }{
           break\;
       }
       $\AppendToMatches(M)$\;
       $Q' \leftarrow \Remove(M, Q')$\;
   }
   \KwRet{matches}
   \caption{The gather method}
\end{algorithm}

\begin{algorithm}[ht]
  \DontPrintSemicolon
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \SetKwFunction{containment}{containment}
  \SetKwBlock{Loop}{Loop}{}
  \Input{query $Q$}
  \Input{a list $C$ of reference datasets}
  \Input{a containment threshold $T$}
  \Output{the containment $b$ and the match $m$ for $m \in C$ with best containment $b$ in $Q$, or $\emptyset$ if no match above threshold}
  \BlankLine
  $b \leftarrow T$\;
  $m \leftarrow \emptyset$\;
  \For{$c \in C$}{
     $containment \leftarrow \containment(c,Q)$\;
     \If{$containment \ge b$ }{
       $b \leftarrow containment$\;
       $m \leftarrow c$\;
     }
  }
  \KwRet{$(b, m)$}
  \caption{a \emph{FindBestContainment} implementation for a list}
  \label{alg:list}
\end{algorithm}

### Implementation

#### sourmash

`gather` is implemented as a method of the `Index` abstract base class (ABC) in `sourmash`.
This ABC declares methods that any index in `sourmash` has to implement,
but each index is allowed to implement it in the most efficient or performant way:

  1. For `Linear` indices, the `FindBestContainment` operation is implemented as a linear scan over the list of signatures (Algorithm \@ref(alg:list));

  2. For `MHBT` indices, `FindBestContainment` is implemented as a depth-first search that tracks the best containment found,
     and prunes the search if it the current estimated containment in an internal node is lower than the current best containment.

  3. `LCA` indices can implement `gather` by building a counter of how many hashes of the query are present in each signature,
    and then using the signature with the largest count as a match.
    As matches are found,
    the count for the hashes in the match are decreased in the counter,
    and then the new signature with the largest count is the next match.

`sourmash gather`,
the command-line interface that adds further user experience improvements to the API level,
also allows passing multiple indices to be searched,
providing incremental support for rerunning with additional data without having to recompute,
merge or update the original databases.

#### CAMI Evaluation

Experiments are implemented in `snakemake` workflows and use `conda` for
managing dependencies,
allowing reproducibility of the results with one command:
`snakemake --use-conda`.
This will download all data,
install dependencies and generate the data used for analysis.
OPAL and other CAMI-related commands are available in
<!-- TODO: set up zenodo for CAMI repo -->
https://github.com/luizirber/2020-cami/
including further instructions to reproduce results.

Analysis and figure generation code is contained in a Jupyter Notebook,
and can be executed in any place where it is supported,
including in a local installation or using Binder,
a service that deploy a live Jupyter environment in cloud instances.
Instructions are available at https://doi.org/10.5281/zenodo.4012667
