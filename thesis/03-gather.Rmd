# Compositional analysis with Scaled MinHash and gather {#chp-gather}

```{=latex}
\begin{epigraphs}
  \qitem{I seldom end up where I wanted to go, but almost always end up where I need to be.}%
        {Douglas Adams}
\end{epigraphs}
```

## Introduction

Compositional data analysis is the study of the parts of a whole using relative abundances [@aitchison_statistical_1982].
This is a general problem with applications across many scientific fields [@aitchison_compositional_2005],
and examples in biology include RNA-Seq [@quinn_field_2019-1],
metatranscriptomics [@macklaim_rna-seq_2018],
microbiome and metagenomics [@li_microbiome_2015].
Taxonomic profiling is a particular instance of this general problem
with the goal of finding the identity and relative abundance of microbial community elements
at a specific taxonomic rank (species, genus, family),
especially in metagenomic samples [@sczyrba_critical_2017].

Existing taxonomic profilers use different methods to solve this problem,
including aligning sequences to a reference database [@huson_megan_2016],
using marker genes derived from known organisms from reference databases [@segata_metagenomic_2012]
or coupled with unknown organisms clustered from metagenomes [@milanese_microbial_2019],
and exact $k$-mer matching using fixed $k$ and lowest common ancestor (LCA) for resolving
$k$-mer assignments matching multiple taxons from a reference database
[@wood_kraken:_2014] or variable $k$ and assigning multiple taxons per sequence,
with an option to reduce it further to the LCA [@kim_centrifuge_2016].

Once each sequence (from raw reads or assembled contigs) has a taxonomic assignment,
these methods resolve the final identity and abundance for each member of the community by summarizing the assignments to a specific taxonomic rank,
Taxonomic profiling is fundamentally limited by the availability of reference datasets to be used for assignments,
and reporting what percentage of the sample is unassigned is important to assess results,
especially in under characterized environments such as oceans and soil.

### Decomposition of queries with gather

Methods summarizing taxonomic assignments from sequences in the query metagenome to calculate the profile for the community follow a bottom-up approach.
`gather` is a new method following a top-down approach:
starting from the $k$-mer composition of the query,
it iteratively finds a match in a collection of datasets with the largest _containment_ of the query (most elements in common),
and create a new query by _removing elements_ in the match from the original query.
The process stop when the new query doesn't have any more matches in the collection,
or a user-provided minimum detection threshold is reached.
This approach differs from previous methods because the co-occurrence of $k$-mers
in a match is considered a strong signal that they are coming from the same organism in the original sample,
and is used instead of the LCA-based methods to resolve ambiguities in the taxonomic assignment of a sequence (or its $k$-mers).

Any data structure supporting both the _containment_
$C(A, B) = \frac{\vert A \cap B \vert }{\vert A \vert}$
and _remove elements_ operations can be used as a query with `gather`.
For example,
a _set_ of the $k$-mer composition of the query supports element removal,
and calculating containment can be done with regular set operations.
Approximate membership query (AMQ) sketches like the _Counting Quotient Filter_ [@pandey_general-purpose_2017] can also be used,
with the benefit of reduced storage and memory usage.
Moreover,
the collection of datasets can be implemented with any data structure that can do containment comparisons with the query data structure,
including implicit representations like an inverted index from hashed $k$-mers to dataset IDs (as detailed in subsection [2.1.2](#inverted-index)).

<!-- TODO crossref with section/subsection is not using all the numbers (only the chapter)... -->

### Implementing gather with Scaled MinHash sketches

_Scaled MinHash_ sketches (section [1.2](#scaled-minhash)) are a subset of the $k$-mer composition $M$ of a dataset,
with the guarantee that if a hash $w'=h(m)$ of $k$-mer $m \in M$ is present in the _Scaled MinHash_ sketch with scaled parameter $s$
$$\mathbf{SCALED}_s(W) = \{\,w \leq \frac{H}{s} \mid \forall w \in W\,\}$$
where $W = \{\,h(m) \mid \forall m \in M\,\}$,
$h(x)$ is an uniform hash function and $H$ is the maximum value possible for $h(.)$,
the same hash $w'$ will be present in sketches for other datasets also containing the $k$-mer $m$,
as long as they have the same parameter $s$ or can be downsampled to the same $s$.
This is not guaranteed for regular _MinHash_ sketches, <!-- TODO because fixed size explanation -->
and is what allows removing elements from the _Scaled MinHash_ sketch of a query once a match is found.
Since the containment of two datasets can also be estimated directly from their _Scaled MinHash_ sketches,
they are viable data structures for `gather`,
especially since they are only a small fraction of the original dataset size and easier to store,
manipulate and share.

_Scaled MinHash_ sketches can be stored in any data structure for representing the $k$-mer composition $M$ of a dataset [@marchet_data_2019],
and as a subset of $M$ they can also be indexed by approaches for the full $k$-mer composition.
`sourmash` [@pierce_large-scale_2019] defines the MinHash Bloom Tree (_MHBT_) index,
a $k$-mer aggregative method with explicit representation of datasets based on hierarchical indices and a specialization of the Sequence Bloom Tree [@solomon_fast_2016],
as well as the _LCA_ index,
a color-aggregative method with implicit representation of the datasets based on inverted indices.

Compared to previous taxonomic profiling methods,
_Scaled MinHash_ can also be seen as a mix of two other approaches:
It uses exact $k$-mer matching and assignment,
and the $k$-mers selected by the MinHashing process are equivalent to implicitly-defined markers.
It differs from previous approaches because only a subset of the $k$-mer composition is used for matching,
and traditional gene markers are explicitly chosen due to sequence conservation and low mutation rates,
while MinHashing $k$-mers generates a randomized,
but consistent across datasets,
set of marker $k$-mers.

### Taxonomic profiling with gather

Taxonomic profiling in `sourmash` is built as an extra step on top of the `gather` algorithm.
`gather` returns assignments to a dataset in a collection,
and based on that assignment the extra step associates a taxonomic ID
(based on some dataset identifier)
and a taxonomic lineage (a path from root to taxonomic ID) derived from a specific taxonomy.
After a lineage is available,
each taxonomic rank is summarized based on the abundances under it.
This information is used to build a profile in the CAMI profiling format,
but can also be adapted for other uses and tools.

<!-- TODO expand a bit -->

<!-- TODO: a diagram here, leave the algorithm for the methods section -->
<!-- Future paper TODO: demonstrate gather on top of other approaches? kraken/mantis/kProcessor? -->

## Results

### CAMI challenges

The Critical Assessment of Metagenome Intepretation (CAMI) [@sczyrba_critical_2017] is a community-driven initiative
bringing together tool developers and users to create standards for reproducibly benchmarking metagenomic methods.
Challenges are organized around datasets representing microbial communities of interest in metagenomics,
like marine,
high-strain and rhizosphere datasets.
Sequencing data is generated by CAMISIM [@fritz_camisim_2019],
a microbial community and metagenome simulator using a gold standard with a known community composition
to model different aspects
(diversity levels, abundances and sequencing technologies features)
of these datasets.

Each challenge typically includes three tasks:
assembly,
taxonomic profiling and binning (at taxon or genome levels).
Since there is a standard output format that tools need to implement,
performance comparisons can be streamlined.
CAMI provides a set of tools for computing performance metrics for each group:
MetaQUAST for assembly,
AMBER for binning,
and OPAL [@meyer_assessing_2019] for taxonomic profiling evaluation.

`gather` can be used for the taxonomic profiling task,
where the goal is finding what organisms are present in a metagenome sampled from a microbial community,
and what are their relative abundances.
Taxonomic profiling is based on a predetermined taxonomy of known organisms,
as well as a collection of genomes for each organism.
It differs from taxonomic classification,
where each read or sequence in the metagenome is given a taxonomic assignment,
and from binning,
which aims to cluster reads or sequences into bins,
possibly representing unknown organisms.

The first set of CAMI challenges happened in 2015 and results were published in 2017.
Since then more tools were developed and improved,
as well as reference databases growing in size and diversity.
Reproducing the running environment used by the original tools is challenging,
even with all the focus on reproducibility by the organizers and community.

<!--- TODO: bring back later, CAMI II is more interesting and enough for discussing results?

#### The first set of CAMI challenges

The initial CAMI challenges [@sczyrba_critical_2017] included three datasets based on genome sequences from
689 bacterial and archaeal isolates (cultured organisms) and 598 sequences derived from plasmids,
viruses and other circular elements.
Each challenge dataset simulates 150-bp paired-end reads with Illumina HighSeq error profiles,
with varying levels of complexity:

 - _low_, a single 15-Gbp sample with 40 genomes and 20 circular elements;

 - _medium_, two samples with 132 genomes and 100 circular elements, totalling 40-Gbp;

 - _high_, a five-sample time series with 596 genomes and 478 circular elements), totalling 75-Gbp.

All datasets also simulate realistic characteristics from sequenced metagenomes,
including species with strain-level diversity,
presence of viruses, plasmids and other circular elements,
and genomes covering distinct evolutionary distances,
with the goal of measuring how these characteristics impact the performance of each method.
-->

<!-- Urgent TODO: medium and high datasets!  -->

<!-- TODO: comment results from CAMI low -->

<!--
```{r gatherCAMIlowTable, eval=TRUE, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, cache=TRUE, out.width="100%", auto_pdf=TRUE, fig.cap="CAMI I Low table"}
knitr::include_graphics('figure/cami_i_low_table.png')
```

```{r gatherCAMIlowSpider, eval=TRUE, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, cache=TRUE, out.width="100%", auto_pdf=TRUE, fig.cap="CAMI I Low spider plot"}
knitr::include_graphics('figure/cami_i_low_recall_precision.png')
```
-->

<!-- TODO
- low and medium datasets have viruses, which are not in sourmash indices
-->

#### CAMI II mouse gut metagenome dataset

The CAMI initiative released new challenges in 2019 (marine, high-strain and pathogen detection)
and 2020 (rhizosphere),
with updated processes for submission,
evaluation and participation.
In addition to short-read sequencing data matching Illumina profiles,
it also includes long-read sequencing data with PacBio and Nanopore profiles,
allowing further benchmarks and comparisons.
CAMI also provides a snapshot of the RefSeq reference genomes for building specialized databases for each tool,
as well with an NCBI Taxonomy to minimize differences in taxonomic reports.
Since challenges only release the gold standard after they are concluded and published,
results for comparison with new methods are still pending.

The CAMI II mouse gut metagenome [@meyer_tutorial_2020] is a toy dataset,
used for preparing and calibrating tools for other CAMI II challenges.
Similar to the concluded challenges from CAMI,
it provides gold standards for expected microbial community composition,
including presence and relative abundance of organisms.
The simulated mouse gut metagenome (_MGM_) was derived from 791 bacterial and archaeal genomes,
representing 8 phyla,
18 classes,
26 orders,
50 families,
157 genera,
and 549 species.
64 samples were generated with _CAMISIM_,
with 91.8 genomes present on each sample on average.
Each sample is 5 GB in size,
and both short-read (Illumina) and long-read (PacBio) sequencing data is available.

Because the official challenges don't have gold standards published yet,
it is currently the only alternative for using the CAMI benchmarking tools to evaluate new methods with updated datasets.
Curated metadata for multiple tools is also available,
and users can submit their tools for inclusion.
All tools currently in the curated metadata repository use the short-read samples.

```{r gatherCAMImgSpider, eval=TRUE, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, cache=TRUE, out.width="75%", auto_pdf=TRUE, fig.cap='(ref:cami2caption)', fig.show="hold", fig.align="center"}
knitr::include_graphics(c('../experiments/profiling/figures/spider_plot_relative.pdf',
                          '../experiments/profiling/figures/ranks_by_tool.pdf',
                          '../experiments/profiling/figures/scores.pdf'))
```

(ref:cami2caption) Updated Fig. 6 from [@meyer_tutorial_2020] including `sourmash`.
**a** Comparison per taxonomic rank of methods in terms of completeness, purity (1% filtered), and L1 norm.
**b** Performance per method at all major taxonomic ranks, with the shaded bands showing the standard deviation of a metric.
In **a** and **b**, completeness, purity, and L1 norm error range between 0 and 1.
The L1 norm error is normalized to this range and is also known as Bray-Curtis distance.
The higher the completeness and purity, and the lower the L1 norm, the better the profiling performance.
**c** Methods rankings and scores obtained for the different metrics over all samples and taxonomic ranks.
For score calculation, all metrics were weighted equally.

Figure \@ref(fig:gatherCAMImgSpider) is an updated version of Figure 6 from [@meyer_tutorial_2020] including `sourmash`,
comparing 10 different methods for taxonomic profiling and their characteristics at each taxonomic rank. 
While previous methods show reduced completeness,
the ratio of taxa correctly identified in the ground truth,
below the genus level,
`sourmash` can reach 88.7\% completeness at the species level with the highest
purity (the ratio of correctly predicted taxa over all predicted taxa) across
all methods:
95.9\% when filtering predictions below 1\% abundance,
and 97\% for unfiltered results.
`sourmash` also has the lowest L1-norm error
(the sum of the absolute difference between the true and predicted abundances at
a specific taxonomic rank),
the highest number of true positives and the lowest number of false positives.

Table: (\#tab:gather-cami2) Updated Supplementary Table 12 from [@meyer_tutorial_2020].
Elapsed (wall clock) time (h:mm) and maximum resident set size
(kbytes) of taxonomic profiling methods on the 64 short read samples of the CAMI II mouse
gut data set. The best results are shown in bold. Bracken requires to run Kraken, hence the times
required to run Bracken and both tools are shown. The taxonomic profilers were run on a
computer with an Intel Xeon E5-4650 v4 CPU (virtualized to 16 CPU cores, 1 thread per core)
and 512 GB (536.870.912 kbytes) of main memory.

| Taxonomic binner                | Time (hh:mm) | Memory (kbytes) |
|:--------------------------------|-------------:|----------------:|
| MetaPhlAn 2.9.21                | 18:44        | 5,139,172       |
| MetaPhlAn 2.2.0                 | 12:30        | 1,741,304       |
| Bracken 2.5 (only Bracken)      | **0:01**     | **24,472**      |
| Bracken 2.5 (Kraken and Bracken)| **3:03**     | 39,439,796      |
| FOCUS 0.31                      | 13:27        | 5,236,199       |
| CAMIARKQuikr 1.0.0              | 16:19        | 27,391,555      |
| mOTUs 1.1                       | 19:50        | **1,251,296**   |
| mOTUs 2.5.1                     | 14:29        | 3,922,448       |
| MetaPalette 1.0.0               | 76:49        | 27,297,132      |
| TIPP 2.0.0                      | 151:01       | 70,789,939      |
| MetaPhyler 1.25                 | 119:30       |  2,684,720      |
| sourmash 3.4.0                  | 16:41        |  5,760,922      |

When considering resource consumption and running times,
`sourmash` used 5.62 GB of memory with an _LCA index_ built from the
RefSeq snapshot (141,677 genomes) with $scaled=10000$ and $k=51$.
Each sample took 597 seconds to run (on average),
totalling 10 hours and 37 minutes for 64 samples.
MetaPhlan 2.9.21 was also executed in the same machine,
a workstation with an AMD Ryzen 9 3900X 12-Core CPU running at 3.80 GHz,
64 GB DDR4 2133 MHz of RAM and loading data from an NVMe SSD,
in order to compare to previously reported times in Table \@ref(tab:gather-cami2) [@meyer_tutorial_2020].
MetaPhlan took 11 hours and 25 minutes to run for all samples,
compared to 18 hours and 44 minutes previously reported,
and correcting the `sourmash` running time by this factor it would likely take
16 hours and 41 minutes in the machine used in the original comparison.
After correction,
`sourmash` has similar runtime and memory consumption to the other best performing tools
(_mOTUs_ and _MetaPhlAn_),
both gene marker and alignment based tools.

Additional points are that `sourmash` is a single-threaded program,
so it didn't benefit from the 16 available CPU cores,
and it is the only tool that could use the full RefSeq snapshot,
while the other tools can only scale to a smaller fraction of it
(or need custom databases).
The CAMI II RefSeq snapshot for reference genomes also doesn't include viruses;
this benefits `sourmash` because viral _Scaled MinHash_ sketches are usually not well supported for containment estimation,
since viral sequences require small scaled values to have enough hashes to be reliable.

<!-- TODO: show results
main points:
- for running times: run kraken2 to serve as a base and compare with running
  times from the tutorial paper?
  - can't really build the standard db, check with minikraken...

  data from tutorial:

		Taxonomic binner Time (hh:mm) Memory (kbytes)
		MetaPhlAn 2.9.21 18:44 5,139,172
		MetaPhlAn 2.2.0 12:30 1,741,304
		Bracken 2.5 (only Bracken) 0:01 24,472
		Bracken 2.5 (Kraken and Bracken) 3:03 39,439,796
		FOCUS 0.31 13:27 5,236,199
		CAMIARKQuikr 1.0.0 16:19 27,391,555
		mOTUs 1.1 19:50 1,251,296
		mOTUs 2.5.1 14:29 3,922,448
		MetaPalette 1.0.0 76:49 27,297,132
		TIPP 2.0.0 151:01 70,789,939
		MetaPhyler 1.25 119:30 2,684,720

  kraken2 (mem doesn't match, so runtime probably doesn't match either...)
    117 seconds per sample, with 8 cores and 8.1 GB of RAM.
		3:03 = 10803s
		117 * 64 = 7488s
		"correction factor" = 10803 / 7488 = 1.44

  trying with metaphlan 2.9.21.
    643 seconds per sample, with 8 cores and 3.46 GB of RAM.
    18:44 = 64844
    643 * 64 = 41152
    "correction factor" = 64844 / 41152 = 1.57

  sourmash
    38251 seconds, with 1 core and 5.62 GB of RAM
    after correction: 38251 * 1.57 = 60054
-->

## Discussion

`gather` is a new method for decomposing datasets into its components that
outperforms current method when using synthetic datasets with known composition.
By leveraging _Scaled MinHash_ sketches and efficient indexing data structures
it can scale the number of reference datasets used by over an order of magnitude when compared
to existing methods.

Other containment estimation methods described in Chapter [1](#chp-scaled),
_CMash_ [@koslicki_improving_2019] and _mash screen_ [@ondov_mash_2019],
can also implement `gather`.
Running a search requires access to the original dataset (_mash screen_) for the query,
or a Bloom Filter derived from the original dataset (_CMash_),
and when the collection of reference sketches is updated the Bloom Filter from _CMash_ can be reused,
but _mash screen_ needs access to original dataset again.

Since _Scaled MinHash_ sketches allow using the sketch directly for `gather`,
which are a fraction of the original data in size and also allow enumerating all the elements,
an operation not possible with Bloom Filters,
they can be stored and reused for large collections of sequencing datasets,
including public databases like the Sequence Read Archive [@leinonen_sequence_2011].
A service that calculate these _Scaled MinHash_ sketches and make them available can improve discoverability of these large collections,
as well as support future use cases derived from other _Scaled MinHash_ features.

<!-- Scaling to large collections of references -->
Taxonomic profiling is fundamentally limited by the availability of reference datasets,
even if new reference datasets can be derived from clustering possible organisms based on sequence data in metagenomes [@milanese_microbial_2019].
`gather` as implemented in `sourmash` is a method that can scale to increasingly larger collections of datasets
due to multiple reasons:

  - containment and similarity estimation with _Scaled MinHash_ sketches has
    lower computational requirements than alignment over all reads of a dataset;

  - since _Scaled MinHash_ sketches use a subset of the $k$-mer composition,
    they also scale better than full $k$-mer composition representations,
    requiring less space and reducing the number of elements to be computed;

  - querying multiple databases can be done independently,
    avoiding the need to merge,
    update or reprocess databases when new datasets are available.
    A new database with the new datasets can be constructed and queried together
    with previous ones.

<!-- TODO to make this point I need more info about the other databases used...
I don't think they were calculated from the refseq snapshot
https://github.com/CAMI-challenge/data/issues/2

These aspects allowed the `sourmash` database to be include the largest number
of reference datasets of all methods compared,
-->

<!-- dependency on taxonomic assignments -->
Taxonomic profiling in `sourmash` is implemented as an extra step on top of `gather` results.
Because these steps are independent of the dataset assignment that `gather` generates,
updates to the taxonomy don't require re-executing `gather`,
since the taxonomic information can be derived from the same dataset identifier
(but potentially with a new associated taxonomic ID).
This allows using new taxonomies derived from the same underlying datasets [@parks_standardized_2018],
as well as updates to the original taxonomy used before.

<!-- Benchmarking -->
Despite improvements to standardization and reproducibility of previous analysis,
benchmarking taxonomic profiling tools is still challenging,
since tools can generate their reference databases from multiple sources and
choosing only one source can bias or make it impossible to evaluate them properly. 
This is especially true for real metagenomic datasets derived from samples
collected from soil and marine environments,
where the number of unknown organisms is frequently larger than those contained in
reference databases.
With the advent of metagenome-assembled genomes (MAGs) there are more resources
available for usage as reference datasets,
even if they are usually incomplete or draft quality.
`sourmash` is well positioned to include these new references to taxonomic
profiling given the minimal requirements (a _Scaled MinHash_ sketch of the
original dataset) and support for indexing hundreds of thousands of datasets.

### Limitations

`gather` as implemented in `sourmash` has the same limitations as _Scaled MinHash_ sketches,
including reduced sensitivity to small genomes/sequences such as viruses.
_Scaled MinHash_ sketches don't preserve information about individual sequences,
and short sequences using large scaled values have increasingly smaller chances of having any of its
$k$-mers (represented as hashes) contained in the sketch.
Because it favors the best containment,
larger genomes are also more likely to be chosen first due to their sketches have more elements,
and further improvements can take the size of the match in consideration too.
Note that this is not necessarily the _similarity_ $J(A, B)$ (which takes the size of both $A$ and $B$),
but a different calculation that normalizes the containment considering the size of the match.

`gather` is also a greedy algorithm,
choosing the best containment match at each step.
Situations where multiple matches are equally well contained or many datasets
are very similar to each other can complicate this approach,
and additional steps must be taken to disambiguate matches.
The availability of abundance counts for each element in the _Scaled MinHash_ is not well explored,
since the process of _removing elements_ from the query doesn't account for them
(the element is removed even if the count is much higher than the count in the match).
Both the multiple match as well as the abundance counts issues can benefit from
existing solutions taken by other methods,
like the _species score_ (for disambiguation) and _Expectation-Maximization_ (for abundance analysis)
approaches from Centrifuge [@kim_centrifuge_2016].

### Future directions

In this chapter `gather` is described in terms of taxonomic profiling of metagenomes.
That is one application of the algorithm,
but it can applied to other biological problems too.
If the query is a genome instead of a metagenome,
`gather` can be used to detect possible contamination in the assembled genome by
using a collection of genomes and removing the query genome from it (if it is present).
This allows finding matches that contain the query genome and evaluating if they agree at specific taxonomic rank,
and in case of large divergence (two different phyla are found, for example)
it is likely to indicative that the query genome contains sequences from different organisms.
This is especially useful for quality control and validation of metagenome-assembled genomes (MAGs),
genomes assembled from reads binned and clustered from metagenomes,
as well as a verification during submission of new assembled genomes to public
genomic databases like GenBank.

Improvements to the calculation of _Scaled MinHash_ sketches can also improve
the taxonomic profiling use case.
Exact $k$-mer matching is limited in phylogenetically distant organisms,
since small nucleotide differences lead to distinct $k$-mers,
breaking homology assumptions. <!-- TODO verify/cite? -->
Different approaches for converting the datasets into a set to be hashed (_shingling_) than computing the nucleotide $k$-mer composition,
such as spaced $k$-mers [@leimeister_fast_2014] and minimizers [@roberts_reducing_2004]
and alternative encodings for the nucleotides using 6-frame translation to amino acid [@gish_identification_1993]
or other reduced alphabets [@peterson_reduced_2009],
can allow comparisons on longer evolutionary distances and so improve taxonomic profiling by increasing the sensitivity of the containment estimation.
These improvements don't fundamentally change the `gather` method,
since it would still be based on the same *containment* and *remove element* operations,
but show how `gather` works as a more general method that can leverage characteristics from different building blocks and explore new or improved use cases.

### Conclusion

`gather` is a new method for decomposing datasets into its components with
application in biological sequencing data analysis (taxonomic profiling) that
can scale to hundreds of thousands of reference datasets with computational
resources requirements that are accessible to a large number of users
when used in conjunction with _Scaled MinHash_ sketches and efficient indices
such as _LCA_ and _MHBT_.
It outperforms current methods in community-develop benchmarks,
and opens the way for new methods that explore a top-down approach for profiling
microbial communities,
including further refinements that can resolve larger evolutionary distances and
also speed up the method computationally.

## Methods

### The gather algorithm

Algorithm [1](\ref{alg:gather}) describes the `gather` method using a generic operation
`FindBestContainment`.
An implementation for `FindBestContainment` for a list of datasets is presented in
Algorithm [2](\ref{alg:list}).
Appendix [A](#smol-source-code) has a minimal implementation in the Rust programming language [@matsakis_rust_2014] for both algorithms,
including a _Scaled MinHash_ sketch implementation using a _set_ data structure from the Rust standard library (`HashSet`).

```{=latex}
\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[ht]
   \label{alg:gather}
   \DontPrintSemicolon
   \SetKwInOut{Input}{Input}
   \SetKwInOut{Output}{Output}
   \SetKwBlock{Loop}{Loop}{}
   \SetKwFunction{FindBestContainment}{FindBestContainment}
   \SetKwFunction{Remove}{Remove}
   \SetKwFunction{AppendToMatches}{AppendToMatches}
   \Input{query $Q$}
   \Input{a collection $C$ of reference datasets}
   \Input{a containment threshold $T$}
   \Output{a list of matches $M$ from $C$ contained in $Q$}
   \BlankLine
   $M \leftarrow \emptyset$\;
   $Q' \leftarrow Q$\;
   \Loop {
       $(best, M) \leftarrow \FindBestContainment(Q', C, T)$\;
       \If{$M = \emptyset$ }{
           break\;
       }
       $\AppendToMatches(M)$\;
       $Q' \leftarrow \Remove(M, Q')$\;
   }
   \KwRet{matches}
   \caption{The gather method}
\end{algorithm}

\begin{algorithm}[ht]
  \label{alg:list}
  \DontPrintSemicolon
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \SetKwFunction{containment}{containment}
  \SetKwBlock{Loop}{Loop}{}
  \Input{query $Q$}
  \Input{a list $C$ of reference datasets}
  \Input{a containment threshold $T$}
  \Output{the containment $b$ and the match $m$ for $m \in C$ with best containment $b$ in $Q$, or $\emptyset$ if no match above threshold}
  \BlankLine
  $b \leftarrow T$\;
  $m \leftarrow \emptyset$\;
  \For{$c \in C$}{
     $containment \leftarrow \containment(c,Q)$\;
     \If{$containment \ge b$ }{
       $b \leftarrow containment$\;
       $m \leftarrow c$\;
     }
  }
  \KwRet{$(b, m)$}
  \caption{a \emph{FindBestContainment} implementation for a list}
\end{algorithm}
```

### Implementation

#### sourmash

`gather` is implemented as a method of the `Index` abstract base class (ABC) in `sourmash`.
This ABC declares methods that any index in `sourmash` has to implement,
but each index is allowed to implement it in the most efficient or performant way:

  1. For `Linear` indices, the `FindBestContainment` operation is implemented as a linear scan over the list of signatures (Algorithm [2](\ref{alg:list}));

  2. For `MHBT` indices, `FindBestContainment` is implemented as a depth-first search that tracks the best containment found,
     and prunes the search if it the current estimated containment in an internal node is lower than the current best containment.

  3. `LCA` indices can implement `gather` by building a counter of how many hashes of the query are present in each signature,
    and then using the signature with the largest count as a match.
    As matches are found,
    the count for the hashes in the match are decreased in the counter,
    and then the new signature with the largest count is the next match.

`sourmash gather`,
the command-line interface that adds further user experience improvements to the API level,
also allows passing multiple indices to be searched,
providing incremental support for rerunning with additional data without having to recompute,
merge or update the original databases.

#### CAMI Evaluation

Experiments are implemented in `snakemake` workflows and use `conda` for
managing dependencies,
allowing reproducibility of the results with one command:
`snakemake --use-conda`.
This will download all data,
install dependencies and generate the data used for analysis.
OPAL and other CAMI-related commands are available in
<!-- TODO: set up zenodo for CAMI repo -->
https://github.com/luizirber/2020-cami/
including further instructions to reproduce results.

Analysis and figure generation code is contained in a Jupyter Notebook,
and can be executed in any place where it is supported,
including in a local installation or using Binder,
a service that deploy a live Jupyter environment in cloud instances.
Instructions are available at https://doi.org/10.5281/zenodo.4012667
