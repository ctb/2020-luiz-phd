from glob import glob

rule all:
  input:
    "outputs/scaled_1000/containments_SRR606249.csv"
    "outputs/scaled_1000/SRR606249.csv",
#    "outputs/scaled_1/SRR606249.csv",
#    "outputs/mash_screen/SRR606249.tsv",
    "outputs/mash_screen/SRR606249-k21-s1000-m2.tsv",
    "outputs/mash_screen/SRR606249-k21-s1000-m1.tsv",
    "outputs/cmash/SRR606249.csv",

### Download the podar metagenome

rule download_reads:
  output: "data/reads/SRR606249_{i}.fastq.gz"
  shell: """
    wget -qO {output[0]} \
      ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR606/SRR606249/SRR606249_{wildcards.i}.fastq.gz
  """

### Download precomputed databases (might not need them)

rule download_mash_screen_db:
  output: "data/mash/RefSeq88n.msh"
  shell: """
    wget -qO {output}.gz https://obj.umiacs.umd.edu/mash/screen/RefSeq88n.msh.gz
    gunzip data/mash/RefSeq88n.msh.gz
  """

rule download_cmash_db:
  output: "data/cmash/cmash_db_n1000_k60.h5"
  shell: "curl -L https://ucla.box.com/shared/static/27xulklfvo60g7heogvi2y43jti5tqxo.gz | tar xzf - -C data/cmash"

### Download reference genomes present in podar

rule download_podar:
  output: expand("data/refs/{i}.fa", i=range(0, 64))
  shell: "curl -L https://osf.io/8uxj9/download | tar xzf - -C data/"

### Compute sourmash signatures for each reference genome and the reads

rule compute_refs:
  output: "outputs/scaled_{scaled}/refs/{sample}.sig"
  input: "data/refs/{sample}.fa"
  params:
    scaled = "{scaled}"
  conda: "envs/sourmash.yml"
  shell: """
    sourmash compute -k 21 \
             --name-from-first \
             --scaled {params.scaled} \
             --track-abundance \
             -o {output} \
             {input}
  """

rule compute_reads:
  output: "outputs/scaled_{scaled}/reads/{sample}.sig"
#  input: expand("data/reads/{{sample}}_{i}.fastq.gz", i=(1,2))
  input: "data/reads/{sample}_2.fastq.gz"
  params:
    scaled = "{scaled}"
  conda: "envs/sourmash.yml"
  shell: """
    sourmash compute -k 21 \
             --name-from-first \
             --scaled {params.scaled} \
             --track-abundance \
             -o {output} \
             {input}
  """

### Run gather

rule gather_sigs:
  output: "outputs/scaled_{scaled,\d+}/{sample}.csv"
  input:
    query = "outputs/scaled_{scaled}/reads/{sample}.sig",
    sigs = expand("outputs/scaled_{{scaled}}/refs/{i}.sig", i=range(0, 64))
  conda: "envs/sourmash.yml"
  shell: """
    sourmash gather -o {output} {input.query} {input.sigs}
  """

rule containment_sigs:
  output: "outputs/scaled_{scaled}/containments/{i}_{sample}.csv"
  input:
    sig = "outputs/scaled_{scaled}/refs/{i}.sig",
    metagenome = "outputs/scaled_{scaled}/reads/{sample}.sig",
  shell: """
    sourmash search --containment -o {output} {input.sig} {input.metagenome}
  """

rule containment_sample:
  output: "outputs/scaled_{scaled,\d+}/containments_{sample}.csv"
  input: expand("outputs/scaled_{{scaled}}/containments/{i}_{{sample}}.csv", i=range(0, 64)),
  run:
    out = ['containment,filename']
    for ref in input:
      with open(ref, 'r') as f:
        data = f.readlines()[-1]
        containment = data.split(',')[0]
        filename = os.path.basename(ref).split("_")[0]
        out.append(f"{containment},{filename}.fa")

    with open(output[0], 'w') as o:
        o.write("\n".join(out))

### Build DB and run mash screen

rule mash_sketch:
  output: "outputs/mash_screen/refs/{sample}-k{ksize}-s{num}-m{cutoff}.msh"
  input: "data/refs/{sample}.fa"
  conda: "envs/mash.yml"
  threads: 1
  params:
    ksize = "{ksize}",
    num = "{num}",
    cutoff = "{cutoff}",
  shell: """
    mash sketch -k {params.ksize} \
                -s {params.num} \
                -m {params.cutoff} \
                -o {output} \
                {input}
  """

rule build_mash_db:
  output: "outputs/mash_screen/db-k{ksize}-s{num}-m{cutoff}.msh"
  input: expand("outputs/mash_screen/refs/{i}-k{{ksize}}-s{{num}}-m{{cutoff}}.msh", i=range(0, 64))
  conda: "envs/mash.yml"
  threads: 1
  shell: """
    mash paste {output} {input}
  """

rule mash_screen:
  output: "outputs/mash_screen/{sample}-k{ksize}-s{num}-m{cutoff}.tsv"
  input:
    query = "data/reads/{sample}_2.fastq.gz",
    db = "outputs/mash_screen/db-k{ksize}-s{num}-m{cutoff}.msh",
#    db = "outputs/mash_screen/db.msh",
  conda: "envs/mash.yml"
  threads: 1
  shell: """
    mash screen -p {threads} {input.db} {input.query} > {output}
  """

### Build DB and run cmash

rule build_cmash_db:
  output:
    db = "outputs/cmash/db.h5",
    tst = "outputs/cmash/db.tst",
  input: expand("data/refs/{i}.fa", i=range(0, 64))
  conda: "envs/cmash.yml"
  shell: """
    MakeStreamingDNADatabase.py -k 21 <(ls -1 data/refs/*.fa) {output.db}
  """

rule cmash:
  output: "outputs/cmash/{sample}.csv"
  input:
    query = "data/reads/{sample}_2.fastq.gz",
    db = "outputs/cmash/db.h5"
  conda: "envs/cmash.yml"
  threads: 1
  shell: """
     StreamingQueryDNADatabase.py -t {threads} {input.query} {input.db} {output} 21-21-21
  """
